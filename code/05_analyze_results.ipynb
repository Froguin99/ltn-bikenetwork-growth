{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Analysis of bicycle network results\n",
    "## Project: Growing Urban Bicycle Networks with LTNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the existing infrastructure, the results from 03_poi_based_generation and calculates/analyzes a number of measures:\n",
    "* cost (length)\n",
    "* coverage  \n",
    "* directness  \n",
    "* efficiency\n",
    "* overlap with existing networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- find neighbourhoods where large amounts of residiental streets are used to potentially convert to LTNs\n",
    "- Comparison between LTN prioirtised and \"normal\" growth\n",
    "- only runs for one place at at time currently (my bad coding skills + getting stuck down rabbitholes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from src import utils\n",
    "PATH = utils.PATH # shortening the var name so that we don't have to change it below\n",
    "\n",
    "# System\n",
    "import csv\n",
    "import os\n",
    "import dill as pickle\n",
    "import itertools\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from copy import deepcopy\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Math/Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Network\n",
    "import networkx as nx\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "# Geo\n",
    "import osmnx as ox\n",
    "ox.settings.log_file = True\n",
    "ox.settings.requests_timeout = 300\n",
    "ox.settings.logs_folder = PATH[\"logs\"]\n",
    "import geopandas as gpd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # If True, will produce plots and/or verbose output to double-check\n",
    "# if not debug: # Only do this if sure the code is bug-free!\n",
    "#     warnings.filterwarnings('ignore')\n",
    "rerun_existing = True # If True, will re-run the costly analysis of existing infra even if files already exist.\n",
    "rerun = True # If True, recompute the analysis. If false, just re-make the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yaml.load(\n",
    "    open(\"../parameters/parameters.yml\"), \n",
    "    Loader=yaml.FullLoader)\n",
    "osmnxparameters = json.load(open(\"../parameters/osmnxparameters.json\", \"r\"))\n",
    "plotparam = json.load(open(\"../parameters/plotparam.json\", \"r\"))\n",
    "plotparam_analysis = json.load(open(\"../parameters/plotparam_analysis.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network weighting by tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_lts = json.load(open(\"../parameters/tag_lts.json\", \"r\"))\n",
    "distance_cost = json.load(open(\"../parameters/distance_cost.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cities\n",
    "cities = utils.load_cities(PATH, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betweenness \n",
    "betweenness_results = {}\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    betweenness_results[scenario] = {}\n",
    "    for placeid in cities:\n",
    "        filename = (PATH[\"results\"] + placeid + \"/\" + scenario + \"/\" + f\"{placeid}_poi_{params['poi_source']}_betweenness_weighted_\" + scenario + \".pickle\")\n",
    "        abs_path = os.path.abspath(filename)\n",
    "        if os.path.exists(abs_path):\n",
    "            with open(abs_path, \"rb\") as f:\n",
    "                betweenness_results[scenario][placeid] = pickle.load(f)\n",
    "        else:\n",
    "            print(f\"File {abs_path} does not exist.\")\n",
    "            print(\"Please run the betweenness analysis first.\")\n",
    "            print(f\"No betweenness files found for {placeid} in scenario {scenario}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random (many runs to get a distribution)\n",
    "random_results = {}\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    random_results[scenario] = {}\n",
    "    for placeid in cities:\n",
    "        pattern = (PATH[\"results\"] + placeid + \"/\" + scenario + \"/\" +\n",
    "                   f\"{placeid}_poi_{params['poi_source']}_random_weighted_{scenario}_run*.pickle\")\n",
    "        random_files = sorted(glob.glob(os.path.abspath(pattern)))[:3]  # only take the first 3 whilst we debug :D\n",
    "        if random_files:\n",
    "            random_results[scenario][placeid] = []\n",
    "            for fn in random_files:\n",
    "                abs_path = os.path.abspath(fn)\n",
    "                with open(abs_path, \"rb\") as f:\n",
    "                    res = pickle.load(f)\n",
    "                random_results[scenario][placeid].append(res)\n",
    "        else:\n",
    "            print(f\"No random files found for {placeid} in scenario {scenario}.\")\n",
    "            print(\"Please run the random growth analysis first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand\n",
    "demand_results = {}\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    demand_results[scenario] = {}\n",
    "    for placeid in cities:\n",
    "        filename = (PATH[\"results\"] + placeid + \"/\" + scenario + \"/\" + f\"{placeid}_poi_{params['poi_source']}_demand_weighted_\" + scenario + \".pickle\")\n",
    "        abs_path = os.path.abspath(filename)\n",
    "        if os.path.exists(abs_path):\n",
    "            with open(abs_path, \"rb\") as f:\n",
    "                demand_results[scenario][placeid] = pickle.load(f)\n",
    "        else:\n",
    "            print(f\"File {abs_path} does not exist.\")\n",
    "            print(\"Please run the demand analysis first.\")\n",
    "            print(f\"No demand files found for {placeid} in scenario {scenario}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand LTN priority\n",
    "demand_ltn_priority_results = {}\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    demand_ltn_priority_results[scenario] = {}\n",
    "    for placeid in cities:\n",
    "        filename = (PATH[\"results\"] + placeid + \"/\" + scenario + \"/\" + f\"{placeid}_poi_{params['poi_source']}_demand_ltn_priority_weighted_\" + scenario + \".pickle\")\n",
    "        abs_path = os.path.abspath(filename)\n",
    "        if os.path.exists(abs_path):\n",
    "            with open(abs_path, \"rb\") as f:\n",
    "                demand_ltn_priority_results[scenario][placeid] = pickle.load(f)\n",
    "        else:\n",
    "            print(f\"File {abs_path} does not exist.\")\n",
    "            print(\"Please run the demand LTN priority analysis first.\")\n",
    "            print(f\"No demand LTN priority files found for {placeid} in scenario {scenario}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betweenness LTN priority\n",
    "betweenness_ltn_priority_results = {}\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    betweenness_ltn_priority_results[scenario] = {}\n",
    "    for placeid in cities:\n",
    "        filename = (PATH[\"results\"] + placeid + \"/\" + scenario + \"/\" + f\"{placeid}_poi_{params['poi_source']}_betweenness_ltn_priority_weighted_\" + scenario + \".pickle\")\n",
    "        abs_path = os.path.abspath(filename)\n",
    "        if os.path.exists(abs_path):\n",
    "            with open(abs_path, \"rb\") as f:\n",
    "                betweenness_ltn_priority_results[scenario][placeid] = pickle.load(f)\n",
    "        else:\n",
    "            print(f\"File {abs_path} does not exist.\")\n",
    "            print(\"Please run the betweenness LTN priority analysis first.\")\n",
    "            print(f\"No betweenness LTN priority files found for {placeid} in scenario {scenario}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find investment level, split results into GTs, GT_abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario_name in params[\"scenarios\"]:\n",
    "    for placeid in cities:\n",
    "\n",
    "        # Demand \n",
    "        if placeid in demand_results.get(scenario_name, {}):\n",
    "            demand_dict = demand_results[scenario_name][placeid]\n",
    "            investment_levels_demand = demand_dict[\"prune_quantiles\"]\n",
    "            GTs_demand               = demand_dict[\"GTs\"]\n",
    "            GT_abstracts_demand      = demand_dict[\"GT_abstracts\"]\n",
    "        else:\n",
    "            print(f\"No demand results for {placeid} in scenario '{scenario_name}'\")\n",
    "            investment_levels_demand = []\n",
    "            GTs_demand               = []\n",
    "            GT_abstracts_demand      = []\n",
    "\n",
    "\n",
    "        # Betweenness‐LTN‐priority \n",
    "        if placeid in betweenness_ltn_priority_results.get(scenario_name, {}):\n",
    "            betweenness_ltn_dict = betweenness_ltn_priority_results[scenario_name][placeid]\n",
    "            investment_levels_betw = betweenness_ltn_dict[\"prune_quantiles\"]\n",
    "            GTs_betw               = betweenness_ltn_dict[\"GTs\"]\n",
    "            GT_abstracts_betw      = betweenness_ltn_dict[\"GT_abstracts\"]\n",
    "        else:\n",
    "            # e.g. scenario == \"no_ltn_scenario\" has no betweenness‐LTN‐priority data\n",
    "            investment_levels_betw = []\n",
    "            GTs_betw               = []\n",
    "            GT_abstracts_betw      = []\n",
    "\n",
    "        # Betweenness\n",
    "        if placeid in betweenness_results.get(scenario_name, {}):\n",
    "            betweenness_dict = betweenness_results[scenario_name][placeid]\n",
    "            investment_levels_betweenness = betweenness_dict[\"prune_quantiles\"]\n",
    "            GTs_betweenness               = betweenness_dict[\"GTs\"]\n",
    "            GT_abstracts_betweenness      = betweenness_dict[\"GT_abstracts\"]\n",
    "        else:\n",
    "            investment_levels_betweenness = []\n",
    "            GTs_betweenness               = []\n",
    "            GT_abstracts_betweenness      = []\n",
    "\n",
    "        # Demand‐LTN‐priority \n",
    "        if placeid in demand_ltn_priority_results.get(scenario_name, {}):\n",
    "            dem_ltn_dict = demand_ltn_priority_results[scenario_name][placeid]\n",
    "            investment_levels_dem_ltn = dem_ltn_dict[\"prune_quantiles\"]\n",
    "            GTs_dem_ltn               = dem_ltn_dict[\"GTs\"]\n",
    "            GT_abstracts_dem_ltn      = dem_ltn_dict[\"GT_abstracts\"]\n",
    "        else:\n",
    "            investment_levels_dem_ltn = []\n",
    "            GTs_dem_ltn               = []\n",
    "            GT_abstracts_dem_ltn      = []\n",
    "\n",
    "        # Random‐runs (loads all run*.pickle files)\n",
    "        random_runs_list = random_results.get(scenario_name, {}).get(placeid, [])\n",
    "        if random_runs_list:\n",
    "            all_GTs_random       = [run_dict[\"GTs\"]          for run_dict in random_runs_list]\n",
    "            all_GTabs_random      = [run_dict[\"GT_abstracts\"]  for run_dict in random_runs_list]\n",
    "            investment_levels_random = random_runs_list[0][\"prune_quantiles\"]\n",
    "        else:\n",
    "            all_GTs_random          = []\n",
    "            all_GTabs_random         = []\n",
    "            investment_levels_random = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing networks, nodes, GeoDataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: ../../bikenwgrowth_external/data/newcastle\\no_ltn_scenario\\newcastle_tessellation_points.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b8008458\\AppData\\Local\\Temp\\ipykernel_22976\\1656686378.py:84: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  all_neighbourhoods_centroids = all_neighbourhoods.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: ../../bikenwgrowth_external/data/newcastle\\current_ltn_scenario\\newcastle_tessellation_points.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b8008458\\AppData\\Local\\Temp\\ipykernel_22976\\1656686378.py:84: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  all_neighbourhoods_centroids = all_neighbourhoods.geometry.centroid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: ../../bikenwgrowth_external/data/newcastle\\more_ltn_scenario\\newcastle_tessellation_points.gpkg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\b8008458\\AppData\\Local\\Temp\\ipykernel_22976\\1656686378.py:84: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  all_neighbourhoods_centroids = all_neighbourhoods.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "G_biketracks_dict               = {}  # (placeid, scenario) → biketrack graph\n",
    "G_biketrack_no_ltns_dict       = {}  # (placeid, scenario) → biketrack_no_ltn graph\n",
    "G_biketrackcaralls_dict        = {}  # (placeid, scenario) → biketrackcarall graph\n",
    "G_biketrackcarall_edges_dict    = {}  # (placeid, scenario) → GeoDataFrame of biketrackcarall edges\n",
    "boundary_gdfs               = {}  # placeid → boundary GeoDataFrame (same for all scenarios)\n",
    "tess_points_dict            = {}  # (placeid, scenario) → tessellation points GeoDataFrame\n",
    "ltn_points_dict             = {}  # (placeid, scenario) → LTN points GeoDataFrame\n",
    "combined_points_dict        = {}  # (placeid, scenario) → combined points GeoDataFrame\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid, placeinfo in cities.items():\n",
    "        base_folder = os.path.join(PATH[\"data\"], placeid, scenario)\n",
    "\n",
    "        # Load biketrack graph\n",
    "        biketrack_gpkg = os.path.join(base_folder, f\"{placeid}_biketrack.gpkg\")\n",
    "        if os.path.exists(biketrack_gpkg):\n",
    "            G_biketrack = utils.ox_gpkg_to_graph(biketrack_gpkg)\n",
    "            G_biketrack.remove_nodes_from(list(nx.isolates(G_biketrack)))\n",
    "            G_biketracks_dict[(placeid, scenario)] = G_biketrack\n",
    "        else:\n",
    "            print(f\"Missing: {biketrack_gpkg}\")\n",
    "            G_biketracks_dict[(placeid, scenario)] = None\n",
    "\n",
    "        # Load biketrack_no_ltn graph\n",
    "        biketrack_no_ltn_gpkg = os.path.join(base_folder, f\"{placeid}_biketrack_no_ltn.gpkg\")\n",
    "        if os.path.exists(biketrack_no_ltn_gpkg):\n",
    "            G_no_ltn = utils.ox_gpkg_to_graph(biketrack_no_ltn_gpkg)\n",
    "            G_no_ltn.remove_nodes_from(list(nx.isolates(G_no_ltn)))\n",
    "            G_biketrack_no_ltns_dict[(placeid, scenario)] = G_no_ltn\n",
    "        else:\n",
    "            print(f\"Missing: {biketrack_no_ltn_gpkg}\")\n",
    "            G_biketrack_no_ltns_dict[(placeid, scenario)] = None\n",
    "\n",
    "        # Load biketrackcarall graph\n",
    "        biketrackcarall_gpkg = os.path.join(base_folder, f\"{placeid}_biketrackcarall.gpkg\")\n",
    "        if os.path.exists(biketrackcarall_gpkg):\n",
    "            G_carall = utils.ox_gpkg_to_graph(biketrackcarall_gpkg)\n",
    "            G_carall.remove_nodes_from(list(nx.isolates(G_carall)))\n",
    "            G_biketrackcaralls_dict[(placeid, scenario)] = G_carall\n",
    "\n",
    "            # also store edges GeoDataFrame\n",
    "            edges_gdf = ox.graph_to_gdfs(G_carall, nodes=False)\n",
    "            G_biketrackcarall_edges_dict[(placeid, scenario)] = edges_gdf\n",
    "        else:\n",
    "            print(f\"Missing: {biketrackcarall_gpkg}\")\n",
    "            G_biketrackcaralls_dict[(placeid, scenario)] = None\n",
    "            G_biketrackcarall_edges_dict[(placeid, scenario)] = None\n",
    "\n",
    "        #  Load boundary once per placeid (it won’t change by scenario)\n",
    "        if placeid not in boundary_gdfs:\n",
    "            boundary_gdf = ox.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "            boundary_gdfs[placeid] = boundary_gdf\n",
    "\n",
    "        # get nodes\n",
    "        tess_points_gpkg = os.path.join(base_folder, f\"{placeid}_tessellation_points.gpkg\")\n",
    "        if os.path.exists(tess_points_gpkg):\n",
    "            tess_points = gpd.read_file(tess_points_gpkg)\n",
    "            tess_points_dict[(placeid, scenario)] = tess_points\n",
    "        else:\n",
    "            print(f\"Missing: {tess_points_gpkg}\")\n",
    "            tess_points_dict[(placeid, scenario)] = None\n",
    "        \n",
    "        # get ltn points\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            ltn_points_gpkg = os.path.join(base_folder, f\"{placeid}_ltn_points.gpkg\")\n",
    "            if os.path.exists(ltn_points_gpkg):\n",
    "                ltn_points = gpd.read_file(ltn_points_gpkg)\n",
    "                ltn_points_dict[(placeid, scenario)] = ltn_points\n",
    "            else:\n",
    "                print(f\"Missing: {ltn_points_gpkg}\")\n",
    "                ltn_points_dict[(placeid, scenario)] = None\n",
    "        \n",
    "        # get combined points\n",
    "        combined_points_gpkg = os.path.join(base_folder, f\"{placeid}_combined_points.gpkg\")\n",
    "        if os.path.exists(combined_points_gpkg):\n",
    "            combined_points = gpd.read_file(combined_points_gpkg)\n",
    "            combined_points_dict[(placeid, scenario)] = combined_points\n",
    "        else:\n",
    "            print(f\"Missing: {combined_points_gpkg}\")\n",
    "            combined_points_dict[(placeid, scenario)] = None\n",
    "\n",
    "        # get all neighbourhoods (ragardless of their low traffic status. This doesn't change by scenario)\n",
    "        all_neighbourhoods = gpd.read_file(PATH[\"data\"] + placeid + \"/\" + 'neighbourhoods_'+  placeid + '.gpkg')\n",
    "        all_neighbourhoods_centroids = all_neighbourhoods.geometry.centroid\n",
    "        all_neighbourhoods_centroids = gpd.GeoDataFrame(geometry= all_neighbourhoods_centroids, crs=all_neighbourhoods.crs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "def csv_to_ox(p, placeid, parameterid):\n",
    "    '''\n",
    "    Load graph from csv files (nodes and edge)\n",
    "    Include OSMID, length, highway, x, y attributes\n",
    "    '''\n",
    "\n",
    "    prefix = placeid + '_' + parameterid\n",
    "    compress = utils.check_extract_zip(p, prefix)\n",
    "    \n",
    "    with open(p + prefix + '_edges.csv', 'r') as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "        lines = []\n",
    "        for line in csv.reader(f, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):\n",
    "            line_list = [c for c in line]\n",
    "            osmid = str(eval(line_list[header.index(\"osmid\")])[0]) if isinstance(eval(line_list[header.index(\"osmid\")]), list) else line_list[header.index(\"osmid\")]\n",
    "            length = str(eval(line_list[header.index(\"length\")])[0]) if isinstance(eval(line_list[header.index(\"length\")]), list) else line_list[header.index(\"length\")]\n",
    "            highway = line_list[header.index(\"highway\")]\n",
    "            if highway.startswith(\"[\") and highway.endswith(\"]\"):\n",
    "                highway = highway.strip(\"[]\").split(\",\")[0].strip(\" '\")\n",
    "            line_string = f\"{line_list[header.index('u')]} {line_list[header.index('v')]} {osmid} {length} {highway}\"\n",
    "            lines.append(line_string)\n",
    "        G = nx.parse_edgelist(lines, nodetype=int, data=((\"osmid\", int), (\"length\", float), (\"highway\", str)), create_using=nx.MultiDiGraph)\n",
    "    \n",
    "    with open(p + prefix + '_nodes.csv', 'r') as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "        values_x = {}\n",
    "        values_y = {}\n",
    "        for line in csv.reader(f, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):\n",
    "            line_list = [c for c in line]\n",
    "            osmid = int(line_list[header.index(\"osmid\")])\n",
    "            values_x[osmid] = float(line_list[header.index(\"x\")])\n",
    "            values_y[osmid] = float(line_list[header.index(\"y\")])\n",
    "        nx.set_node_attributes(G, values_x, \"x\")\n",
    "        nx.set_node_attributes(G, values_y, \"y\")\n",
    "    \n",
    "    if compress:\n",
    "        os.remove(p + prefix + '_nodes.csv')\n",
    "        os.remove(p + prefix + '_edges.csv')\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis saving setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_res_pickle_paths = {}  \n",
    "analysis_res_json_paths    = {}  \n",
    "analysis_results          = {}\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    analysis_res_pickle_paths[scenario] = os.path.join(PATH[\"results\"] + placeid + \"/\" + scenario + \"/\" + f\"{placeid}_{scenario}_analysis_results.pickle\")\n",
    "    analysis_res_json_paths[scenario] = os.path.join(PATH[\"results\"], placeid + \"/\" + scenario + \"/\" + f\"{placeid}_{scenario}_analysis_results.json\")\n",
    "    analysis_results[scenario] = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelimiary Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length - finding the distance of the connected network, along with the investment distance (length - existing infrastructure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed newcastle - no_ltn_scenario\n",
      "Completed newcastle - current_ltn_scenario\n",
      "Completed newcastle - more_ltn_scenario\n"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    G_biketrack        = G_biketracks_dict.get((placeid, scenario))\n",
    "    G_biketrack_no_ltn = G_biketrack_no_ltns_dict.get((placeid, scenario))\n",
    "    GTs                = demand_results.get(scenario, {}).get(placeid, {}).get(\"GTs\", [])\n",
    "\n",
    "    if not (G_biketrack and G_biketrack_no_ltn and GTs):\n",
    "        print(f\"Missing data for {placeid} - {scenario}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # File paths\n",
    "    analysis_res_pickle = os.path.join(PATH[\"results\"], placeid, scenario, f\"{placeid}_{scenario}_analysis_results.pickle\")\n",
    "    analysis_res_csv    = os.path.join(PATH[\"results\"], placeid, scenario, f\"{placeid}_{scenario}_analysis_results.csv\")\n",
    "    output_path         = os.path.join(PATH[\"plots\"], placeid, scenario, \"allLengths.png\")\n",
    "\n",
    "    # Load existing results\n",
    "    if os.path.exists(analysis_res_pickle):\n",
    "        with open(analysis_res_pickle, 'rb') as f:\n",
    "            analysis_results[scenario] = pickle.load(f)\n",
    "    else:\n",
    "        analysis_results[scenario] = {}\n",
    "\n",
    "    # Calculations\n",
    "    total_biketrack        = sum(nx.get_edge_attributes(G_biketrack, 'length').values())\n",
    "    total_biketrack_no_ltn = sum(nx.get_edge_attributes(G_biketrack_no_ltn, 'length').values())\n",
    "    total_network          = sum(nx.get_edge_attributes(GTs[-1], 'length').values())\n",
    "    investment_length      = sum(\n",
    "        data.get('length', 0) * distance_cost.get(data.get('highway', 'unclassified'), 1)\n",
    "        for _, _, data in GTs[-1].edges(data=True))\n",
    "\n",
    "    length_stats = {'length_comparison_labels': [ \"Existing Cycle Infrastructure (Including LTNs)\", \"Existing Cycle Infrastructure (Excluding LTNs)\", \"LTNs\", \"Fully Connected Cycle Network\", \"Investment Distance\"],\n",
    "        'length_comparison_values': [total_biketrack, total_biketrack_no_ltn, abs(total_biketrack - total_biketrack_no_ltn), total_network, investment_length],\n",
    "        'length_comparison_colors': ['deepskyblue'] * 5,\n",
    "        'total_network_length': total_network,\n",
    "        'total_biketrack_length': total_biketrack,\n",
    "        'total_biketrack_no_ltn_length': total_biketrack_no_ltn,\n",
    "        'length_difference': abs(total_biketrack - total_biketrack_no_ltn),\n",
    "        'total_investment_length': investment_length}\n",
    "\n",
    "    # Save to pickle & CSV\n",
    "    analysis_results[scenario].update(length_stats)\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results[scenario], f)\n",
    "    analysis_res_json = os.path.join(PATH[\"results\"], placeid, scenario, f\"{placeid}_{scenario}_analysis_results.json\")\n",
    "    with open(analysis_res_json, 'w') as f:\n",
    "        json.dump(analysis_results[scenario], f, indent=2)\n",
    "    # removed csv - can't take columns with different lengths\n",
    "    #pd.DataFrame({k: [v] for k, v in analysis_results[scenario].items()}).to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(\n",
    "        analysis_results[scenario]['length_comparison_labels'],\n",
    "        analysis_results[scenario]['length_comparison_values'],\n",
    "        color=analysis_results[scenario]['length_comparison_colors']\n",
    "    )\n",
    "    plt.xlabel('Network Type')\n",
    "    plt.ylabel('Total Length (meters)')\n",
    "    plt.title(f'{placeid} - {scenario} - Lengths of Cycle Networks')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    ltn_difference = abs(total_biketrack - total_biketrack_no_ltn)\n",
    "    labels = [\"Total Cycle Infrastructure\", \"Protected Cycle Infrastructure\",\"LTNs\"]\n",
    "    values = [total_biketrack, total_biketrack_no_ltn, ltn_difference]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(labels, values, color=['deepskyblue'] * 3)\n",
    "    plt.xlabel('Network Type')\n",
    "    plt.ylabel('Total Length (meters)')\n",
    "    plt.title(f'{placeid} - {scenario} - Total Lengths of Cycle Infrastructure')\n",
    "    plt.tight_layout()\n",
    "    output_path_total = os.path.join(PATH[\"plots\"], placeid, scenario, \"TotalLengthsCycleNet.png\")\n",
    "    plt.savefig(output_path_total, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Completed {placeid} - {scenario}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pure length - how is the budget used per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated analysis results for no_ltn_scenario in newcastle\n",
      "Plots saved for newcastle - no_ltn_scenario\n",
      "Updated analysis results for current_ltn_scenario in newcastle\n",
      "Plots saved for newcastle - current_ltn_scenario\n",
      "Updated analysis results for more_ltn_scenario in newcastle\n",
      "Plots saved for newcastle - more_ltn_scenario\n"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    # load scenario-specific results and data\n",
    "    analysis_res_pickle = analysis_res_pickle_paths[scenario]\n",
    "    analysis_res_json = analysis_res_json_paths[scenario]\n",
    "    analysis_results[scenario] = utils.load_results(analysis_res_pickle)\n",
    "    GTs_betweenness = betweenness_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    GTs_demand = demand_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        GTs_demand_ltn_priority = demand_ltn_priority_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "        GTs_betweenness_ltn_priority = betweenness_ltn_priority_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    random_runs = random_results[scenario].get(placeid, [])\n",
    "\n",
    "    if rerun or 'total_lengths' not in analysis_results[scenario]:\n",
    "        results_list = []\n",
    "        total_lengths_betweenness = utils.compute_total_lengths(GTs_betweenness)\n",
    "        total_lengths_demand = utils.compute_total_lengths(GTs_demand)\n",
    "        total_lengths_random_runs = [utils.compute_total_lengths(run[\"GTs\"]) for run in random_runs]\n",
    "        total_lengths_random_mean = np.mean(total_lengths_random_runs, axis=0).tolist()\n",
    "\n",
    "        # save results\n",
    "        results_list.append((\"Betweenness Growth - Total Length\", total_lengths_betweenness))\n",
    "        results_list.append((\"Demand Growth - Total Length\", total_lengths_demand))\n",
    "        for i, run_lengths in enumerate(total_lengths_random_runs):\n",
    "            results_list.append((f\"Random Run {i+1} - Total Length\", run_lengths))\n",
    "        results_list.append((\"Random Growth (mean) - Total Length\", total_lengths_random_mean))\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            total_lengths_demand_ltn_priority = utils.compute_total_lengths(GTs_demand_ltn_priority)\n",
    "            total_lengths_betweenness_ltn_priority = utils.compute_total_lengths(GTs_betweenness_ltn_priority)\n",
    "            results_list.append((\"Demand LTN Priority Growth - Total Length\", total_lengths_demand_ltn_priority))\n",
    "            results_list.append((\"Betweenness LTN Priority Growth - Total Length\", total_lengths_betweenness_ltn_priority))\n",
    "        utils.save_results(results_list, analysis_res_pickle, analysis_res_json)\n",
    "        analysis_results[scenario] = {label: data for label, data in results_list}\n",
    "        print(f\"Updated analysis results for {scenario} in {placeid}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(1, 100):\n",
    "        key = f\"Random Run {i} - Total Length\"\n",
    "        if key in analysis_results[scenario]:\n",
    "            plt.plot(analysis_results[scenario][key], color='lightgray', linewidth=1, alpha=0.5)\n",
    "        else:\n",
    "            break\n",
    "    plt.plot(analysis_results[scenario]['Random Growth (mean) - Total Length'], linestyle='--', linewidth=2, label='Random Growth (mean)')\n",
    "    plt.plot(analysis_results[scenario]['Betweenness Growth - Total Length'], '-', label='Betweenness Growth', color='orange')\n",
    "    plt.plot(analysis_results[scenario]['Demand Growth - Total Length'], '-.', label='Demand Growth', color='red')\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        plt.plot(analysis_results[scenario]['Demand LTN Priority Growth - Total Length'], ':', label='Demand LTN Priority Growth', color='green')\n",
    "        plt.plot(analysis_results[scenario]['Betweenness LTN Priority Growth - Total Length'], '-', label='Betweenness LTN Priority Growth', color='purple')\n",
    "    plt.xlabel('Investment Iteration')\n",
    "    plt.ylabel('Total Length (meters)')\n",
    "    plt.title(f'Length of Invested Cycle Network for {scenario} - {placeid}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(PATH[\"plots\"], placeid, scenario, \"L_of_Investment.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Plots saved for {placeid} - {scenario}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deviation from random - pure length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved absolute deviation results for no_ltn_scenario in newcastle\n",
      "Saved absolute deviation results for current_ltn_scenario in newcastle\n",
      "Saved absolute deviation results for more_ltn_scenario in newcastle\n"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    analysis_res_pickle = analysis_res_pickle_paths[scenario]\n",
    "    analysis_res_json = analysis_res_json_paths[scenario]\n",
    "    analysis_results[scenario] = utils.load_results(analysis_res_pickle)\n",
    "\n",
    "    if rerun or not any(k.endswith(\"Deviation from Random\") for k in analysis_results[scenario]):\n",
    "        baseline = analysis_results[scenario]['Random Growth (mean) - Total Length']\n",
    "        results_list = []\n",
    "        # Calculate deviation from random baseline\n",
    "        results_list.append((\n",
    "            \"Betweenness Growth - Total Length Deviation from Random\",\n",
    "            utils.compute_abs_deviation(analysis_results[scenario][\"Betweenness Growth - Total Length\"], baseline)))\n",
    "        results_list.append((\n",
    "            \"Demand Growth - Total Length Deviation from Random\",\n",
    "            utils.compute_abs_deviation(analysis_results[scenario][\"Demand Growth - Total Length\"], baseline)))\n",
    "\n",
    "        # Calculate mean deviation for random runs\n",
    "        random_runs_keys = [key for key in analysis_results[scenario] if key.startswith(\"Random Run\") and \"Total Length\" in key]\n",
    "        random_runs = [analysis_results[scenario][key] for key in random_runs_keys]\n",
    "        random_runs_deviations = [utils.compute_abs_deviation(run, baseline) for run in random_runs]\n",
    "        random_deviations_mean = np.mean(random_runs_deviations, axis=0).tolist()\n",
    "        results_list.append((\"Random Growth (mean) - Total Length Deviation from Random\", random_deviations_mean))\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            results_list.append((\"Demand LTN Priority Growth - Total Length Deviation from Random\",\n",
    "                utils.compute_abs_deviation(analysis_results[scenario][\"Demand LTN Priority Growth - Total Length\"], baseline)))\n",
    "            results_list.append((\"Betweenness LTN Priority Growth - Total Length Deviation from Random\",\n",
    "                utils.compute_abs_deviation(analysis_results[scenario][\"Betweenness LTN Priority Growth - Total Length\"], baseline)))\n",
    "\n",
    "        # Add random runs deviations\n",
    "        for i, dev in enumerate(random_runs_deviations):\n",
    "            results_list.append((f\"Random Run {i+1} - Total Length Deviation from Random\", dev))\n",
    "\n",
    "        # Save all results as list of (label, data)\n",
    "        utils.save_results(results_list, analysis_res_pickle, analysis_res_json)\n",
    "        analysis_results[scenario] = {label: data for label, data in results_list}\n",
    "        print(f\"Saved absolute deviation results for {scenario} in {placeid}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for key in analysis_results[scenario]:\n",
    "        if key.startswith(\"Random Run\") and \"Deviation from Random\" in key:\n",
    "            plt.plot(analysis_results[scenario][key], color='lightgray', linewidth=1, alpha=0.4)\n",
    "    plt.axhline(0, color='blue', linestyle='--', linewidth=2, label='Random Growth (mean)')\n",
    "    plot_lines = [\n",
    "        (\"Betweenness Growth - Total Length Deviation from Random\", '-', 'orange', 'Betweenness Growth'),\n",
    "        (\"Demand Growth - Total Length Deviation from Random\", '-.', 'red', 'Demand Growth'),]\n",
    "    if scenario != \"no_ltn_scenario\": plot_lines += [ (\"Demand LTN Priority Growth - Total Length Deviation from Random\", ':', 'green', 'Demand LTN Priority Growth'), (\"Betweenness LTN Priority Growth - Total Length Deviation from Random\", '-', 'purple', 'Betweenness LTN Priority Growth'),]\n",
    "    for key, ls, color, label in plot_lines:\n",
    "        plt.plot(analysis_results[scenario][key], linestyle=ls, color=color, label=label)\n",
    "    plt.xlabel('Investment Iteration')\n",
    "    plt.ylabel('Deviation from Random Growth Baseline (meters)')\n",
    "    plt.title(f'Deviation from Random Growth Baseline for {scenario} - {placeid}')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(PATH[\"plots\"], placeid, scenario, \"abs_dev_from_random_length.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Actual\" investment length - how much do we actually need to use to close gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate length, minus the existing infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find how much we actually need to invest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated investment cost results for no_ltn_scenario in newcastle\n",
      "Updated investment cost results for current_ltn_scenario in newcastle\n",
      "Updated investment cost results for more_ltn_scenario in newcastle\n"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    # Load results\n",
    "    analysis_res_pickle = analysis_res_pickle_paths[scenario]\n",
    "    analysis_res_json = analysis_res_json_paths[scenario]\n",
    "    analysis_results[scenario] = utils.load_results(analysis_res_pickle)\n",
    "\n",
    "    GTs_betweenness = betweenness_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    GTs_demand = demand_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        GTs_demand_ltn_priority = demand_ltn_priority_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "        GTs_betweenness_ltn_priority = betweenness_ltn_priority_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    random_runs = random_results[scenario].get(placeid, [])\n",
    "\n",
    "    if rerun or \"Betweenness Growth - Total Investment Length\" not in analysis_results[scenario]:\n",
    "        results_list = []\n",
    "\n",
    "        total_investment_betweenness = utils.compute_total_investment_lengths(GTs_betweenness, distance_cost)\n",
    "        total_investment_demand = utils.compute_total_investment_lengths(GTs_demand, distance_cost)\n",
    "        random_runs_investments = [utils.compute_total_investment_lengths(run[\"GTs\"], distance_cost) for run in random_runs]\n",
    "        random_investment_mean = np.mean(random_runs_investments, axis=0).tolist()\n",
    "\n",
    "        results_list.append((\"Betweenness Growth - Total Investment Length\", total_investment_betweenness))\n",
    "        results_list.append((\"Demand Growth - Total Investment Length\", total_investment_demand))\n",
    "        for i, run_lengths in enumerate(random_runs_investments):\n",
    "            results_list.append((f\"Random Run {i+1} - Total Investment Length\", run_lengths))\n",
    "        results_list.append((\"Random Growth (mean) - Total Investment Length\", random_investment_mean))\n",
    "\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            total_investment_demand_ltn_priority = utils.compute_total_investment_lengths(GTs_demand_ltn_priority, distance_cost)\n",
    "            total_investment_betweenness_ltn_priority = utils.compute_total_investment_lengths(GTs_betweenness_ltn_priority, distance_cost)\n",
    "            results_list.append((\"Demand LTN Priority Growth - Total Investment Length\", total_investment_demand_ltn_priority))\n",
    "            results_list.append((\"Betweenness LTN Priority Growth - Total Investment Length\", total_investment_betweenness_ltn_priority))\n",
    "\n",
    "        utils.save_results(results_list, analysis_res_pickle, analysis_res_json)\n",
    "        analysis_results[scenario] = {label: data for label, data in results_list}\n",
    "        print(f\"Updated investment cost results for {scenario} in {placeid}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(1, 100):\n",
    "        key = f\"Random Run {i} - Total Investment Length\"\n",
    "        if key in analysis_results[scenario]:\n",
    "            plt.plot(analysis_results[scenario][key], color='lightgray', linewidth=1, alpha=0.5)\n",
    "        else:\n",
    "            break\n",
    "    plt.plot(analysis_results[scenario][\"Random Growth (mean) - Total Investment Length\"], linestyle='--', linewidth=2, color='blue', label='Random Growth (mean)')\n",
    "    plt.plot(analysis_results[scenario][\"Betweenness Growth - Total Investment Length\"], '-', color='orange', label='Betweenness Growth')\n",
    "    plt.plot(analysis_results[scenario][\"Demand Growth - Total Investment Length\"], '-.', color='red', label='Demand Growth')\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        plt.plot(analysis_results[scenario][\"Demand LTN Priority Growth - Total Investment Length\"], ':', color='green', label='Demand LTN Priority Growth')\n",
    "        plt.plot(analysis_results[scenario][\"Betweenness LTN Priority Growth - Total Investment Length\"], '-', color='purple', label='Betweenness LTN Priority Growth')\n",
    "\n",
    "    plt.xlabel('Investment Iteration')\n",
    "    plt.ylabel('Total Investment Cost (Meters)')\n",
    "    plt.title(f'Total Investment Cost per Growth Strategy for {scenario} - {placeid}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = os.path.join(PATH[\"plots\"], placeid, scenario, \"total_investment_cost.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved deviation-from-random investment cost results for no_ltn_scenario in newcastle\n",
      "Saved deviation-from-random investment cost results for current_ltn_scenario in newcastle\n",
      "Saved deviation-from-random investment cost results for more_ltn_scenario in newcastle\n"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    analysis_res_pickle = analysis_res_pickle_paths[scenario]\n",
    "    analysis_res_json = analysis_res_json_paths[scenario]\n",
    "    analysis_results[scenario] = utils.load_results(analysis_res_pickle)\n",
    "\n",
    "    if rerun or not any(k.endswith(\"Deviation from Random - Total Investment Length\") for k in analysis_results[scenario]):\n",
    "        baseline = analysis_results[scenario][\"Random Growth (mean) - Total Investment Length\"]\n",
    "        results_list = []\n",
    "\n",
    "        # Compute deviations from random baseline\n",
    "        results_list.append((\"Betweenness Growth - Deviation from Random - Total Investment Length\",\n",
    "            utils.compute_abs_deviation(analysis_results[scenario][\"Betweenness Growth - Total Investment Length\"], baseline)))\n",
    "        results_list.append((\"Demand Growth - Deviation from Random - Total Investment Length\",\n",
    "            utils.compute_abs_deviation(analysis_results[scenario][\"Demand Growth - Total Investment Length\"], baseline)))\n",
    "        random_keys = [key for key in analysis_results[scenario] if key.startswith(\"Random Run\") and \"Total Investment Length\" in key]\n",
    "        random_runs = [analysis_results[scenario][key] for key in random_keys]\n",
    "        random_deviations = [utils.compute_abs_deviation(run, baseline) for run in random_runs]\n",
    "        mean_random_dev = np.mean(random_deviations, axis=0).tolist()\n",
    "        results_list.append((\"Random Growth (mean) - Deviation from Random - Total Investment Length\", mean_random_dev))\n",
    "\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            results_list.append((\n",
    "                \"Demand LTN Priority Growth - Deviation from Random - Total Investment Length\",\n",
    "                utils.compute_abs_deviation(analysis_results[scenario][\"Demand LTN Priority Growth - Total Investment Length\"], baseline)))\n",
    "            results_list.append((\n",
    "                \"Betweenness LTN Priority Growth - Deviation from Random - Total Investment Length\",\n",
    "                utils.compute_abs_deviation(analysis_results[scenario][\"Betweenness LTN Priority Growth - Total Investment Length\"], baseline)))\n",
    "        for i, dev in enumerate(random_deviations):\n",
    "            results_list.append((f\"Random Run {i+1} - Deviation from Random - Total Investment Length\", dev))\n",
    "        utils.save_results(results_list, analysis_res_pickle, analysis_res_json)\n",
    "        analysis_results[scenario].update({label: data for label, data in results_list})\n",
    "        print(f\"Saved deviation-from-random investment cost results for {scenario} in {placeid}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for key in analysis_results[scenario]:\n",
    "        if key.startswith(\"Random Run\") and \"Deviation from Random - Total Investment Length\" in key:\n",
    "            plt.plot(analysis_results[scenario][key], color='lightgray', linewidth=1, alpha=0.4)\n",
    "    plt.axhline(0, color='blue', linestyle='--', linewidth=2, label='Random Growth (mean)')\n",
    "\n",
    "    plot_lines = [(\"Betweenness Growth - Deviation from Random - Total Investment Length\", '-', 'orange', 'Betweenness Growth'),\n",
    "        (\"Demand Growth - Deviation from Random - Total Investment Length\", '-.', 'red', 'Demand Growth'),]\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        plot_lines += [ (\"Demand LTN Priority Growth - Deviation from Random - Total Investment Length\", ':', 'green', 'Demand LTN Priority Growth'),\n",
    "                       (\"Betweenness LTN Priority Growth - Deviation from Random - Total Investment Length\", '-', 'purple', 'Betweenness LTN Priority Growth')]\n",
    "\n",
    "    for key, linestyle, color, label in plot_lines:\n",
    "        plt.plot(analysis_results[scenario][key], linestyle=linestyle, color=color, label=label)\n",
    "\n",
    "    plt.xlabel('Investment Iteration')\n",
    "    plt.ylabel('Deviation from Random Growth Baseline (meters × weight)')\n",
    "    plt.title(f'Deviation from Random Growth Baseline (Total Investment Cost) for {scenario} - {placeid}')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = os.path.join(PATH[\"plots\"], placeid, scenario, \"abs_dev_from_random_investment_cost.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find comparison between how much we need against full route lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find differance between network size and required investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved length difference results for no_ltn_scenario in newcastle\n",
      "Saved length difference results for current_ltn_scenario in newcastle\n",
      "Saved length difference results for more_ltn_scenario in newcastle\n"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    analysis_res_pickle = analysis_res_pickle_paths[scenario]\n",
    "    analysis_res_json = analysis_res_json_paths[scenario]\n",
    "    analysis_results[scenario] = utils.load_results(analysis_res_pickle)\n",
    "\n",
    "    GTs_betweenness = betweenness_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    GTs_demand = demand_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        GTs_demand_ltn_priority = demand_ltn_priority_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "        GTs_betweenness_ltn_priority = betweenness_ltn_priority_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    random_runs = random_results[scenario].get(placeid, [])\n",
    "\n",
    "    if rerun or \"Betweenness Growth - Length Difference\" not in analysis_results[scenario]:\n",
    "        results_list = []\n",
    "\n",
    "        length_diff_betweenness = utils.compute_length_difference(GTs_betweenness)\n",
    "        length_diff_demand = utils.compute_length_difference(GTs_demand)\n",
    "        random_run_differences = [utils.compute_length_difference(run[\"GTs\"]) for run in random_runs]\n",
    "        random_diff_mean = np.mean(random_run_differences, axis=0).tolist()\n",
    "\n",
    "        results_list.append((\"Betweenness Growth - Length Difference\", length_diff_betweenness))\n",
    "        results_list.append((\"Demand Growth - Length Difference\", length_diff_demand))\n",
    "        for i, run_diff in enumerate(random_run_differences):\n",
    "            results_list.append((f\"Random Run {i+1} - Length Difference\", run_diff))\n",
    "        results_list.append((\"Random Growth (mean) - Length Difference\", random_diff_mean))\n",
    "\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            length_diff_demand_ltn = utils.compute_length_difference(GTs_demand_ltn_priority)\n",
    "            length_diff_betweenness_ltn = utils.compute_length_difference(GTs_betweenness_ltn_priority)\n",
    "            results_list.append((\"Demand LTN Priority Growth - Length Difference\", length_diff_demand_ltn))\n",
    "            results_list.append((\"Betweenness LTN Priority Growth - Length Difference\", length_diff_betweenness_ltn))\n",
    "\n",
    "        utils.save_results(results_list, analysis_res_pickle, analysis_res_json)\n",
    "        analysis_results[scenario] = {label: data for label, data in results_list}\n",
    "        print(f\"Saved length difference results for {scenario} in {placeid}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(1, 100):\n",
    "        key = f\"Random Run {i} - Length Difference\"\n",
    "        if key in analysis_results[scenario]:\n",
    "            plt.plot(analysis_results[scenario][key], color='lightgray', linewidth=1, alpha=0.4)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    plt.plot(analysis_results[scenario][\"Random Growth (mean) - Length Difference\"], linestyle='--', linewidth=2, color='blue', label='Random Growth (mean)')\n",
    "    plt.plot(analysis_results[scenario][\"Betweenness Growth - Length Difference\"], '-', color='orange', label='Betweenness Growth')\n",
    "    plt.plot(analysis_results[scenario][\"Demand Growth - Length Difference\"], '-.', color='red', label='Demand Growth')\n",
    "\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        plt.plot(analysis_results[scenario][\"Demand LTN Priority Growth - Length Difference\"], ':', color='green', label='Demand LTN Priority Growth')\n",
    "        plt.plot(analysis_results[scenario][\"Betweenness LTN Priority Growth - Length Difference\"], '-', color='purple', label='Betweenness LTN Priority Growth')\n",
    "\n",
    "    plt.xlabel(\"Investment Iteration\")\n",
    "    plt.ylabel(\"Length Difference (meters)\")\n",
    "    plt.title(f\"Difference Between Total Network Size and Investment Size for {scenario} - {placeid}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = os.path.join(PATH[\"plots\"], placeid, scenario, \"length_difference.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved deviation-from-random length difference results for no_ltn_scenario in newcastle\n",
      "Saved deviation-from-random length difference results for current_ltn_scenario in newcastle\n",
      "Saved deviation-from-random length difference results for more_ltn_scenario in newcastle\n"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    analysis_res_pickle = analysis_res_pickle_paths[scenario]\n",
    "    analysis_res_json = analysis_res_json_paths[scenario]\n",
    "    analysis_results[scenario] = utils.load_results(analysis_res_pickle)\n",
    "\n",
    "    if rerun or \"Betweenness Growth - Deviation from Random Length Difference\" not in analysis_results[scenario]:\n",
    "        results_list = []\n",
    "        baseline = np.array(analysis_results[scenario][\"Random Growth (mean) - Length Difference\"])\n",
    "        deviation_betweenness = (np.array(analysis_results[scenario][\"Betweenness Growth - Length Difference\"]) - baseline).tolist()\n",
    "        deviation_demand = (np.array(analysis_results[scenario][\"Demand Growth - Length Difference\"]) - baseline).tolist()\n",
    "\n",
    "        results_list.append((\"Betweenness Growth - Deviation from Random Length Difference\", deviation_betweenness))\n",
    "        results_list.append((\"Demand Growth - Deviation from Random Length Difference\", deviation_demand))\n",
    "\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            deviation_demand_ltn = (np.array(analysis_results[scenario][\"Demand LTN Priority Growth - Length Difference\"]) - baseline).tolist()\n",
    "            deviation_betweenness_ltn = (np.array(analysis_results[scenario][\"Betweenness LTN Priority Growth - Length Difference\"]) - baseline).tolist()\n",
    "            results_list.append((\"Demand LTN Priority Growth - Deviation from Random Length Difference\", deviation_demand_ltn))\n",
    "            results_list.append((\"Betweenness LTN Priority Growth - Deviation from Random Length Difference\", deviation_betweenness_ltn))\n",
    "\n",
    "        utils.save_results(results_list, analysis_res_pickle, analysis_res_json)\n",
    "        analysis_results[scenario].update({k: v for k, v in results_list})\n",
    "        print(f\"Saved deviation-from-random length difference results for {scenario} in {placeid}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(1, 100):\n",
    "        key = f\"Random Run {i} - Length Difference\"\n",
    "        if key in analysis_results[scenario]:\n",
    "            run_dev = np.array(analysis_results[scenario][key]) - np.array(analysis_results[scenario][\"Random Growth (mean) - Length Difference\"])\n",
    "            plt.plot(run_dev, color='lightgray', linewidth=1, alpha=0.4)\n",
    "        else:\n",
    "            break\n",
    "    plt.axhline(0, color='blue', linestyle='--', linewidth=2, label='Random Growth (mean)')\n",
    "    plt.plot(analysis_results[scenario][\"Betweenness Growth - Deviation from Random Length Difference\"], '-', color='orange', label='Betweenness Growth')\n",
    "    plt.plot(analysis_results[scenario][\"Demand Growth - Deviation from Random Length Difference\"], '-.', color='red', label='Demand Growth')\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        plt.plot(analysis_results[scenario][\"Demand LTN Priority Growth - Deviation from Random Length Difference\"], ':', color='green', label='Demand LTN Priority Growth')\n",
    "        plt.plot(analysis_results[scenario][\"Betweenness LTN Priority Growth - Deviation from Random Length Difference\"], '-', color='purple', label='Betweenness LTN Priority Growth')\n",
    "    plt.xlabel(\"Investment Iteration\")\n",
    "    plt.ylabel(\"Deviation from Random (meters)\")\n",
    "    plt.title(f\"Deviation from Random Growth Strategy for {scenario} - {placeid}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(PATH[\"plots\"], placeid, scenario, \"length_difference_deviation_from_random.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated analysis results for no_ltn_scenario in newcastle\n",
      "Updated analysis results for current_ltn_scenario in newcastle\n",
      "Updated analysis results for more_ltn_scenario in newcastle\n"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    # Load paths and results for this scenario\n",
    "    analysis_res_pickle = analysis_res_pickle_paths[scenario]\n",
    "    analysis_res_json = analysis_res_json_paths[scenario]\n",
    "    analysis_results[scenario] = utils.load_results(analysis_res_pickle)\n",
    "\n",
    "    # Load GTs data per growth strategy\n",
    "    GTs_betweenness = betweenness_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    GTs_demand = demand_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        GTs_demand_ltn_priority = demand_ltn_priority_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "        GTs_betweenness_ltn_priority = betweenness_ltn_priority_results[scenario].get(placeid, {}).get(\"GTs\", [])\n",
    "    random_runs = random_results[scenario].get(placeid, [])\n",
    "\n",
    "    if rerun or 'total_lengths_vs_investment' not in analysis_results[scenario]:\n",
    "        results_list = []\n",
    "\n",
    "        # Compute total lengths and investment lengths for each growth type\n",
    "        lengths_betweenness = utils.compute_total_lengths(GTs_betweenness)\n",
    "        investment_betweenness = utils.compute_total_investment_lengths(GTs_betweenness, distance_cost)\n",
    "        lengths_demand = utils.compute_total_lengths(GTs_demand)\n",
    "        investment_demand = utils.compute_total_investment_lengths(GTs_demand, distance_cost)\n",
    "        random_lengths_runs = [utils.compute_total_lengths(run[\"GTs\"]) for run in random_runs]\n",
    "        random_investment_runs = [utils.compute_total_investment_lengths(run[\"GTs\"], distance_cost) for run in random_runs]\n",
    "        random_lengths_mean = np.mean(random_lengths_runs, axis=0).tolist()\n",
    "        random_investment_mean = np.mean(random_investment_runs, axis=0).tolist()\n",
    "        # Append results \n",
    "        results_list.append((\"Betweenness Growth - Total Length\", lengths_betweenness))\n",
    "        results_list.append((\"Betweenness Growth - Total Investment Length\", investment_betweenness))\n",
    "        results_list.append((\"Demand Growth - Total Length\", lengths_demand))\n",
    "        results_list.append((\"Demand Growth - Total Investment Length\", investment_demand))\n",
    "        for i, (run_lengths, run_investment) in enumerate(zip(random_lengths_runs, random_investment_runs)):\n",
    "            results_list.append((f\"Random Run {i+1} - Total Length\", run_lengths))\n",
    "            results_list.append((f\"Random Run {i+1} - Total Investment Length\", run_investment))\n",
    "        results_list.append((\"Random Growth (mean) - Total Length\", random_lengths_mean))\n",
    "        results_list.append((\"Random Growth (mean) - Total Investment Length\", random_investment_mean))\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            lengths_demand_ltn = utils.compute_total_lengths(GTs_demand_ltn_priority)\n",
    "            investment_demand_ltn = utils.compute_total_investment_lengths(GTs_demand_ltn_priority, distance_cost)\n",
    "            lengths_betweenness_ltn = utils.compute_total_lengths(GTs_betweenness_ltn_priority)\n",
    "            investment_betweenness_ltn = utils.compute_total_investment_lengths(GTs_betweenness_ltn_priority, distance_cost)\n",
    "            results_list.append((\"Demand LTN Priority Growth - Total Length\", lengths_demand_ltn))\n",
    "            results_list.append((\"Demand LTN Priority Growth - Total Investment Length\", investment_demand_ltn))\n",
    "            results_list.append((\"Betweenness LTN Priority Growth - Total Length\", lengths_betweenness_ltn))\n",
    "            results_list.append((\"Betweenness LTN Priority Growth - Total Investment Length\", investment_betweenness_ltn))\n",
    "        # Save all results\n",
    "        utils.save_results(results_list, analysis_res_pickle, analysis_res_json)\n",
    "        analysis_results[scenario] = {label: data for label, data in results_list}\n",
    "        print(f\"Updated analysis results for {scenario} in {placeid}\")\n",
    "\n",
    "    # Plotting: investment length vs total length\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(1, 100):\n",
    "        len_key = f\"Random Run {i} - Total Length\"\n",
    "        invest_key = f\"Random Run {i} - Total Investment Length\"\n",
    "        if len_key in analysis_results[scenario] and invest_key in analysis_results[scenario]:\n",
    "            plt.plot(analysis_results[scenario][len_key], analysis_results[scenario][invest_key], \n",
    "                     color='lightgray', linewidth=1, alpha=0.5)\n",
    "        else:\n",
    "            break\n",
    "    plt.plot(analysis_results[scenario]['Random Growth (mean) - Total Length'],\n",
    "             analysis_results[scenario]['Random Growth (mean) - Total Investment Length'],\n",
    "             linestyle='--', linewidth=2, label='Random Growth (mean)', color='blue')\n",
    "    plt.plot(analysis_results[scenario]['Betweenness Growth - Total Length'],\n",
    "             analysis_results[scenario]['Betweenness Growth - Total Investment Length'],\n",
    "             '-', label='Betweenness Growth', color='orange')\n",
    "    plt.plot(analysis_results[scenario]['Demand Growth - Total Length'],\n",
    "             analysis_results[scenario]['Demand Growth - Total Investment Length'],\n",
    "             '-.', label='Demand Growth', color='red')\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        # Plot LTN priority demand growth\n",
    "        plt.plot(analysis_results[scenario]['Demand LTN Priority Growth - Total Length'],\n",
    "                 analysis_results[scenario]['Demand LTN Priority Growth - Total Investment Length'],\n",
    "                 ':', label='Demand LTN Priority Growth', color='green')\n",
    "\n",
    "        # Plot LTN priority betweenness growth\n",
    "        plt.plot(analysis_results[scenario]['Betweenness LTN Priority Growth - Total Length'],\n",
    "                 analysis_results[scenario]['Betweenness LTN Priority Growth - Total Investment Length'],\n",
    "                 '-', label='Betweenness LTN Priority Growth', color='purple')\n",
    "\n",
    "    plt.xlabel('Total Length (meters)')\n",
    "    plt.ylabel('Total Investment Length (meters)')\n",
    "    plt.title(f'Investment Length vs Total Length for {scenario} - {placeid}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = os.path.join(PATH[\"plots\"], placeid, scenario, \"Investment_vs_Length.png\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'no_ltn_scenario'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m analysis_results \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mload_results(analysis_res_pickle)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Use mean random runs as baseline\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m random_lengths_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43manalysis_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Growth (mean) - Total Length\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      8\u001b[0m random_investments_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(analysis_results[scenario][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Growth (mean) - Total Investment Length\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Define strategies\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'no_ltn_scenario'"
     ]
    }
   ],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    # Load results\n",
    "    analysis_res_pickle = analysis_res_pickle_paths[scenario]\n",
    "    analysis_results = utils.load_results(analysis_res_pickle)\n",
    "\n",
    "    # Use mean random runs as baseline\n",
    "    random_lengths_mean = np.array(analysis_results[scenario][\"Random Growth (mean) - Total Length\"])\n",
    "    random_investments_mean = np.array(analysis_results[scenario][\"Random Growth (mean) - Total Investment Length\"])\n",
    "\n",
    "\n",
    "    # Define strategies\n",
    "    strategies = {\n",
    "        'Betweenness Growth': {\n",
    "            'lengths': np.array(analysis_results[scenario][\"Betweenness Growth - Total Length\"]),\n",
    "            'investments': np.array(analysis_results[scenario][\"Betweenness Growth - Total Investment Length\"]),\n",
    "            'color': 'orange', 'marker': 'o'},\n",
    "        'Demand Growth': {\n",
    "            'lengths': np.array(analysis_results[scenario][\"Demand Growth - Total Length\"]),\n",
    "            'investments': np.array(analysis_results[scenario][\"Demand Growth - Total Investment Length\"]),\n",
    "            'color': 'red', 'marker': 's' },\n",
    "        'Demand LTN Growth': {\n",
    "            'lengths': np.array(analysis_results[scenario].get(\"Demand LTN Priority Growth - Total Length\", [])),\n",
    "            'investments': np.array(analysis_results[scenario].get(\"Demand LTN Priority Growth - Total Investment Length\", [])),\n",
    "            'color': 'green', 'marker': '^'},\n",
    "        'Betweenness LTN Growth': {\n",
    "            'lengths': np.array(analysis_results[scenario].get(\"Betweenness LTN Priority Growth - Total Length\", [])),\n",
    "            'investments': np.array(analysis_results[scenario].get(\"Betweenness LTN Priority Growth - Total Investment Length\", [])),\n",
    "            'color': 'purple', 'marker': 'D'}}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot individual random runs\n",
    "    random_runs_lengths = analysis_results.get('random_runs_lengths_list', [])\n",
    "    random_runs_investments = analysis_results.get('random_runs_investment_lengths_list', [])\n",
    "    for i in range(len(random_runs_lengths)):\n",
    "        run_lengths = np.array(random_runs_lengths[i])\n",
    "        run_investments = np.array(random_runs_investments[i])\n",
    "        plt.scatter(run_lengths - random_lengths_mean,\n",
    "                    run_investments - random_investments_mean,\n",
    "                    color='lightgray', alpha=0.3, s=10, label='_nolegend_')\n",
    "\n",
    "    # Plot strategy deviations\n",
    "    for label, data in strategies.items():\n",
    "        if data['lengths'].size == 0 or data['investments'].size == 0:\n",
    "            continue  # skip missing\n",
    "        x_dev = data['lengths'] - random_lengths_mean\n",
    "        y_dev = data['investments'] - random_investments_mean\n",
    "        plt.scatter(x_dev, y_dev, label=label, color=data['color'], marker=data['marker'], alpha=0.8, s=50)\n",
    "\n",
    "    # Reference lines\n",
    "    plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    plt.axvline(0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.xlabel('Deviation in Total Length (m) from Random Growth (mean)')\n",
    "    plt.ylabel('Deviation in Investment Length (m) from Random Growth (mean)')\n",
    "    plt.title(f'Investment Cost vs Length: Deviation from Random ({scenario})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = os.path.join(PATH[\"plots\"], placeid, scenario, \"investment_vs_length_deviation_scatter.png\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance gained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are trying to find how much of the existing network is connected per iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total bike network - G_bikeall\n",
    "\n",
    "G'investment_length' - investment size\n",
    "\n",
    "G'length' - length of created network, not including netowrk size\n",
    "\n",
    "need to do a compose of G_bikeall and G in GTs\n",
    "\n",
    "but only compose where infrastucutre is connected to our generated network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the length of infrastructure connected to generated network, along with the combined length. Thus we now know how much extra cycle network is connected per level of investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or initialize results\n",
    "analysis_results = utils.load_results(analysis_res_pickle)\n",
    "\n",
    "if rerun or 'biketrack_lengths' not in analysis_results:\n",
    "    results_list = []\n",
    "    GT_lengths, biketrack_lengths, combined_lengths = utils.compute_biketrack_connected_lengths(GTs, G_biketrack)\n",
    "    results_list.append((\"GT Connected Lengths\", GT_lengths))\n",
    "    results_list.append((\"Biketrack Connected Lengths\", biketrack_lengths))\n",
    "    results_list.append((\"Combined Connected Lengths\", combined_lengths))\n",
    "\n",
    "    random_runs_GT_lengths = []\n",
    "    random_runs_biketrack_lengths = []\n",
    "    random_runs_combined_lengths = []\n",
    "    for run in random_runs:\n",
    "        gt, bike, combined = utils.compute_biketrack_connected_lengths(run[\"GTs\"], G_biketrack)\n",
    "        random_runs_GT_lengths.append(gt)\n",
    "        random_runs_biketrack_lengths.append(bike)\n",
    "        random_runs_combined_lengths.append(combined)\n",
    "    # random runs\n",
    "    for i, run_lengths in enumerate(random_runs_biketrack_lengths):\n",
    "        results_list.append((f\"Random Run {i+1} - Biketrack Connected Lengths\", run_lengths))\n",
    "    GT_random_mean = np.mean(random_runs_GT_lengths, axis=0).tolist()\n",
    "    biketrack_random_mean = np.mean(random_runs_biketrack_lengths, axis=0).tolist()\n",
    "    combined_random_mean = np.mean(random_runs_combined_lengths, axis=0).tolist()\n",
    "    results_list.append((\"GT Random Mean - Connected Lengths\", GT_random_mean))\n",
    "    results_list.append((\"Biketrack Random Mean - Connected Lengths\", biketrack_random_mean))\n",
    "    results_list.append((\"Combined Random Mean - Connected Lengths\", combined_random_mean))\n",
    "\n",
    "    gt, bike, combined = utils.compute_biketrack_connected_lengths(GTs_demand, G_biketrack)\n",
    "    results_list.append((\"GT Demand Connected Lengths\", gt))\n",
    "    results_list.append((\"Biketrack Demand Connected Lengths\", bike))\n",
    "    results_list.append((\"Combined Demand Connected Lengths\", combined))\n",
    "\n",
    "    gt, bike, combined = utils.compute_biketrack_connected_lengths(GTs_demand_ltn_priority, G_biketrack)\n",
    "    results_list.append((\"GT Demand LTN Priority Connected Lengths\", gt))\n",
    "    results_list.append((\"Biketrack Demand LTN Priority Connected Lengths\", bike))\n",
    "    results_list.append((\"Combined Demand LTN Priority Connected Lengths\", combined))\n",
    "\n",
    "    gt, bike, combined = utils.compute_biketrack_connected_lengths(GTs_betweenness_ltn_priority, G_biketrack)\n",
    "    results_list.append((\"GT Betweenness LTN Priority Connected Lengths\", gt))\n",
    "    results_list.append((\"Biketrack Betweenness LTN Priority Connected Lengths\", bike))\n",
    "    results_list.append((\"Combined Betweenness LTN Priority Connected Lengths\", combined))\n",
    "\n",
    "    # Save\n",
    "    utils.save_results(results_list, analysis_res_pickle, analysis_res_csv)\n",
    "    analysis_results = {label: data for label, data in results_list}\n",
    "    print(f\"Updated biketrack connected length analysis results for {placeid}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot random runs\n",
    "for i in range(1, 100):\n",
    "    key = f\"Random Run {i} - Biketrack Connected Lengths\"\n",
    "    if key in analysis_results:\n",
    "        plt.plot(analysis_results[key], color='lightgray', linewidth=1, alpha=0.5)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Plot means and strategies\n",
    "plt.plot(analysis_results[\"Biketrack Random Mean - Connected Lengths\"], '--', color='blue', linewidth=2, label=\"Random Growth (mean)\")\n",
    "plt.plot(analysis_results[\"Biketrack Connected Lengths\"], '-', color='orange', label=\"Betweenness\")\n",
    "plt.plot(analysis_results[\"Biketrack Demand Connected Lengths\"], '-.', color='red', label=\"Demand\")\n",
    "plt.plot(analysis_results[\"Biketrack Demand LTN Priority Connected Lengths\"], ':', color='green', label=\"Demand LTN Priority\")\n",
    "plt.plot(analysis_results[\"Biketrack Betweenness LTN Priority Connected Lengths\"], '-', color='purple', label=\"Betweenness LTN Priority\")\n",
    "\n",
    "# Finalize plot\n",
    "plt.xlabel(\"Investment Iteration\")\n",
    "plt.ylabel(\"Additional Cycle Infrastructure Connected Length (meters)\")\n",
    "plt.title(\"Additional Cycle Infrastructure Connected per Iteration\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(PATH[\"plots\"], placeid, \"additional_cyclenet_connected.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = load_results(analysis_res_pickle)\n",
    "\n",
    "if rerun or 'biketrack_lengths' not in analysis_results:\n",
    "    analysis_results.update({\n",
    "        'GT_lengths': compute_biketrack_connected_lengths(GTs, G_biketrack)[0],\n",
    "        'biketrack_lengths': compute_biketrack_connected_lengths(GTs, G_biketrack)[1],\n",
    "        'combined_lengths': compute_biketrack_connected_lengths(GTs, G_biketrack)[2],\n",
    "\n",
    "        'random_runs_GT_lengths': [compute_biketrack_connected_lengths(run[\"GTs\"], G_biketrack)[0] for run in random_runs],\n",
    "        'random_runs_biketrack_lengths': [compute_biketrack_connected_lengths(run[\"GTs\"], G_biketrack)[1] for run in random_runs],\n",
    "        'random_runs_combined_lengths': [compute_biketrack_connected_lengths(run[\"GTs\"], G_biketrack)[2] for run in random_runs],\n",
    "\n",
    "        'GT_random_lengths': np.mean(\n",
    "            [compute_biketrack_connected_lengths(run[\"GTs\"], G_biketrack)[0] for run in random_runs],\n",
    "            axis=0\n",
    "        ).tolist(),\n",
    "        'biketrack_random_lengths': np.mean(\n",
    "            [compute_biketrack_connected_lengths(run[\"GTs\"], G_biketrack)[1] for run in random_runs],\n",
    "            axis=0\n",
    "        ).tolist(),\n",
    "        'combined_random_lengths': np.mean(\n",
    "            [compute_biketrack_connected_lengths(run[\"GTs\"], G_biketrack)[2] for run in random_runs],\n",
    "            axis=0\n",
    "        ).tolist(),\n",
    "\n",
    "        'GT_demand_lengths': compute_biketrack_connected_lengths(GTs_demand, G_biketrack)[0],\n",
    "        'biketrack_demand_lengths': compute_biketrack_connected_lengths(GTs_demand, G_biketrack)[1],\n",
    "        'combined_demand_lengths': compute_biketrack_connected_lengths(GTs_demand, G_biketrack)[2],\n",
    "\n",
    "        'GT_demand_lengths_ltn_priority': compute_biketrack_connected_lengths(GTs_demand_ltn_priority, G_biketrack)[0],\n",
    "        'biketrack_demand_lengths_ltn_priority': compute_biketrack_connected_lengths(GTs_demand_ltn_priority, G_biketrack)[1],\n",
    "        'combined_demand_lengths_ltn_priority': compute_biketrack_connected_lengths(GTs_demand_ltn_priority, G_biketrack)[2],\n",
    "\n",
    "        'GT_betweenness_lengths_ltn_priority': compute_biketrack_connected_lengths(GTs_betweenness_ltn_priority, G_biketrack)[0],\n",
    "        'biketrack_betweenness_lengths_ltn_priority': compute_biketrack_connected_lengths(GTs_betweenness_ltn_priority, G_biketrack)[1],\n",
    "        'combined_betweenness_lengths_ltn_priority': compute_biketrack_connected_lengths(GTs_betweenness_ltn_priority, G_biketrack)[2],\n",
    "    })\n",
    "\n",
    "    save_results(analysis_results, analysis_res_pickle, analysis_res_csv)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot individual random runs\n",
    "for run_lengths in analysis_results['random_runs_biketrack_lengths']:\n",
    "    plt.plot(run_lengths, color='lightgray', linewidth=1, alpha=0.5)\n",
    "\n",
    "# Plot mean of random runs\n",
    "plt.plot(analysis_results['biketrack_random_lengths'], '--', color='blue', linewidth=2, label=\"Random Growth (mean)\")\n",
    "\n",
    "# Other strategies\n",
    "plt.plot(analysis_results['biketrack_lengths'], '-', color='orange', label=\"Betweenness\")\n",
    "plt.plot(analysis_results['biketrack_demand_lengths'], '-.', color='red', label=\"Demand\")\n",
    "plt.plot(analysis_results['biketrack_demand_lengths_ltn_priority'], ':', color='green', label=\"Demand LTN Priority\")\n",
    "plt.plot(analysis_results['biketrack_betweenness_lengths_ltn_priority'], '-', color='purple', label=\"Betweenness LTN Priority\")\n",
    "\n",
    "plt.xlabel(\"Investment Iteration\")\n",
    "plt.ylabel(\"Additional Cycle Infrastructure Connected Length (meters)\")\n",
    "plt.title(\"Additional Cycle Infrastructure Connected per Iteration\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(PATH[\"plots\"], placeid, \"additional_cyclenet_connected.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = load_results(analysis_res_pickle)\n",
    "if rerun or 'biketrack_deviation_from_random' not in analysis_results:\n",
    "\n",
    "    random_runs = analysis_results['random_runs_biketrack_lengths']\n",
    "    random_mean = np.mean(random_runs, axis=0)\n",
    "\n",
    "    # Deviation of each random run from mean\n",
    "    random_runs_deviations = [np.array(run) - random_mean for run in random_runs]\n",
    "    random_deviations_mean = np.mean(random_runs_deviations, axis=0).tolist()\n",
    "\n",
    "    biketrack_dev_data = {\n",
    "        'dev_betweenness': compute_abs_deviation(\n",
    "            analysis_results['biketrack_lengths'], random_mean\n",
    "        ),\n",
    "        'dev_demand': compute_abs_deviation(\n",
    "            analysis_results['biketrack_demand_lengths'], random_mean\n",
    "        ),\n",
    "        'dev_demand_ltn': compute_abs_deviation(\n",
    "            analysis_results['biketrack_demand_lengths_ltn_priority'], random_mean\n",
    "        ),\n",
    "        'dev_betweenness_ltn': compute_abs_deviation(\n",
    "            analysis_results['biketrack_betweenness_lengths_ltn_priority'], random_mean\n",
    "        ),\n",
    "        'random_runs_deviations': [d.tolist() for d in random_runs_deviations],\n",
    "        'random_deviations_mean': random_deviations_mean\n",
    "    }\n",
    "\n",
    "    analysis_results['biketrack_deviation_from_random'] = biketrack_dev_data\n",
    "    save_results(analysis_results, analysis_res_pickle, analysis_res_csv)\n",
    "\n",
    "\n",
    "\n",
    "# Plot deviation from random for biketrack-connected lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "for dev in analysis_results['biketrack_deviation_from_random']['random_runs_deviations']:\n",
    "    plt.plot(dev, color='lightgray', linewidth=1, alpha=0.4)\n",
    "plt.axhline(0, color='blue', linestyle='--', linewidth=2, label='Random Growth (mean)')\n",
    "for key, style in [\n",
    "    ('dev_betweenness', ('-', 'orange', 'Betweenness Growth')),\n",
    "    ('dev_demand', ('-.', 'red', 'Demand Growth')),\n",
    "    ('dev_demand_ltn', (':', 'green', 'Demand LTN Priority Growth')),\n",
    "    ('dev_betweenness_ltn', ('-', 'purple', 'Betweenness LTN Priority Growth')),\n",
    "]:\n",
    "    plt.plot(\n",
    "        analysis_results['biketrack_deviation_from_random'][key],\n",
    "        linestyle=style[0], color=style[1], label=style[2]\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Investment Iteration\")\n",
    "plt.ylabel(\"Deviation from Random Growth Baseline (meters)\")\n",
    "plt.title(\"Deviation in Connected Biketrack Length from Random Growth Baseline\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(PATH[\"plots\"], placeid, \"biketrack_connected__deviation_from_random.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deviation from Random: Biketrack connected lengths\n",
    "# random_baseline = np.array(analysis_results['biketrack_random_lengths'])\n",
    "\n",
    "# # Prepare deviations from random for each strategy\n",
    "# deviations = {\n",
    "#     'Betweenness': {\n",
    "#         'values': np.array(analysis_results['biketrack_lengths']) - random_baseline,\n",
    "#         'color': 'orange',\n",
    "#         'linestyle': '--'\n",
    "#     },\n",
    "#     'Demand': {\n",
    "#         'values': np.array(analysis_results['biketrack_demand_lengths']) - random_baseline,\n",
    "#         'color': 'red',\n",
    "#         'linestyle': '-.'\n",
    "#     },\n",
    "#     'Demand LTN': {\n",
    "#         'values': np.array(analysis_results['biketrack_demand_lengths_ltn_priority']) - random_baseline,\n",
    "#         'color': 'green',\n",
    "#         'linestyle': '-.'\n",
    "#     },\n",
    "#     'Betweenness LTN': {\n",
    "#         'values': np.array(analysis_results['biketrack_betweenness_lengths_ltn_priority']) - random_baseline,\n",
    "#         'color': 'purple',\n",
    "#         'linestyle': '-'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for label, data in deviations.items():\n",
    "#     plt.plot(\n",
    "#         range(1, len(data['values']) + 1),\n",
    "#         data['values'],\n",
    "#         linestyle=data['linestyle'],\n",
    "#         color=data['color'],\n",
    "#         label=label\n",
    "#     )\n",
    "\n",
    "# plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Deviation in Connected Length vs Random (meters)\")\n",
    "# plt.title(\"Additional Cycle Infrastructure Connected — Deviation from Random Growth (Baseline)\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save\n",
    "# output_path = PATH[\"plots\"] + f\"/{placeid}/additional_cyclenet_connected__deviation_from_random.png\"\n",
    "# plt.savefig(output_path, dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected Components "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the length of the largest connected component, first a just our investment, then combined with existing network, then by combined but only where its connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this computes LCC in the \"typical\" way by measuring componet size by number of nodes\n",
    "\n",
    "# if os.path.exists(analysis_res_pickle):\n",
    "#     with open(analysis_res_pickle, 'rb') as f:\n",
    "#         analysis_results = pickle.load(f)\n",
    "# else:\n",
    "#     analysis_results = {}\n",
    "\n",
    "# if rerun or 'lcc_lengths_GTs' not in analysis_results:\n",
    "#     # Compute LCC lengths\n",
    "#     lcc_data = {\n",
    "#         'lcc_lengths_GTs': [\n",
    "#             sum(data['length'] for _, _, data in \n",
    "#                 G.subgraph(max(nx.weakly_connected_components(G), key=len)).edges(data=True))\n",
    "#             for G in GTs\n",
    "#         ],\n",
    "#         'lcc_lengths_GTs_random': [\n",
    "#             sum(data['length'] for _, _, data in \n",
    "#                 G.subgraph(max(nx.weakly_connected_components(G), key=len)).edges(data=True))\n",
    "#             for G in GTs_random\n",
    "#         ],\n",
    "#         'lcc_lengths_GTs_demand': [\n",
    "#             sum(data['length'] for _, _, data in \n",
    "#                 G.subgraph(max(nx.weakly_connected_components(G), key=len)).edges(data=True))\n",
    "#             for G in GTs_demand\n",
    "#         ],\n",
    "#         'lcc_lengths_GTs_demand_ltn_priority': [\n",
    "#             sum(data['length'] for _, _, data in \n",
    "#                 G.subgraph(max(nx.weakly_connected_components(G), key=len)).edges(data=True))\n",
    "#             for G in GTs_demand_ltn_priority\n",
    "#         ],\n",
    "#         'lcc_lengths_GTs_betweenness_ltn_priority': [\n",
    "#             sum(data['length'] for _, _, data in \n",
    "#                 G.subgraph(max(nx.weakly_connected_components(G), key=len)).edges(data=True))\n",
    "#             for G in GTs_betweenness_ltn_priority\n",
    "#         ]\n",
    "#     }\n",
    "\n",
    "#     analysis_results.update(lcc_data)\n",
    "\n",
    "#     with open(analysis_res_pickle, 'wb') as f:\n",
    "#         pickle.dump(analysis_results, f)\n",
    "#     pd.DataFrame({k: pd.Series(v) for k,v in analysis_results.items()}) \\\n",
    "#       .to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# # Plot LCC lengths\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs'], \n",
    "#     '--', color='orange', label='Betweeness Growth'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs_random'], \n",
    "#     '-', color='blue', label='Random Growth'\n",
    "# )\n",
    "\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs_demand'],\n",
    "#     '-.', color='red', label='Demand Growth'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs_demand_ltn_priority'],\n",
    "#     '-.', color='green', label='Demand LTN Growth'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs_betweenness_ltn_priority'],\n",
    "#     '-', color='purple', label='Betweenness LTN Growth'\n",
    "# )\n",
    "# plt.xlabel('Investment Iteration')\n",
    "# plt.ylabel('Length (meters)')\n",
    "# plt.title('Size of Largest Connected Component per Iteration')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# output_path = PATH[\"plots\"] + \"/\" + placeid + \"/size_of_lcc.png\"\n",
    "# plt.savefig(output_path, dpi=300)\n",
    "\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun or 'lcc_lengths' not in analysis_results:\n",
    "    lcc_data = {\n",
    "        'lcc_lengths': [get_longest_connected_components(G) for G in GTs],\n",
    "        'random_runs_lcc_lengths': [\n",
    "            [get_longest_connected_components(G) for G in run[\"GTs\"]] for run in random_runs\n",
    "        ],\n",
    "        'demand_lcc_lengths': [get_longest_connected_components(G) for G in GTs_demand],\n",
    "        'demand_lcc_lengths_ltn_priority': [get_longest_connected_components(G) for G in GTs_demand_ltn_priority],\n",
    "        'betweenness_lcc_lengths_ltn_priority': [get_longest_connected_components(G) for G in GTs_betweenness_ltn_priority]\n",
    "    }\n",
    "\n",
    "    # Compute mean random lcc lengths across runs and graphs\n",
    "    lcc_data['random_lcc_lengths_mean'] = np.mean(\n",
    "    lcc_data['random_runs_lcc_lengths'], axis=0\n",
    "    ).tolist()\n",
    "\n",
    "    analysis_results.update(lcc_data)\n",
    "    save_results(analysis_results, analysis_res_pickle, analysis_res_csv)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for run in analysis_results['random_runs_lcc_lengths']:\n",
    "    plt.plot(run, color='lightgray', linewidth=1, alpha=0.4)\n",
    "\n",
    "plt.plot(analysis_results['random_lcc_lengths_mean'], linestyle='--', linewidth=2, label='Random Growth (mean)', color='blue')\n",
    "plt.plot(analysis_results['lcc_lengths'], '-', label='Betweenness Growth', color='orange')\n",
    "plt.plot(analysis_results['demand_lcc_lengths'], '-.', label='Demand Growth', color='red')\n",
    "plt.plot(analysis_results['demand_lcc_lengths_ltn_priority'], ':', label='Demand LTN Priority Growth', color='green')\n",
    "plt.plot(analysis_results['betweenness_lcc_lengths_ltn_priority'], '-', label='Betweenness LTN Priority Growth', color='purple')\n",
    "\n",
    "plt.xlabel('Investment Iteration')\n",
    "plt.ylabel('LCC Length (meters)')\n",
    "plt.title('Size of Largest Connected Component per Iteration')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(PATH[\"plots\"], placeid, \"size_of_lcc.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## largest connected component is calculated as longest (length of edges) connected component\n",
    "# # this is because we are interested in how far a cyclist can travel, rather than the numeber of nodes\n",
    "\n",
    "# if os.path.exists(analysis_res_pickle):\n",
    "#     with open(analysis_res_pickle, 'rb') as f:\n",
    "#         analysis_results = pickle.load(f)\n",
    "# else:\n",
    "#     analysis_results = {}\n",
    "\n",
    "# if rerun or 'lcc_lengths_GTs' not in analysis_results:\n",
    "#     # Compute LCC lengths\n",
    "#         lcc_data = {\n",
    "#             'lcc_lengths_GTs': [\n",
    "#                 get_longest_connected_components(G) for G in GTs\n",
    "#             ],\n",
    "#             'lcc_lengths_GTs_random': [\n",
    "#                 get_longest_connected_components(G) for G in GTs_random\n",
    "#             ],\n",
    "#             'lcc_lengths_GTs_demand': [\n",
    "#                 get_longest_connected_components(G) for G in GTs_demand\n",
    "#             ],\n",
    "#             'lcc_lengths_GTs_demand_ltn_priority': [\n",
    "#                 get_longest_connected_components(G) for G in GTs_demand_ltn_priority\n",
    "#             ],\n",
    "#             'lcc_lengths_GTs_betweenness_ltn_priority': [\n",
    "#                 get_longest_connected_components(G) for G in GTs_betweenness_ltn_priority\n",
    "#             ]\n",
    "#         }\n",
    "\n",
    "\n",
    "# analysis_results.update(lcc_data)\n",
    "\n",
    "# with open(analysis_res_pickle, 'wb') as f:\n",
    "#     pickle.dump(analysis_results, f)\n",
    "# pd.DataFrame({k: pd.Series(v) for k,v in analysis_results.items()}) \\\n",
    "#     .to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# # Plot LCC lengths\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs'], \n",
    "#     '--', color='orange', label='Betweeness Growth'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs_random'], \n",
    "#     '-', color='blue', label='Random Growth'\n",
    "# )\n",
    "\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs_demand'],\n",
    "#     '-.', color='red', label='Demand Growth'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs_demand_ltn_priority'],\n",
    "#     '-.', color='green', label='Demand LTN Growth'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['lcc_lengths_GTs_betweenness_ltn_priority'],\n",
    "#     '-', color='purple', label='Betweenness LTN Growth'\n",
    "# )\n",
    "# plt.xlabel('Investment Iteration')\n",
    "# plt.ylabel('Length (meters)')\n",
    "# plt.title('Size of Largest Connected Component per Iteration')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# output_path = PATH[\"plots\"] + \"/\" + placeid + \"/size_of_lcc.png\"\n",
    "# plt.savefig(output_path, dpi=300)\n",
    "\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = load_results(analysis_res_pickle)\n",
    "if rerun or 'lcc_deviation_from_random' not in analysis_results:\n",
    "    random_runs = analysis_results['random_runs_lcc_lengths']\n",
    "    random_mean = np.array(analysis_results['random_lcc_lengths_mean'])\n",
    "\n",
    "    # Deviation of each random run from the mean\n",
    "    random_runs_deviations = [np.array(run) - random_mean for run in random_runs]\n",
    "    random_deviations_mean = np.mean(random_runs_deviations, axis=0).tolist()\n",
    "\n",
    "    lcc_dev_data = {\n",
    "        'dev_betweenness': np.array(analysis_results['lcc_lengths']) - random_mean,\n",
    "        'dev_demand': np.array(analysis_results['demand_lcc_lengths']) - random_mean,\n",
    "        'dev_demand_ltn': np.array(analysis_results['demand_lcc_lengths_ltn_priority']) - random_mean,\n",
    "        'dev_betweenness_ltn': np.array(analysis_results['betweenness_lcc_lengths_ltn_priority']) - random_mean,\n",
    "        'random_runs_deviations': [d.tolist() for d in random_runs_deviations],\n",
    "        'random_deviations_mean': random_deviations_mean\n",
    "    }\n",
    "\n",
    "    analysis_results['lcc_deviation_from_random'] = lcc_dev_data\n",
    "    save_results(analysis_results, analysis_res_pickle, analysis_res_csv)\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Gray lines: deviation of each random run from the mean\n",
    "for dev in analysis_results['lcc_deviation_from_random']['random_runs_deviations']:\n",
    "    plt.plot(dev, color='lightgray', linewidth=1, alpha=0.4)\n",
    "\n",
    "# Dashed blue line at 0 deviation\n",
    "plt.axhline(0, color='blue', linestyle='--', linewidth=2, label='Random Growth (mean)')\n",
    "\n",
    "# Plot deviations for each strategy\n",
    "for key, style in [\n",
    "    ('dev_betweenness', ('-', 'orange', 'Betweenness Growth')),\n",
    "    ('dev_demand', ('-.', 'red', 'Demand Growth')),\n",
    "    ('dev_demand_ltn', (':', 'green', 'Demand LTN Priority Growth')),\n",
    "    ('dev_betweenness_ltn', ('-', 'purple', 'Betweenness LTN Priority Growth')),\n",
    "]:\n",
    "    plt.plot(\n",
    "        analysis_results['lcc_deviation_from_random'][key],\n",
    "        linestyle=style[0], color=style[1], label=style[2]\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Investment Iteration\")\n",
    "plt.ylabel(\"Deviation from Random Growth Baseline (meters)\")\n",
    "plt.title(\"Deviation in LCC Length from Random Growth Baseline\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(PATH[\"plots\"], placeid, \"lcc_length__deviation_from_random.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## compared to random baseline\n",
    "# #  Load previous results\n",
    "# if os.path.exists(analysis_res_pickle):\n",
    "#     with open(analysis_res_pickle, 'rb') as f:\n",
    "#         analysis_results = pickle.load(f)\n",
    "# else:\n",
    "#     analysis_results = {}\n",
    "\n",
    "# # Recalculate if needed\n",
    "# if rerun or 'lcc_lengths_GTs' not in analysis_results:\n",
    "#     lcc_data = {\n",
    "#         'lcc_lengths_GTs': [\n",
    "#             get_longest_connected_components(G) for G in GTs\n",
    "#         ],\n",
    "#         'lcc_lengths_GTs_random': [\n",
    "#             get_longest_connected_components(G) for G in GTs_random\n",
    "#         ],\n",
    "#         'lcc_lengths_GTs_demand': [\n",
    "#             get_longest_connected_components(G) for G in GTs_demand\n",
    "#         ],\n",
    "#         'lcc_lengths_GTs_demand_ltn_priority': [\n",
    "#             get_longest_connected_components(G) for G in GTs_demand_ltn_priority\n",
    "#         ],\n",
    "#         'lcc_lengths_GTs_betweenness_ltn_priority': [\n",
    "#             get_longest_connected_components(G) for G in GTs_betweenness_ltn_priority\n",
    "#         ]\n",
    "#     }\n",
    "#     analysis_results.update(lcc_data)\n",
    "#     with open(analysis_res_pickle, 'wb') as f:\n",
    "#         pickle.dump(analysis_results, f)\n",
    "#     pd.DataFrame({k: pd.Series(v) for k,v in analysis_results.items()}) \\\n",
    "#         .to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# # Calculate deviation from random\n",
    "# random_lcc = np.array(analysis_results['lcc_lengths_GTs_random'])\n",
    "\n",
    "# lcc_deviations = {\n",
    "#     'Betweenness': {\n",
    "#         'values': np.array(analysis_results['lcc_lengths_GTs']) - random_lcc,\n",
    "#         'color': 'orange',\n",
    "#         'linestyle': '--'\n",
    "#     },\n",
    "#     'Demand': {\n",
    "#         'values': np.array(analysis_results['lcc_lengths_GTs_demand']) - random_lcc,\n",
    "#         'color': 'red',\n",
    "#         'linestyle': '-.'\n",
    "#     },\n",
    "#     'Demand LTN': {\n",
    "#         'values': np.array(analysis_results['lcc_lengths_GTs_demand_ltn_priority']) - random_lcc,\n",
    "#         'color': 'green',\n",
    "#         'linestyle': '-.'\n",
    "#     },\n",
    "#     'Betweenness LTN': {\n",
    "#         'values': np.array(analysis_results['lcc_lengths_GTs_betweenness_ltn_priority']) - random_lcc,\n",
    "#         'color': 'purple',\n",
    "#         'linestyle': '-'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for label, data in lcc_deviations.items():\n",
    "#     plt.plot(\n",
    "#         range(1, len(data['values']) + 1),\n",
    "#         data['values'],\n",
    "#         linestyle=data['linestyle'],\n",
    "#         color=data['color'],\n",
    "#         label=label\n",
    "#     )\n",
    "\n",
    "# plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "# plt.xlabel('Investment Iteration')\n",
    "# plt.ylabel('Deviation in LCC Length vs Random (meters)')\n",
    "# plt.title('Size of Largest Connected Component — Deviation from Random Growth (Baseline)')\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save\n",
    "# output_path = PATH[\"plots\"] + f\"/{placeid}/size_of_lcc__deviation_from_random.png\"\n",
    "# plt.savefig(output_path, dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def compute_lcc_lengths(graph_list, G_biketrack):\n",
    "# #     \"\"\"Computes the total length of the largest connected component for each graph in the list.\"\"\"\n",
    "# #     total_lengths_lcc = []\n",
    "    \n",
    "# #     for G in graph_list:\n",
    "# #         # Compose graphs and find largest connected component\n",
    "# #         merged = nx.compose(G, G_biketrack)\n",
    "        \n",
    "# #         # Get weakly connected components (works for both directed/undirected graphs)\n",
    "# #         components = list(nx.weakly_connected_components(merged))\n",
    "        \n",
    "# #         if not components:\n",
    "# #             total_length = 0.0  # Handle empty graph case\n",
    "# #         else:\n",
    "# #             # Find largest component by node count\n",
    "# #             largest_component_nodes = max(components, key=len)\n",
    "# #             largest_component = merged.subgraph(largest_component_nodes)\n",
    "            \n",
    "# #             # Calculate total edge length in the largest component\n",
    "# #             total_length = sum(data['length'] for u, v, data in largest_component.edges(data=True))\n",
    "        \n",
    "# #         total_lengths_lcc.append(total_length)\n",
    "    \n",
    "# #     return total_lengths_lcc\n",
    "\n",
    "# # # Compute LCC lengths for GTs and GTs_random\n",
    "# # total_lengths_lcc_GTs = compute_lcc_lengths(GTs, G_biketrack)\n",
    "# # total_lengths_lcc_GTs_random = compute_lcc_lengths(GTs_random, G_biketrack)\n",
    "\n",
    "# # # Create the plot\n",
    "# # plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # # Plot GTs\n",
    "# # plt.plot(\n",
    "# #     range(len(total_lengths_lcc_GTs)), total_lengths_lcc_GTs, linestyle='-', color='blue', label=\"GTs\"\n",
    "# # )\n",
    "\n",
    "# # # Plot GTs_random\n",
    "# # plt.plot(\n",
    "# #     range(len(total_lengths_lcc_GTs_random)), total_lengths_lcc_GTs_random, linestyle='--', color='orange', label=\"GTs_random\"\n",
    "# # )\n",
    "\n",
    "# # # Labels and title\n",
    "# # plt.title('Total Length of Largest Connected Component')\n",
    "# # plt.xlabel('Graph Index')\n",
    "# # plt.ylabel('Total Length (meters)')\n",
    "# # plt.grid(True, axis='y', alpha=0.3)\n",
    "# # plt.legend()\n",
    "# # plt.tight_layout()\n",
    "# # plt.show()\n",
    "\n",
    "# if os.path.exists(analysis_res_pickle):\n",
    "#     with open(analysis_res_pickle, 'rb') as f:\n",
    "#         analysis_results = pickle.load(f)\n",
    "# else:\n",
    "#     analysis_results = {}\n",
    "\n",
    "# if rerun or 'composite_lcc_GTs' not in analysis_results:\n",
    "#     def compute_lcc_lengths(graph_list, G_biketrack):\n",
    "#         total_lengths_lcc = []\n",
    "#         for G in graph_list:\n",
    "#             merged = nx.compose(G, G_biketrack)\n",
    "#             components = list(nx.weakly_connected_components(merged))\n",
    "#             max_length = 0.0\n",
    "#             for comp in components:\n",
    "#                 subgraph = merged.subgraph(comp)\n",
    "#                 total_length = sum(data.get('length', 0) for _, _, data in subgraph.edges(data=True))\n",
    "#                 if total_length > max_length:\n",
    "#                     max_length = total_length\n",
    "#             total_lengths_lcc.append(max_length)\n",
    "#         return total_lengths_lcc\n",
    "\n",
    "#     composite_data = {\n",
    "#         'composite_lcc_GTs': compute_lcc_lengths(GTs, G_biketrack),\n",
    "#         'composite_lcc_GTs_random': compute_lcc_lengths(GTs_random, G_biketrack),\n",
    "#         'composite_lcc_GTs_demand': compute_lcc_lengths(GTs_demand, G_biketrack),\n",
    "#         'composite_lcc_GTs_demand_ltn_priority': compute_lcc_lengths(GTs_demand_ltn_priority, G_biketrack),\n",
    "#         'composite_lcc_GTs_betweenness_ltn_priority': compute_lcc_lengths(GTs_betweenness_ltn_priority, G_biketrack)\n",
    "#     }\n",
    "\n",
    "#     analysis_results.update(composite_data)\n",
    "    \n",
    "#     with open(analysis_res_pickle, 'wb') as f:\n",
    "#         pickle.dump(analysis_results, f)\n",
    "#     df = pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()})\n",
    "#     df.to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# # Plot composite LCC results\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(\n",
    "#     analysis_results['composite_lcc_GTs'],\n",
    "#     '--', color='orange',\n",
    "#     label='Betweenness Growth (with Bike Track)'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['composite_lcc_GTs_random'],\n",
    "#     '-', color='blue',\n",
    "#     label='Random Growth (with Bike Track)'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['composite_lcc_GTs_demand'],\n",
    "#     '-.', color='red',\n",
    "#     label='Demand Growth (with Bike Track)'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['composite_lcc_GTs_demand_ltn_priority'],\n",
    "#     '-.', color='green',\n",
    "#     label='Demand LTN Growth (with Bike Track)'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     analysis_results['composite_lcc_GTs_betweenness_ltn_priority'],\n",
    "#     '-', color='purple',\n",
    "#     label='Betweenness LTN Growth (with Bike Track)'\n",
    "# )\n",
    "\n",
    "# plt.title('Largest Connected Component (Including Bike Network)')\n",
    "# plt.xlabel('Investment Iteration')\n",
    "# plt.ylabel('Total Length (meters)')\n",
    "# plt.grid(True, axis='y', alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# output_path = PATH[\"plots\"] + \"/\" + placeid + \"/lengthof_lcc_inc_cyclenet.png\"\n",
    "# plt.savefig(output_path, dpi=300)\n",
    "\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_lcc_lengths(graph_list, G_biketrack):\n",
    "#     \"\"\"Computes the total length of the largest connected component for each graph in the list.\"\"\"\n",
    "#     total_lengths_lcc = []\n",
    "    \n",
    "#     for G in graph_list:\n",
    "#         # Compose graphs and find largest connected component\n",
    "#         merged = nx.compose(G, G_biketrack)\n",
    "        \n",
    "#         # Get weakly connected components (works for both directed/undirected graphs)\n",
    "#         components = list(nx.weakly_connected_components(merged))\n",
    "        \n",
    "#         if not components:\n",
    "#             total_length = 0.0  # Handle empty graph case\n",
    "#         else:\n",
    "#             # Find largest component by node count\n",
    "#             largest_component_nodes = max(components, key=len)\n",
    "#             largest_component = merged.subgraph(largest_component_nodes)\n",
    "            \n",
    "#             # Calculate total edge length in the largest component\n",
    "#             total_length = sum(data['length'] for u, v, data in largest_component.edges(data=True))\n",
    "        \n",
    "#         total_lengths_lcc.append(total_length)\n",
    "    \n",
    "#     return total_lengths_lcc\n",
    "\n",
    "# # Compute LCC lengths for GTs and GTs_random\n",
    "# total_lengths_lcc_GTs = compute_lcc_lengths(GTs, G_biketrack)\n",
    "# total_lengths_lcc_GTs_random = compute_lcc_lengths(GTs_random, G_biketrack)\n",
    "\n",
    "# # Create the plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot GTs\n",
    "# plt.plot(\n",
    "#     range(len(total_lengths_lcc_GTs)), total_lengths_lcc_GTs, linestyle='-', color='blue', label=\"GTs\"\n",
    "# )\n",
    "\n",
    "# # Plot GTs_random\n",
    "# plt.plot(\n",
    "#     range(len(total_lengths_lcc_GTs_random)), total_lengths_lcc_GTs_random, linestyle='--', color='orange', label=\"GTs_random\"\n",
    "# )\n",
    "\n",
    "# # Labels and title\n",
    "# plt.title('Total Length of Largest Connected Component')\n",
    "# plt.xlabel('Graph Index')\n",
    "# plt.ylabel('Total Length (meters)')\n",
    "# plt.grid(True, axis='y', alpha=0.3)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# --- Load or init results ---\n",
    "analysis_results = load_results(analysis_res_pickle)\n",
    "if rerun or 'composite_lcc_lengths' not in analysis_results:\n",
    "    composite_lcc_data = {\n",
    "    'composite_lcc_lengths': [get_composite_lcc_length(G, G_biketrack) for G in GTs],\n",
    "    'random_runs_composite_lcc_lengths': [\n",
    "        [get_composite_lcc_length(G, G_biketrack) for G in run['GTs']] for run in random_runs\n",
    "    ],\n",
    "    'composite_lcc_lengths_demand': [get_composite_lcc_length(G, G_biketrack) for G in GTs_demand],\n",
    "    'composite_lcc_lengths_demand_ltn_priority': [get_composite_lcc_length(G, G_biketrack) for G in GTs_demand_ltn_priority],\n",
    "    'composite_lcc_lengths_betweenness_ltn_priority': [get_composite_lcc_length(G, G_biketrack) for G in GTs_betweenness_ltn_priority]\n",
    "    }\n",
    "\n",
    "    composite_lcc_data['random_composite_lcc_lengths_mean'] = np.mean(\n",
    "        composite_lcc_data['random_runs_composite_lcc_lengths'], axis=0\n",
    "    ).tolist()\n",
    "\n",
    "    analysis_results.update(composite_lcc_data)\n",
    "    save_results(analysis_results, analysis_res_pickle, analysis_res_csv)\n",
    "\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for run in analysis_results['random_runs_composite_lcc_lengths']:\n",
    "    plt.plot(run, color='lightgray', linewidth=1, alpha=0.4)\n",
    "\n",
    "plt.plot(\n",
    "    analysis_results['random_composite_lcc_lengths_mean'],\n",
    "    linestyle='--', linewidth=2, label='Random Growth (mean)', color='blue'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['composite_lcc_lengths'],\n",
    "    '-', label='Betweenness Growth', color='orange'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['composite_lcc_lengths_demand'],\n",
    "    '-.', label='Demand Growth', color='red'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['composite_lcc_lengths_demand_ltn_priority'],\n",
    "    ':', label='Demand LTN Priority Growth', color='green'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['composite_lcc_lengths_betweenness_ltn_priority'],\n",
    "    '-', label='Betweenness LTN Priority Growth', color='purple'\n",
    ")\n",
    "plt.xlabel('Investment Iteration')\n",
    "plt.ylabel('LCC Length (meters)')\n",
    "plt.title('Size of Largest Connected Component Including Bike Network per Iteration')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(PATH[\"plots\"], placeid, \"size_of_composite_lcc.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = load_results(analysis_res_pickle)\n",
    "\n",
    "# Extract baseline and strategies\n",
    "random_runs_array = np.array(analysis_results['random_runs_composite_lcc_lengths'])\n",
    "random_lcc_mean = np.array(analysis_results['random_composite_lcc_lengths_mean'])\n",
    "\n",
    "# Compute deviation of each strategy from mean random baseline\n",
    "composite_deviations = {\n",
    "    'Betweenness': np.array(analysis_results['composite_lcc_lengths']) - random_lcc_mean,\n",
    "    'Demand': np.array(analysis_results['composite_lcc_lengths_demand']) - random_lcc_mean,\n",
    "    'Demand LTN': np.array(analysis_results['composite_lcc_lengths_demand_ltn_priority']) - random_lcc_mean,\n",
    "    'Betweenness LTN': np.array(analysis_results['composite_lcc_lengths_betweenness_ltn_priority']) - random_lcc_mean,\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot baseline (mean of random runs)\n",
    "plt.plot(random_lcc_mean, color='blue', linestyle='-', linewidth=2, label='Random Growth Mean Baseline')\n",
    "for run in random_runs_array:\n",
    "    plt.plot(run, color='lightgray', linewidth=1, alpha=0.3)\n",
    "\n",
    "# Plot strategies relative to baseline\n",
    "styles = {\n",
    "    'Betweenness': ('--', 'orange'),\n",
    "    'Demand': ('-.', 'red'),\n",
    "    'Demand LTN': (':', 'green'),\n",
    "    'Betweenness LTN': ('-', 'purple'),\n",
    "}\n",
    "\n",
    "for label, deviation in composite_deviations.items():\n",
    "    absolute_values = deviation + random_lcc_mean  # Plot absolute LCC lengths\n",
    "    linestyle, color = styles[label]\n",
    "    plt.plot(range(1, len(absolute_values) + 1), absolute_values, linestyle=linestyle, color=color, label=label)\n",
    "\n",
    "# Finalize plot\n",
    "plt.title('Composite LCC Length Compared to Random Growth Baseline')\n",
    "plt.xlabel('Investment Iteration')\n",
    "plt.ylabel('Total Length (meters)')\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(PATH[\"plots\"], placeid, \"lengthof_lcc_inc_cyclenet_vs_random_mean_baseline.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(analysis_res_pickle):\n",
    "#     with open(analysis_res_pickle, 'rb') as f:\n",
    "#         analysis_results = pickle.load(f)\n",
    "# else:\n",
    "#     analysis_results = {}\n",
    "\n",
    "# # Recalculate composite LCCs if needed\n",
    "# if rerun or 'composite_lcc_GTs' not in analysis_results:\n",
    "#     def compute_lcc_lengths(graph_list, G_biketrack):\n",
    "#         total_lengths_lcc = []\n",
    "#         for G in graph_list:\n",
    "#             merged = nx.compose(G, G_biketrack)\n",
    "#             components = list(nx.weakly_connected_components(merged))\n",
    "#             max_length = 0.0\n",
    "#             for comp in components:\n",
    "#                 subgraph = merged.subgraph(comp)\n",
    "#                 total_length = sum(data.get('length', 0) for _, _, data in subgraph.edges(data=True))\n",
    "#                 if total_length > max_length:\n",
    "#                     max_length = total_length\n",
    "#             total_lengths_lcc.append(max_length)\n",
    "#         return total_lengths_lcc\n",
    "\n",
    "#     composite_data = {\n",
    "#         'composite_lcc_GTs': compute_lcc_lengths(GTs, G_biketrack),\n",
    "#         'composite_lcc_GTs_random': compute_lcc_lengths(GTs_random, G_biketrack),\n",
    "#         'composite_lcc_GTs_demand': compute_lcc_lengths(GTs_demand, G_biketrack),\n",
    "#         'composite_lcc_GTs_demand_ltn_priority': compute_lcc_lengths(GTs_demand_ltn_priority, G_biketrack),\n",
    "#         'composite_lcc_GTs_betweenness_ltn_priority': compute_lcc_lengths(GTs_betweenness_ltn_priority, G_biketrack)\n",
    "#     }\n",
    "\n",
    "#     analysis_results.update(composite_data)\n",
    "#     with open(analysis_res_pickle, 'wb') as f:\n",
    "#         pickle.dump(analysis_results, f)\n",
    "#     df = pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()})\n",
    "#     df.to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# # Compute deviation from random\n",
    "# random_composite = np.array(analysis_results['composite_lcc_GTs_random'])\n",
    "\n",
    "# composite_deviations = {\n",
    "#     'Betweenness': {\n",
    "#         'values': np.array(analysis_results['composite_lcc_GTs']) - random_composite,\n",
    "#         'color': 'orange',\n",
    "#         'linestyle': '--'\n",
    "#     },\n",
    "#     'Demand': {\n",
    "#         'values': np.array(analysis_results['composite_lcc_GTs_demand']) - random_composite,\n",
    "#         'color': 'red',\n",
    "#         'linestyle': '-.'\n",
    "#     },\n",
    "#     'Demand LTN': {\n",
    "#         'values': np.array(analysis_results['composite_lcc_GTs_demand_ltn_priority']) - random_composite,\n",
    "#         'color': 'green',\n",
    "#         'linestyle': '-.'\n",
    "#     },\n",
    "#     'Betweenness LTN': {\n",
    "#         'values': np.array(analysis_results['composite_lcc_GTs_betweenness_ltn_priority']) - random_composite,\n",
    "#         'color': 'purple',\n",
    "#         'linestyle': '-'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Plot deviation from random\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for label, data in composite_deviations.items():\n",
    "#     plt.plot(\n",
    "#         range(1, len(data['values']) + 1),\n",
    "#         data['values'],\n",
    "#         linestyle=data['linestyle'],\n",
    "#         color=data['color'],\n",
    "#         label=label\n",
    "#     )\n",
    "\n",
    "# plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "# plt.title('Composite LCC Length (With Cycle Network) — Deviation from Random Growth (Baseline)')\n",
    "# plt.xlabel('Investment Iteration')\n",
    "# plt.ylabel('Deviation in Total Length vs Random (meters)')\n",
    "# plt.legend()\n",
    "# plt.grid(True, axis='y', alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Save\n",
    "# output_path = PATH[\"plots\"] + f\"/{placeid}/lengthof_lcc_inc_cyclenet__deviation_from_random.png\"\n",
    "# plt.savefig(output_path, dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # def total_length(G):\n",
    "# #     \"\"\"Computes total edge length in a graph.\"\"\"\n",
    "# #     return sum(data.get('length', 1) for _, _, data in G.edges(data=True))\n",
    "\n",
    "# # def compute_lcc_lengths(graph_list, G_biketrack):\n",
    "# #     \"\"\"Computes the total length of the largest connected component for each graph in the list.\"\"\"\n",
    "# #     lcc_lengths = []\n",
    "\n",
    "# #     for G in graph_list:\n",
    "# #         # Find the common nodes between G and G_biketrack\n",
    "# #         common_nodes = set(G.nodes) & set(G_biketrack.nodes)\n",
    "\n",
    "# #         # If there are no common nodes, we can't compose, so we skip and set LCC length to 0\n",
    "# #         if not common_nodes:\n",
    "# #             lcc_lengths.append(0.0)\n",
    "# #             continue\n",
    "        \n",
    "# #         # Create a subgraph of G_biketrack with only the common nodes\n",
    "# #         G_biketrack_subgraph = G_biketrack.subgraph(common_nodes)\n",
    "\n",
    "# #         # Merge G with the G_biketrack subgraph\n",
    "# #         merged = nx.compose(G, G_biketrack_subgraph)\n",
    "\n",
    "# #         # Find weakly connected components (works for both directed/undirected graphs)\n",
    "# #         components = list(nx.weakly_connected_components(merged))\n",
    "\n",
    "# #         if not components:\n",
    "# #             total_length_lcc = 0.0  # Handle empty graph case\n",
    "# #         else:\n",
    "# #             # Find the largest connected component by node count\n",
    "# #             largest_component_nodes = max(components, key=len)\n",
    "# #             largest_component = merged.subgraph(largest_component_nodes)\n",
    "\n",
    "# #             # Calculate total edge length in the largest component\n",
    "# #             total_length_lcc = sum(data.get('length', 1) for u, v, data in largest_component.edges(data=True))\n",
    "\n",
    "# #         lcc_lengths.append(total_length_lcc)\n",
    "\n",
    "# #     return lcc_lengths\n",
    "\n",
    "# # # Compute LCC lengths for both GTs and GTs_random\n",
    "# # lcc_lengths_GTs = compute_lcc_lengths(GTs, G_biketrack)\n",
    "# # lcc_lengths_GTs_random = compute_lcc_lengths(GTs_random, G_biketrack)\n",
    "\n",
    "# # # Create the plot\n",
    "# # plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # # Plot LCC lengths for GTs\n",
    "# # plt.plot(range(1, len(lcc_lengths_GTs) + 1), lcc_lengths_GTs, linestyle='-', color='blue', label=\"GTs - LCC Length\")\n",
    "\n",
    "# # # Plot LCC lengths for GTs_random\n",
    "# # plt.plot(range(1, len(lcc_lengths_GTs_random) + 1), lcc_lengths_GTs_random, linestyle='--', color='orange', label=\"GTs_random - LCC Length\")\n",
    "\n",
    "# # # Labels and title\n",
    "# # plt.xlabel(\"Graph Index\")\n",
    "# # plt.ylabel(\"Largest Connected Component Length (meters)\")\n",
    "# # plt.title(\"Largest Connected Component Length (GTs vs. GTs_random)\")\n",
    "# # plt.legend()\n",
    "# # plt.grid(True, alpha=0.3)\n",
    "\n",
    "# # # Show the plot\n",
    "# # plt.tight_layout()\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# if os.path.exists(analysis_res_pickle):\n",
    "#     with open(analysis_res_pickle, 'rb') as f:\n",
    "#         analysis_results = pickle.load(f)\n",
    "# else:\n",
    "#     analysis_results = {}\n",
    "\n",
    "# if rerun or 'lcc_lengths_GTs' not in analysis_results:\n",
    "\n",
    "#     def total_length(G):\n",
    "#         return sum(data.get('length', 1) for _, _, data in G.edges(data=True))  \n",
    "\n",
    "#     def compute_lcc_lengths(graph_list, G_biketrack):\n",
    "#         lcc_lengths = []\n",
    "#         for G in graph_list:\n",
    "#             common_nodes = set(G.nodes) & set(G_biketrack.nodes)\n",
    "#             if not common_nodes:\n",
    "#                 lcc_lengths.append(0.0)\n",
    "#                 continue\n",
    "\n",
    "#             G_biketrack_subgraph = G_biketrack.subgraph(common_nodes)\n",
    "#             merged = nx.compose(G, G_biketrack_subgraph)\n",
    "#             components = list(nx.weakly_connected_components(merged))\n",
    "\n",
    "#             max_length = 0.0\n",
    "#             for comp in components:\n",
    "#                 subgraph = merged.subgraph(comp)\n",
    "#                 total_length = sum(data.get('length', 1) for _, _, data in subgraph.edges(data=True))\n",
    "#                 if total_length > max_length:\n",
    "#                     max_length = total_length\n",
    "\n",
    "#             lcc_lengths.append(max_length)\n",
    "#         return lcc_lengths\n",
    "\n",
    "#     lcc_data = {\n",
    "#         'lcc_lengths_GTs':        compute_lcc_lengths(GTs, G_biketrack),\n",
    "#         'lcc_lengths_GTs_random': compute_lcc_lengths(GTs_random, G_biketrack),\n",
    "#         'lcc_lengths_GTs_demand': compute_lcc_lengths(GTs_demand, G_biketrack),\n",
    "#         'lcc_lengths_GTs_demand_ltn_priority': compute_lcc_lengths(GTs_demand_ltn_priority, G_biketrack),\n",
    "#         'lcc_lengths_GTs_betweenness_ltn_priority': compute_lcc_lengths(GTs_betweenness_ltn_priority, G_biketrack)\n",
    "#     }\n",
    "\n",
    "#     analysis_results.update(lcc_data)\n",
    "\n",
    "#     with open(analysis_res_pickle, 'wb') as f:\n",
    "#         pickle.dump(analysis_results, f)\n",
    "#     df = pd.DataFrame({k: pd.Series(v) for k,v in analysis_results.items()})\n",
    "#     df.to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(\n",
    "#     range(1, len(analysis_results['lcc_lengths_GTs']) + 1), \n",
    "#     analysis_results['lcc_lengths_GTs'], \n",
    "#     linestyle='-', color='orange',  \n",
    "#     label=\"Betweenness growth\"\n",
    "# )\n",
    "# plt.plot(\n",
    "#     range(1, len(analysis_results['lcc_lengths_GTs_random']) + 1), \n",
    "#     analysis_results['lcc_lengths_GTs_random'], \n",
    "#     linestyle='--', color='blue', \n",
    "#     label=\"Random growth\"\n",
    "# )\n",
    "# plt.plot(\n",
    "#     range(1, len(analysis_results['lcc_lengths_GTs_demand']) + 1), \n",
    "#     analysis_results['lcc_lengths_GTs_demand'], \n",
    "#     linestyle='-.', color='red', \n",
    "#     label=\"Demand growth\"\n",
    "# )\n",
    "# plt.plot(\n",
    "#     range(1, len(analysis_results['lcc_lengths_GTs_demand_ltn_priority']) + 1), \n",
    "#     analysis_results['lcc_lengths_GTs_demand_ltn_priority'], \n",
    "#     linestyle='-.', color='green', \n",
    "#     label=\"Demand LTN growth\"\n",
    "# )\n",
    "# plt.plot(\n",
    "#     range(1, len(analysis_results['lcc_lengths_GTs_betweenness_ltn_priority']) + 1), \n",
    "#     analysis_results['lcc_lengths_GTs_betweenness_ltn_priority'], \n",
    "#     linestyle='-', color='purple', \n",
    "#     label=\"Betweenness LTN growth\"\n",
    "# )\n",
    "\n",
    "# plt.xlabel(\"Graph Index\")\n",
    "# plt.ylabel(\"Largest Connected Component Length (meters)\")\n",
    "# plt.title(\"Largest Connected Component Length\")\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# output_path = PATH[\"plots\"] + \"/\" + placeid + \"/lcc_length.png\"\n",
    "# plt.savefig(output_path, dpi=300)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to running any coverage analysis, we create buffers of each graph to avoid re-calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if rerun == True or 'GTs_buffers' not in locals():\n",
    "#     GTs_buffers = []\n",
    "#     for G in GTs:\n",
    "#         gdf_edges = ox.graph_to_gdfs(G, nodes=False).to_crs(epsg=3857) # convert graph to geodataframe\n",
    "#         buffer_gdf = gdf_edges.geometry.buffer(buffer_walk).unary_union # make a buffer\n",
    "#         buffer_gdf = gpd.GeoDataFrame(geometry=[buffer_gdf], crs=gdf_edges.crs) # set crs and geometry\n",
    "#         buffer_gdf = buffer_gdf.to_crs(epsg=4326)\n",
    "#         GTs_buffers.append(buffer_gdf) # add buffer to a list\n",
    "#     with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers.pickle\", \"wb\") as f:\n",
    "#         pickle.dump(GTs_buffers, f) # save buffers\n",
    "\n",
    "#     GTs_buffers_random = []\n",
    "#     for G in GTs_random:\n",
    "#         gdf_edges = ox.graph_to_gdfs(G, nodes=False).to_crs(epsg=3857)\n",
    "#         buffer_gdf = gdf_edges.geometry.buffer(buffer_walk).unary_union\n",
    "#         buffer_gdf = gpd.GeoDataFrame(geometry=[buffer_gdf], crs=gdf_edges.crs)\n",
    "#         buffer_gdf = buffer_gdf.to_crs(epsg=4326)\n",
    "#         GTs_buffers_random.append(buffer_gdf)\n",
    "#     with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers_random.pickle\", \"wb\") as f:\n",
    "#         pickle.dump(GTs_buffers_random, f)\n",
    "\n",
    "#     GTs_buffers_demand = []\n",
    "#     for G in GTs_demand:\n",
    "#         gdf_edges = ox.graph_to_gdfs(G, nodes=False).to_crs(epsg=3857)\n",
    "#         buffer_gdf = gdf_edges.geometry.buffer(buffer_walk).unary_union\n",
    "#         buffer_gdf = gpd.GeoDataFrame(geometry=[buffer_gdf], crs=gdf_edges.crs)\n",
    "#         buffer_gdf = buffer_gdf.to_crs(epsg=4326)\n",
    "#         GTs_buffers_demand.append(buffer_gdf)\n",
    "#     with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers_demand.pickle\", \"wb\") as f:\n",
    "#         pickle.dump(GTs_buffers_demand, f)\n",
    "\n",
    "\n",
    "#     GTs_buffers_demand_ltn_priority = []\n",
    "#     for G in GTs_demand_ltn_priority:\n",
    "#         gdf_edges = ox.graph_to_gdfs(G, nodes=False).to_crs(epsg=3857)\n",
    "#         buffer_gdf = gdf_edges.geometry.buffer(buffer_walk).unary_union\n",
    "#         buffer_gdf = gpd.GeoDataFrame(geometry=[buffer_gdf], crs=gdf_edges.crs)\n",
    "#         buffer_gdf = buffer_gdf.to_crs(epsg=4326)\n",
    "#         GTs_buffers_demand_ltn_priority.append(buffer_gdf)\n",
    "#     with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers_demand_ltn_priority.pickle\", \"wb\") as f:\n",
    "#         pickle.dump(GTs_buffers_demand_ltn_priority, f)\n",
    "\n",
    "    \n",
    "#     GTs_buffers_betweenness_ltn_priority = []\n",
    "#     for G in GTs_betweenness_ltn_priority:\n",
    "#         gdf_edges = ox.graph_to_gdfs(G, nodes=False).to_crs(epsg=3857)\n",
    "#         buffer_gdf = gdf_edges.geometry.buffer(buffer_walk).unary_union\n",
    "#         buffer_gdf = gpd.GeoDataFrame(geometry=[buffer_gdf], crs=gdf_edges.crs)\n",
    "#         buffer_gdf = buffer_gdf.to_crs(epsg=4326)\n",
    "#         GTs_buffers_betweenness_ltn_priority.append(buffer_gdf)\n",
    "#     with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers_betweenness_ltn_priority.pickle\", \"wb\") as f:\n",
    "#         pickle.dump(GTs_buffers_betweenness_ltn_priority, f)\n",
    "\n",
    "# else:\n",
    "#     try:\n",
    "#         with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers.pickle\", \"rb\") as f:\n",
    "#             GTs_buffers = pickle.load(f)\n",
    "#         with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers_random.pickle\", \"rb\") as f:\n",
    "#             GTs_buffers_random = pickle.load(f)\n",
    "#         with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers_demand.pickle\", \"rb\") as f:\n",
    "#             GTs_buffers_demand = pickle.load(f)\n",
    "#         with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers_demand_ltn_priority.pickle\", \"rb\") as f:\n",
    "#             GTs_buffers_demand_ltn_priority = pickle.load(f)\n",
    "#         with open(PATH[\"results\"] + placeid + \"/\" + placeid + \"_GTs_buffers_betweenness_ltn_priority.pickle\", \"rb\") as f:\n",
    "#             GTs_buffers_betweenness_ltn_priority = pickle.load(f)\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"Buffer files not found. Please set rerun to True to regenerate them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(PATH[\"results\"], placeid, placeid)\n",
    "GTs_buffers = process_and_save_buffers_parallel(GTs, \"GTs_buffers\", rerun, base_path, buffer_walk)\n",
    "GTs_buffers_demand = process_and_save_buffers_parallel(GTs_demand, \"GTs_buffers_demand\", rerun, base_path, buffer_walk)\n",
    "GTs_buffers_demand_ltn_priority = process_and_save_buffers_parallel(GTs_demand_ltn_priority, \"GTs_buffers_demand_ltn_priority\", rerun, base_path, buffer_walk)\n",
    "GTs_buffers_betweenness_ltn_priority = process_and_save_buffers_parallel(GTs_betweenness_ltn_priority, \"GTs_buffers_betweenness_ltn_priority\", rerun, base_path, buffer_walk)\n",
    "# For multiple random runs\n",
    "GTs_buffers_random_all = []\n",
    "for run_id, run_res in enumerate(random_runs, start=1):\n",
    "    name = f\"GTs_buffers_random_run{run_id:02d}\"\n",
    "    buffers = process_and_save_buffers_parallel(run_res[\"GTs\"], name, rerun, base_path, buffer_walk)\n",
    "    GTs_buffers_random_all.append(buffers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # area\n",
    "# target_crs = \"EPSG:3857\"\n",
    "# boundary_proj = boundary.to_crs(target_crs)\n",
    "# total_area = boundary_proj.unary_union.area  # total area in m²\n",
    "\n",
    "# # Function to compute areas (km²) and percentage coverage for a list of buffers\n",
    "# def compute_metrics(buffer_list):\n",
    "#     areas = []\n",
    "#     percentages = []\n",
    "#     for gdf in buffer_list:\n",
    "#         gdf_proj = gdf.to_crs(target_crs)\n",
    "#         inter = gpd.overlay(gdf_proj, boundary_proj, how='intersection')\n",
    "#         inter_area = inter.unary_union.area if not inter.empty else 0\n",
    "#         areas.append(inter_area / 1e6)  # convert m² to km²\n",
    "#         percentages.append((inter_area / total_area * 100) if total_area else 0)\n",
    "#     return areas, percentages\n",
    "\n",
    "# # Compute metrics for both buffer sets\n",
    "# areas1, perc1 = compute_metrics(GTs_buffers)\n",
    "# areas2, perc2 = compute_metrics(GTs_buffers_random)\n",
    "\n",
    "# # Plot 1: Compare areas (in km²) for both buffer sets on one graph\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(areas1, 'b-o', label='GTs_buffers Area (km²)')\n",
    "# plt.plot(areas2, 'g-o', label='GTs_buffers_random Area (km²)')\n",
    "# plt.xlabel('Buffer Index')\n",
    "# plt.ylabel('Area (km²)')\n",
    "# plt.title('Boundary Intersection Area Comparison')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot 2: Compare coverage percentages for both buffer sets on one graph\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(perc1, 'r-s', label='GTs_buffers Coverage (%)')\n",
    "# plt.plot(perc2, 'm-s', label='GTs_buffers_random Coverage (%)')\n",
    "# plt.xlabel('Buffer Index')\n",
    "# plt.ylabel('Coverage (%)')\n",
    "# plt.title('Boundary Coverage Percentage Comparison')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Area analysis cell\n",
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {}\n",
    "\n",
    "if rerun or 'buffer_areas' not in analysis_results:\n",
    "    target_crs = \"EPSG:3857\"\n",
    "    boundary_proj = boundary.to_crs(target_crs)\n",
    "    total_area = boundary_proj.unary_union.area\n",
    "\n",
    "    def compute_metrics(buffer_list):\n",
    "        areas = []\n",
    "        percentages = []\n",
    "        for gdf in buffer_list:\n",
    "            gdf_proj = gdf.to_crs(target_crs)\n",
    "            inter = gpd.overlay(gdf_proj, boundary_proj, how='intersection')\n",
    "            inter_area = inter.unary_union.area if not inter.empty else 0\n",
    "            areas.append(inter_area / 1e6)  # Convert m² to km²\n",
    "            percentages.append((inter_area / total_area * 100) if total_area else 0)\n",
    "        return areas, percentages\n",
    "\n",
    "    buffer_metrics = {\n",
    "        'buffer_areas': compute_metrics(GTs_buffers)[0],\n",
    "        'buffer_percentages': compute_metrics(GTs_buffers)[1],\n",
    "        'random_buffer_areas': compute_metrics(GTs_buffers_random)[0],\n",
    "        'random_buffer_percentages': compute_metrics(GTs_buffers_random)[1],\n",
    "        'demand_buffer_areas': compute_metrics(GTs_buffers_demand)[0],\n",
    "        'demand_buffer_percentages': compute_metrics(GTs_buffers_demand)[1],\n",
    "        'demand_buffer_areas_ltn_priority': compute_metrics(GTs_buffers_demand_ltn_priority)[0],\n",
    "        'demand_buffer_percentages_ltn_priority': compute_metrics(GTs_buffers_demand_ltn_priority)[1],\n",
    "        'betweenness_buffer_areas_ltn_priority': compute_metrics(GTs_buffers_betweenness_ltn_priority)[0],\n",
    "        'betweenness_buffer_percentages_ltn_priority': compute_metrics(GTs_buffers_betweenness_ltn_priority)[1]\n",
    "    }\n",
    "\n",
    "    analysis_results.update(buffer_metrics)\n",
    "\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()}).to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# Plotting - Area (km²)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    analysis_results['buffer_areas'], \n",
    "    color='orange', \n",
    "    linestyle='-', \n",
    "    label='Betweenness Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['random_buffer_areas'], \n",
    "    color='blue', \n",
    "    linestyle='--', \n",
    "    label='Random Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['demand_buffer_areas'], \n",
    "    color='red', \n",
    "    linestyle='-.', \n",
    "    label='Demand Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['demand_buffer_areas_ltn_priority'],\n",
    "    color='green',\n",
    "    linestyle=':',\n",
    "    label='Demand LTN Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['betweenness_buffer_areas_ltn_priority'],\n",
    "    color='purple',\n",
    "    linestyle='-',\n",
    "    label='Betweenness LTN Growth'\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel('Growth Iteration')\n",
    "plt.ylabel('Area (km²)')\n",
    "plt.title('Total Area Coverage')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True, alpha=0.3)\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/area_coverage_km2.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plotting - Percentage Coverage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    analysis_results['buffer_percentages'], \n",
    "    color='orange', \n",
    "    linestyle='-', \n",
    "    label='Betweeness Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['random_buffer_percentages'], \n",
    "    color='blue', \n",
    "    linestyle='--', \n",
    "    label='Random Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['demand_buffer_percentages'], \n",
    "    color='red', \n",
    "    linestyle='-.', \n",
    "    label='Demand Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['demand_buffer_percentages_ltn_priority'],\n",
    "    color='green',\n",
    "    linestyle=':',\n",
    "    label='Demand LTN Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['betweenness_buffer_percentages_ltn_priority'],\n",
    "    color='purple',\n",
    "    linestyle='-',\n",
    "    label='Betweenness LTN Growth'\n",
    ")\n",
    "\n",
    "plt.xlabel('Growth Iteration')\n",
    "plt.ylabel('Coverage (%)')\n",
    "plt.title('Boundary Coverage')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True, alpha=0.3)\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/boundary_cov_percentage.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streets coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_crs = G_biketrackcarall_edges.crs\n",
    "# total_network_length = G_biketrackcarall_edges[\"length\"].sum()\n",
    "\n",
    "# def compute_street_coverage(buffer_list):\n",
    "#     lengths = []\n",
    "#     percentages = []\n",
    "#     for gdf in buffer_list:\n",
    "#         # Reproject buffers to network CRS if needed\n",
    "#         gdf_proj = gdf.to_crs(network_crs)\n",
    "#         # Compute intersection between network and buffer\n",
    "#         inter = gpd.overlay(G_biketrackcarall_edges, gdf_proj, how='intersection')\n",
    "#         # Sum the existing \"length\" values from the intersected segments\n",
    "#         seg_length = inter[\"length\"].sum() if not inter.empty else 0\n",
    "#         lengths.append(seg_length)\n",
    "#         percentages.append((seg_length / total_network_length * 100) if total_network_length else 0)\n",
    "#     return lengths, percentages\n",
    "\n",
    "# # Compute metrics for both buffer sets\n",
    "# net_lengths1, net_perc1 = compute_street_coverage(GTs_buffers)\n",
    "# net_lengths2, net_perc2 = compute_street_coverage(GTs_buffers_random)\n",
    "\n",
    "# # Plot 1: Compare network lengths (in meters) within each buffer\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(net_lengths1, 'b-o', label='GTs_buffers Network (m)')\n",
    "# plt.plot(net_lengths2, 'g-o', label='GTs_buffers_random Network (m)')\n",
    "# plt.xlabel('Buffer Index')\n",
    "# plt.ylabel('Network Length (m)')\n",
    "# plt.title('Street Network Length within Buffers')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot 2: Compare network coverage percentages\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(net_perc1, 'r-s', label='GTs_buffers Coverage (%)')\n",
    "# plt.plot(net_perc2, 'm-s', label='GTs_buffers_random Coverage (%)')\n",
    "# plt.xlabel('Buffer Index')\n",
    "# plt.ylabel('Coverage (%)')\n",
    "# plt.title('Percentage of Total Network within Buffers')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {}\n",
    "\n",
    "\n",
    "\n",
    "if rerun or 'street_lengths' not in analysis_results:\n",
    "    network_crs = G_biketrackcarall_edges.crs\n",
    "    total_network_length = G_biketrackcarall_edges[\"length\"].sum()\n",
    "\n",
    "    # simplfy to reduce computation time\n",
    "    proj_crs = network_crs if network_crs.is_projected else \"EPSG:3857\"\n",
    "    edges_proj = G_biketrackcarall_edges.to_crs(proj_crs)\n",
    "    edges_simpl = edges_proj.copy()\n",
    "    edges_simpl.geometry = edges_proj.geometry.simplify(tolerance=10,\n",
    "                                                         preserve_topology=True)\n",
    "    edges_simpl = edges_simpl.to_crs(network_crs)\n",
    "\n",
    "    def compute_street_coverage(buffer_list):\n",
    "        lengths = []\n",
    "        percentages = []\n",
    "        for gdf in buffer_list:\n",
    "            gdf_proj = gdf.to_crs(network_crs)\n",
    "            # simplfy to reduce computation time\n",
    "            gdf_proj = gdf.to_crs(proj_crs).copy()\n",
    "            gdf_proj.geometry = gdf_proj.geometry.simplify(tolerance=10,\n",
    "                                                           preserve_topology=True)\n",
    "            gdf_proj = gdf_proj.to_crs(network_crs)\n",
    "            \n",
    "            inter = gpd.overlay(G_biketrackcarall_edges, gdf_proj, how='intersection')\n",
    "            seg_length = inter[\"length\"].sum() if not inter.empty else 0\n",
    "            lengths.append(seg_length)\n",
    "            percentages.append((seg_length / total_network_length * 100) if total_network_length else 0)\n",
    "        return lengths, percentages\n",
    "\n",
    "    street_metrics = {\n",
    "        'street_cov_lengths': compute_street_coverage(GTs_buffers)[0],\n",
    "        'street_cov_percentages': compute_street_coverage(GTs_buffers)[1],\n",
    "        'random_street_cov_lengths': compute_street_coverage(GTs_buffers_random)[0],\n",
    "        'random_street_cov_percentages': compute_street_coverage(GTs_buffers_random)[1],\n",
    "        'demand_street_cov_lengths': compute_street_coverage(GTs_buffers_demand)[0],\n",
    "        'demand_street_cov_percentages': compute_street_coverage(GTs_buffers_demand)[1],\n",
    "        'demand_street_cov_lengths_ltn_priority': compute_street_coverage(GTs_buffers_demand_ltn_priority)[0],\n",
    "        'demand_street_cov_percentages_ltn_priority': compute_street_coverage(GTs_buffers_demand_ltn_priority)[1],\n",
    "        'betweenness_street_cov_lengths_ltn_priority': compute_street_coverage(GTs_buffers_betweenness_ltn_priority)[0],\n",
    "        'betweenness_street_cov_percentages_ltn_priority': compute_street_coverage(GTs_buffers_betweenness_ltn_priority)[1]\n",
    "    }\n",
    "\n",
    "    analysis_results.update(street_metrics)\n",
    "\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "    df = pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()})\n",
    "    df.to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# Plot: Network Length within Buffers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(analysis_results['street_cov_lengths'], color='orange', linestyle='-', label='Betweenness Growth')\n",
    "plt.plot(analysis_results['random_street_cov_lengths'], color='blue', linestyle='--', label='Random Growth')\n",
    "plt.plot(analysis_results['demand_street_cov_lengths'], color='red', linestyle='-.', label='Demand Growth')\n",
    "plt.plot(analysis_results['demand_street_cov_lengths_ltn_priority'], color='green', linestyle=':', label='Demand LTN Growth')\n",
    "plt.plot(analysis_results['betweenness_street_cov_lengths_ltn_priority'], color='purple', linestyle='-', label='Betweenness LTN Growth')\n",
    "plt.xlabel('Growth Iteration')\n",
    "plt.ylabel('Street Network Length (m)')\n",
    "plt.title('Street Network Length within Buffers')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/streets_within_cyclenet.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot: Percentage of Network within Buffers\n",
    "plt.figure(10, 6)\n",
    "plt.plot(analysis_results['street_cov_percentages'], color='orange', linestyle='-', label='Betweenness Growth')\n",
    "plt.plot(analysis_results['random_street_cov_percentages'], color='blue', linestyle='--', label='Random Growth')\n",
    "plt.plot(analysis_results['demand_street_cov_percentages'], color='red', linestyle='-.', label='Demand Growth')\n",
    "plt.plot(analysis_results['demand_street_cov_percentages_ltn_priority'], color='green', linestyle=':', label='Demand LTN Growth')\n",
    "plt.plot(analysis_results['betweenness_street_cov_percentages_ltn_priority'], color='purple', linestyle='-', label='Betweenness LTN Growth')\n",
    "plt.xlabel('Growth Iteration')\n",
    "plt.ylabel('Coverage (%)')\n",
    "plt.title('Percentage of Total Network within Buffers')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/percentage_within_cyclenet.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get population data from census, asign census data to buildings, find population within cycle route buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get lsoas and population\n",
    "# lsoa_bound = gpd.read_file(PATH[\"data\"] + \"/\" + placeid + \"/lsoa_bound.gpkg\")\n",
    "# boundary = ox.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "# lsoa_bound = gpd.clip(lsoa_bound, boundary)\n",
    "# lsoa_bound = add_lsoa_population(lsoa_bound) # using 2011 census data\n",
    "\n",
    "# # get buildings\n",
    "# buildings = get_building_populations(lsoa_bound, boundary) ## add more detail??\n",
    "# buildings = buildings.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # pop_counts_GT = []\n",
    "# # pop_counts_random_GT = []\n",
    "\n",
    "\n",
    "# # # Function to calculate total pop_count within each buffer\n",
    "# # def calculate_pop_count(buffers_list, buildings):\n",
    "# #     pop_counts = []\n",
    "# #     for buffer in buffers_list:\n",
    "# #         intersecting_buildings = gpd.sjoin(buildings, buffer, predicate=\"intersects\")\n",
    "# #         total_pop = intersecting_buildings[\"pop_assigned\"].sum()\n",
    "# #         pop_counts.append(total_pop)\n",
    "# #     return pop_counts\n",
    "\n",
    "# # # Calculate for both sets of buffers\n",
    "# # pop_counts_GT = calculate_pop_count(GTs_buffers, buildings)\n",
    "# # pop_counts_random_GT = calculate_pop_count(GTs_buffers_random, buildings)\n",
    "\n",
    "# # plt.figure(figsize=(10, 5))\n",
    "# # buffer_indices = np.arange(len(GTs_buffers))  # Common x-axis indices for both datasets\n",
    "\n",
    "# # plt.plot(buffer_indices, pop_counts_GT, label=\"GTs Buffers\", linestyle='-', color='blue')\n",
    "# # plt.plot(buffer_indices, pop_counts_random_GT, label=\"Random GTs Buffers\", linestyle='--', color='orange')\n",
    "\n",
    "# # plt.xlabel(\"Buffer Index\")\n",
    "# # plt.ylabel(\"Total Population Count\")\n",
    "# # plt.title(\"Comparison of Population Within Buffers\")\n",
    "# # plt.legend()\n",
    "# # plt.grid(True)\n",
    "# # plt.show()\n",
    "# if os.path.exists(analysis_res_pickle):\n",
    "#     with open(analysis_res_pickle, 'rb') as f:\n",
    "#         analysis_results = pickle.load(f)\n",
    "# else:\n",
    "#     analysis_results = {}\n",
    "\n",
    "# if rerun or 'pop_counts_GT' not in analysis_results:\n",
    "#     def calculate_pop_count(buffers_list, buildings):\n",
    "#         pop_counts = []\n",
    "#         for buffer in buffers_list:\n",
    "#             intersecting_buildings = gpd.sjoin(buildings, buffer, predicate=\"intersects\")\n",
    "#             pop_counts.append(intersecting_buildings[\"pop_assigned\"].sum())\n",
    "#         return pop_counts\n",
    "\n",
    "#     pop_metrics = {\n",
    "#         'pop_counts_GT': calculate_pop_count(GTs_buffers, buildings),\n",
    "#         'pop_counts_random_GT': calculate_pop_count(GTs_buffers_random, buildings),\n",
    "#         'pop_counts_demand_GT': calculate_pop_count(GTs_buffers_demand, buildings)\n",
    "#     }\n",
    "\n",
    "#     analysis_results.update(pop_metrics)\n",
    "\n",
    "#     with open(analysis_res_pickle, 'wb') as f:\n",
    "#         pickle.dump(analysis_results, f)\n",
    "#     df = pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()})\n",
    "#     df.to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# buffer_indices = np.arange(len(GTs_buffers))\n",
    "\n",
    "# plt.plot(\n",
    "#     buffer_indices,\n",
    "#     analysis_results['pop_counts_GT'],\n",
    "#     label=\"Betweenness Growth\",\n",
    "#     linestyle='-',\n",
    "#     color='orange'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     buffer_indices,\n",
    "#     analysis_results['pop_counts_random_GT'],\n",
    "#     label=\"Random Growth\",\n",
    "#     linestyle='--',\n",
    "#     color='blue'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     buffer_indices,\n",
    "#     analysis_results['pop_counts_demand_GT'],\n",
    "#     label=\"Demand-based Growth\",\n",
    "#     linestyle='-.',\n",
    "#     color='red'\n",
    "# )\n",
    "\n",
    "# plt.xlabel(\"Buffer Index\")\n",
    "# plt.ylabel(\"Total Population Count\")\n",
    "# plt.title(\"Population Within Buffers Over Investment Iterations\")\n",
    "# plt.legend()\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POI coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seed points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_buffers = []\n",
    "# counts_random = []\n",
    "\n",
    "# # Iterate over each buffer GeoDataFrame in GTs_buffers\n",
    "# for gdf in GTs_buffers:\n",
    "#     # Create a union of all polygons in the buffer gdf (if there is more than one)\n",
    "#     buffer_union = gdf.unary_union\n",
    "#     # Count the points in combined_points that fall within this union\n",
    "#     count = combined_points.within(buffer_union).sum()\n",
    "#     counts_buffers.append(count)\n",
    "\n",
    "# # Do the same for GTs_buffers_random\n",
    "# for gdf in GTs_buffers_random:\n",
    "#     buffer_union = gdf.unary_union\n",
    "#     count = combined_points.within(buffer_union).sum()\n",
    "#     counts_random.append(count)\n",
    "\n",
    "# # Plotting the results on a line graph\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# x_vals = range(1, len(counts_buffers) + 1)  # Assuming you want x-axis as buffer index\n",
    "\n",
    "# plt.plot(x_vals, counts_buffers, marker='o', label='GTs_buffers')\n",
    "# plt.plot(x_vals, counts_random, marker='o', label='GTs_buffers_random')\n",
    "\n",
    "# plt.xlabel('Buffer Index')\n",
    "# plt.ylabel('Number of Points Covered')\n",
    "# plt.title('Points Covered by Each Buffer')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# Seed point analysis cell\n",
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {}  \n",
    "\n",
    "if rerun or 'points_covered_GT' not in analysis_results:\n",
    "    point_metrics = {\n",
    "        'points_covered_GT': [\n",
    "            combined_points.within(gdf.unary_union).sum()\n",
    "            for gdf in GTs_buffers\n",
    "        ],\n",
    "        'points_covered_random': [\n",
    "            combined_points.within(gdf.unary_union).sum()\n",
    "            for gdf in GTs_buffers_random\n",
    "        ],\n",
    "        'points_covered_demand': [\n",
    "            combined_points.within(gdf.unary_union).sum()\n",
    "            for gdf in GTs_buffers_demand\n",
    "        ],\n",
    "        'points_covered_demand_ltn_priority': [\n",
    "            combined_points.within(gdf.unary_union).sum()\n",
    "            for gdf in GTs_buffers_demand_ltn_priority\n",
    "        ],\n",
    "        'points_covered_betweenness_ltn_priority': [\n",
    "            combined_points.within(gdf.unary_union).sum()\n",
    "            for gdf in GTs_buffers_betweenness_ltn_priority\n",
    "        ]\n",
    "    }\n",
    "    analysis_results.update(point_metrics)\n",
    "\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()}).to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_vals = range(1, len(analysis_results['points_covered_GT']) + 1)\n",
    "\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['points_covered_GT'],\n",
    "    color='orange',\n",
    "    linestyle='-',\n",
    "    label='Betweenness Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['points_covered_random'],\n",
    "    color='blue',\n",
    "    linestyle='--',\n",
    "    label='Random Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['points_covered_demand'],\n",
    "    color='red',\n",
    "    linestyle='-.',\n",
    "    label='Demand-based Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['points_covered_demand_ltn_priority'],\n",
    "    color='green',\n",
    "    linestyle=':',\n",
    "    label='Demand LTN Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['points_covered_betweenness_ltn_priority'],\n",
    "    color='purple',\n",
    "    linestyle='-',\n",
    "    label='Betweenness LTN Growth'\n",
    ")\n",
    "\n",
    "plt.xlabel('Growth Iteration')\n",
    "plt.ylabel('Number of Points Covered')\n",
    "plt.title('Seed Points Covered by Cycle Network')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/seed_point_coverage.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LTN Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_buffers = []\n",
    "# counts_random = []\n",
    "\n",
    "# # Iterate over each buffer GeoDataFrame in GTs_buffers\n",
    "# for gdf in GTs_buffers:\n",
    "#     # Create a union of all polygons in the buffer gdf (if there is more than one)\n",
    "#     buffer_union = gdf.unary_union\n",
    "#     # Count the points that fall within this union\n",
    "#     count = ltn_points.within(buffer_union).sum()\n",
    "#     counts_buffers.append(count)\n",
    "\n",
    "# # Do the same for GTs_buffers_random\n",
    "# for gdf in GTs_buffers_random:\n",
    "#     buffer_union = gdf.unary_union\n",
    "#     count = ltn_points.within(buffer_union).sum()\n",
    "#     counts_random.append(count)\n",
    "\n",
    "# # Plotting the results on a line graph\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# x_vals = range(1, len(counts_buffers) + 1)  # Assuming you want x-axis as buffer index\n",
    "\n",
    "# plt.plot(x_vals, counts_buffers, marker='o', label='GTs_buffers')\n",
    "# plt.plot(x_vals, counts_random, marker='o', label='GTs_buffers_random')\n",
    "\n",
    "# plt.xlabel('Buffer Index')\n",
    "# plt.ylabel('Number of Points Covered')\n",
    "# plt.title('Points Covered by Each Buffer')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# LTN point coverage analysis cell\n",
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {} \n",
    "\n",
    "if rerun or 'ltn_points_covered_GT' not in analysis_results:\n",
    "    def compute_ltn_coverage(buffers_list):\n",
    "        return [\n",
    "            ltn_points.within(gdf.unary_union).sum()\n",
    "            for gdf in buffers_list\n",
    "        ]\n",
    "    \n",
    "    analysis_results.update({\n",
    "        'ltn_points_covered_GT': compute_ltn_coverage(GTs_buffers),\n",
    "        'ltn_points_covered_random': compute_ltn_coverage(GTs_buffers_random),\n",
    "        'ltn_points_covered_demand': compute_ltn_coverage(GTs_buffers_demand),\n",
    "        'ltn_points_covered_demand_ltn_priority': compute_ltn_coverage(GTs_buffers_demand_ltn_priority),\n",
    "        'ltn_points_covered_betweenness_ltn_priority': compute_ltn_coverage(GTs_buffers_betweenness_ltn_priority)\n",
    "    })\n",
    "\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()}).to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_vals = range(1, len(analysis_results['ltn_points_covered_GT']) + 1)\n",
    "\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['ltn_points_covered_GT'],\n",
    "    color='orange',\n",
    "    linestyle='-',\n",
    "    label='Betweenness Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['ltn_points_covered_random'],\n",
    "    color='blue',\n",
    "    linestyle='--',\n",
    "    label='Random Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['ltn_points_covered_demand'],\n",
    "    color='red',\n",
    "    linestyle='-.',\n",
    "    label='Demand-based Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['ltn_points_covered_demand_ltn_priority'],\n",
    "    color='green',\n",
    "    linestyle=':',\n",
    "    label='Demand LTN Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['ltn_points_covered_betweenness_ltn_priority'],\n",
    "    color='purple',\n",
    "    linestyle='-',\n",
    "    label='Betweenness LTN Growth'\n",
    ")\n",
    "\n",
    "plt.xlabel('Growth Iteration')\n",
    "plt.ylabel('Number of LTN Points Covered')\n",
    "plt.title('LTNs Covered by Cycle Network')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/ltns_coverage.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All neighbourhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about how if we were to create future LTNs, where could these go based purely on making more cycling safe?\n",
    "\n",
    "# should these be where the most cycling is on? or which area has the longest bit of cycle network added? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts_buffers = []\n",
    "# counts_random = []\n",
    "\n",
    "# # Iterate over each buffer GeoDataFrame in GTs_buffers\n",
    "# for gdf in GTs_buffers:\n",
    "#     # Create a union of all polygons in the buffer gdf (if there is more than one)\n",
    "#     buffer_union = gdf.unary_union\n",
    "#     # Count the points in combined_points that fall within this union\n",
    "#     count = all_neighbourhoods_centroids.within(buffer_union).sum()\n",
    "#     counts_buffers.append(count)\n",
    "\n",
    "# # Do the same for GTs_buffers_random\n",
    "# for gdf in GTs_buffers_random:\n",
    "#     buffer_union = gdf.unary_union\n",
    "#     count = all_neighbourhoods_centroids.within(buffer_union).sum()\n",
    "#     counts_random.append(count)\n",
    "\n",
    "# # Plotting the results on a line graph\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# x_vals = range(1, len(counts_buffers) + 1)  # Assuming you want x-axis as buffer index\n",
    "\n",
    "# plt.plot(x_vals, counts_buffers, marker='o', label='GTs_buffers')\n",
    "# plt.plot(x_vals, counts_random, marker='o', label='GTs_buffers_random')\n",
    "\n",
    "# plt.xlabel('Buffer Index')\n",
    "# plt.ylabel('Number of Points Covered')\n",
    "# plt.title('Points Covered by Each Buffer')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# Neighborhood centroids analysis cell\n",
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {} \n",
    "\n",
    "if rerun or 'neighborhood_points_covered_GT' not in analysis_results:\n",
    "    def count_neighborhood_coverage(buffers_list):\n",
    "        return [\n",
    "            all_neighbourhoods_centroids.within(gdf.unary_union).sum()\n",
    "            for gdf in buffers_list\n",
    "        ]\n",
    "\n",
    "    neighborhood_metrics = {\n",
    "        'neighborhood_points_covered_GT': count_neighborhood_coverage(GTs_buffers),\n",
    "        'neighborhood_points_covered_random': count_neighborhood_coverage(GTs_buffers_random),\n",
    "        'neighborhood_points_covered_demand': count_neighborhood_coverage(GTs_buffers_demand),\n",
    "        'neighborhood_points_covered_demand_ltn_priority': count_neighborhood_coverage(GTs_buffers_demand_ltn_priority),\n",
    "        'neighborhood_points_covered_betweenness_ltn_priority': count_neighborhood_coverage(GTs_buffers_betweenness_ltn_priority)\n",
    "    }\n",
    "\n",
    "    analysis_results.update(neighborhood_metrics)\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()}).to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_vals = range(1, len(analysis_results['neighborhood_points_covered_GT']) + 1)\n",
    "\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['neighborhood_points_covered_GT'],\n",
    "    color='orange',\n",
    "    linestyle='-',\n",
    "    label='Betweenness Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['neighborhood_points_covered_random'],\n",
    "    color='blue',\n",
    "    linestyle='--',\n",
    "    label='Random Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['neighborhood_points_covered_demand'],\n",
    "    color='red',\n",
    "    linestyle='-.',\n",
    "    label='Demand-based Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['neighborhood_points_covered_demand_ltn_priority'],\n",
    "    color='green',\n",
    "    linestyle=':',\n",
    "    label='Demand LTN Growth'\n",
    ")\n",
    "plt.plot(\n",
    "    x_vals,\n",
    "    analysis_results['neighborhood_points_covered_betweenness_ltn_priority'],\n",
    "    color='purple',\n",
    "    linestyle='-',\n",
    "    label='Betweenness LTN Growth'\n",
    ")\n",
    "\n",
    "plt.xlabel('Growth Iteration')\n",
    "plt.ylabel('Neighbourhoods Covered')\n",
    "plt.title('Neighbourhoods Covered by Cycle Network')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/neighbourhoods_coverage.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## against random baseline\n",
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {} \n",
    "\n",
    "if rerun or 'neighborhood_points_covered_GT' not in analysis_results:\n",
    "    def count_neighborhood_coverage(buffers_list):\n",
    "        return [\n",
    "            all_neighbourhoods_centroids.within(gdf.unary_union).sum()\n",
    "            for gdf in buffers_list\n",
    "        ]\n",
    "\n",
    "    neighborhood_metrics = {\n",
    "        'neighborhood_points_covered_GT': count_neighborhood_coverage(GTs_buffers),\n",
    "        'neighborhood_points_covered_random': count_neighborhood_coverage(GTs_buffers_random),\n",
    "        'neighborhood_points_covered_demand': count_neighborhood_coverage(GTs_buffers_demand),\n",
    "        'neighborhood_points_covered_demand_ltn_priority': count_neighborhood_coverage(GTs_buffers_demand_ltn_priority),\n",
    "        'neighborhood_points_covered_betweenness_ltn_priority': count_neighborhood_coverage(GTs_buffers_betweenness_ltn_priority)\n",
    "    }\n",
    "\n",
    "    analysis_results.update(neighborhood_metrics)\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()}) \\\n",
    "        .to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "# Calculate deviation from random\n",
    "random_coverage = np.array(analysis_results['neighborhood_points_covered_random'])\n",
    "\n",
    "coverage_deviations = {\n",
    "    'Betweenness': {\n",
    "        'values': np.array(analysis_results['neighborhood_points_covered_GT']) - random_coverage,\n",
    "        'color': 'orange',\n",
    "        'linestyle': '-'\n",
    "    },\n",
    "    'Demand': {\n",
    "        'values': np.array(analysis_results['neighborhood_points_covered_demand']) - random_coverage,\n",
    "        'color': 'red',\n",
    "        'linestyle': '-.'\n",
    "    },\n",
    "    'Demand LTN': {\n",
    "        'values': np.array(analysis_results['neighborhood_points_covered_demand_ltn_priority']) - random_coverage,\n",
    "        'color': 'green',\n",
    "        'linestyle': ':'\n",
    "    },\n",
    "    'Betweenness LTN': {\n",
    "        'values': np.array(analysis_results['neighborhood_points_covered_betweenness_ltn_priority']) - random_coverage,\n",
    "        'color': 'purple',\n",
    "        'linestyle': '-'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot deviation from random\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_vals = range(1, len(random_coverage) + 1)\n",
    "\n",
    "for label, data in coverage_deviations.items():\n",
    "    plt.plot(\n",
    "        x_vals,\n",
    "        data['values'],\n",
    "        linestyle=data['linestyle'],\n",
    "        color=data['color'],\n",
    "        label=label\n",
    "    )\n",
    "\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Growth Iteration')\n",
    "plt.ylabel('Deviation in Neighbourhoods Covered (vs Random)')\n",
    "plt.title('Neighbourhood Coverage — Deviation from Random Growth (Baseline)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "output_path = PATH[\"plots\"] + f\"/{placeid}/neighbourhoods_coverage__deviation_from_random.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlap with existing infrastructure. Finding how much of the existing network we overlap, in terms of edges, distance, and % of total network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_against_reference(graph_list1, graph_list2, reference_graph):\n",
    "#     \"\"\"\n",
    "#     Compare two lists of graphs against a reference, calculating both:\n",
    "#     1. How much of the reference is covered by each graph (original metric)\n",
    "#     2. How much of each graph is covered by the reference (reverse metric)\n",
    "#     \"\"\"\n",
    "#     def calculate_both_ways(graph, reference):\n",
    "#         # Original: how much of reference is covered by graph\n",
    "#         orig_size_pct, orig_len_pct, orig_edges, orig_len = calculate_overlap_percentages(reference, graph)\n",
    "#         # Reverse: how much of graph is covered by reference\n",
    "#         rev_size_pct, rev_len_pct, rev_edges, rev_len = calculate_overlap_percentages(graph, reference)\n",
    "#         return (orig_size_pct, orig_len_pct, orig_edges, orig_len,\n",
    "#                 rev_size_pct, rev_len_pct, rev_edges, rev_len)\n",
    "    \n",
    "#     metrics_list1 = [calculate_both_ways(g, reference_graph) for g in graph_list1]\n",
    "#     metrics_list2 = [calculate_both_ways(g, reference_graph) for g in graph_list2]\n",
    "    \n",
    "#     return metrics_list1, metrics_list2\n",
    "\n",
    "# def plot_comparison(metrics_GTs, metrics_GTs_random):\n",
    "#     \"\"\"Plot comparison with separate views for both metrics\"\"\"\n",
    "#     fig, axes = plt.subplots(4, 1, figsize=(12, 16))\n",
    "    \n",
    "#     # Original percentage metrics (how much of REFERENCE is covered)\n",
    "#     axes[0].plot([m[0] for m in metrics_GTs], 'b-', label='GTs Size (Ref Covered)')\n",
    "#     axes[0].plot([m[0] for m in metrics_GTs_random], 'r--', label='GTs_random Size (Ref Covered)')\n",
    "#     axes[0].plot([m[1] for m in metrics_GTs], 'g-', label='GTs Length (Ref Covered)')\n",
    "#     axes[0].plot([m[1] for m in metrics_GTs_random], 'm--', label='GTs_random Length (Ref Covered)')\n",
    "#     axes[0].set_title('Percentage of Reference Covered')\n",
    "#     axes[0].set_ylabel('Percentage')\n",
    "#     axes[0].legend()\n",
    "#     axes[0].grid(True)\n",
    "    \n",
    "#     # Reverse percentage metrics (how much of NETWORK is covered by reference)\n",
    "#     axes[1].plot([m[4] for m in metrics_GTs], 'b-', label='GTs Size (Network Covered)')\n",
    "#     axes[1].plot([m[4] for m in metrics_GTs_random], 'r--', label='GTs_random Size (Network Covered)')\n",
    "#     axes[1].plot([m[5] for m in metrics_GTs], 'g-', label='GTs Length (Network Covered)')\n",
    "#     axes[1].plot([m[5] for m in metrics_GTs_random], 'm--', label='GTs_random Length (Network Covered)')\n",
    "#     axes[1].set_title('Percentage of Network Covered by Reference')\n",
    "#     axes[1].set_ylabel('Percentage')\n",
    "#     axes[1].legend()\n",
    "#     axes[1].grid(True)\n",
    "    \n",
    "#     # Raw edge counts\n",
    "#     axes[2].plot([m[2] for m in metrics_GTs], 'b-', label='GTs Edges (Ref Covered)')\n",
    "#     axes[2].plot([m[2] for m in metrics_GTs_random], 'r--', label='GTs_random Edges (Ref Covered)')\n",
    "#     axes[2].plot([m[6] for m in metrics_GTs], 'g-', label='GTs Edges (Network Covered)')\n",
    "#     axes[2].plot([m[6] for m in metrics_GTs_random], 'm--', label='GTs_random Edges (Network Covered)')\n",
    "#     axes[2].set_title('Raw Edge Counts')\n",
    "#     axes[2].set_ylabel('Edges')\n",
    "#     axes[2].legend()\n",
    "#     axes[2].grid(True)\n",
    "    \n",
    "#     # Raw lengths\n",
    "#     axes[3].plot([m[3] for m in metrics_GTs], 'b-', label='GTs Length (Ref Covered)')\n",
    "#     axes[3].plot([m[3] for m in metrics_GTs_random], 'r--', label='GTs_random Length (Ref Covered)')\n",
    "#     axes[3].plot([m[7] for m in metrics_GTs], 'g-', label='GTs Length (Network Covered)')\n",
    "#     axes[3].plot([m[7] for m in metrics_GTs_random], 'm--', label='GTs_random Length (Network Covered)')\n",
    "#     axes[3].set_title('Raw Length Overlap')\n",
    "#     axes[3].set_ylabel('Length')\n",
    "#     axes[3].legend()\n",
    "#     axes[3].grid(True)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# reference = G_biketrack  # Your reference infrastructure\n",
    "# metrics_GTs, metrics_GTs_random = compare_against_reference(GTs, GTs_random, reference)\n",
    "# plot_comparison(metrics_GTs, metrics_GTs_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_overlap_percentages(G_biketrack, G):\n",
    "#     # Calculate edge overlap and length overlap\n",
    "#     overlapping_edges = 0\n",
    "#     overlapping_length = 0\n",
    "#     total_edges = G_biketrack.number_of_edges()\n",
    "#     total_length = sum(data['length'] for u, v, data in G_biketrack.edges(data=True))\n",
    "    \n",
    "#     for u, v, data in G_biketrack.edges(data=True):\n",
    "#         if G.has_edge(u, v):\n",
    "#             overlapping_edges += 1\n",
    "#             overlapping_length += data['length']\n",
    "    \n",
    "#     if total_edges == 0:\n",
    "#         size_percent = 0.0\n",
    "#     else:\n",
    "#         size_percent = (overlapping_edges / total_edges) * 100\n",
    "    \n",
    "#     if total_length == 0:\n",
    "#         length_percent = 0.0\n",
    "#     else:\n",
    "#         length_percent = (overlapping_length / total_length) * 100\n",
    "    \n",
    "#     return size_percent, length_percent, overlapping_edges, overlapping_length\n",
    "\n",
    "\n",
    "# def compare_against_existing(graph_list1, graph_list2, reference_graph):\n",
    "#     \"\"\"\n",
    "#     Compare two lists of graphs against a common reference graph.\n",
    "#     Returns metrics for both lists compared to the reference.\n",
    "#     \"\"\"\n",
    "#     # Calculate metrics for both lists against the reference\n",
    "#     metrics_list1 = [calculate_overlap_percentages(g, reference_graph) for g in graph_list1]\n",
    "#     metrics_list2 = [calculate_overlap_percentages(g, reference_graph) for g in graph_list2]\n",
    "    \n",
    "#     return metrics_list1, metrics_list2\n",
    "\n",
    "# def plot_comparison(metrics_GTs, metrics_GTs_random):\n",
    "#     \"\"\"Plot comparison between GTs and GTs_random against G_biketrack\"\"\"\n",
    "#     fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 10))\n",
    "    \n",
    "#     # Percentage plot\n",
    "#     ax1.plot([m[0] for m in metrics_GTs], 'b-', label='GTs Size Overlap (%)')\n",
    "#     ax1.plot([m[0] for m in metrics_GTs_random], 'r--', label='GTs_random Size Overlap (%)')\n",
    "#     ax1.plot([m[1] for m in metrics_GTs], 'g-', label='GTs Length Overlap (%)')\n",
    "#     ax1.plot([m[1] for m in metrics_GTs_random], 'm--', label='GTs_random Length Overlap (%)')\n",
    "#     ax1.set_title('Percentage Overlap with Existing Cycle Infrastructure (Including LTNs)')\n",
    "#     ax1.set_ylabel('Percentage')\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(True)\n",
    "    \n",
    "#     # Edge count plot\n",
    "#     ax2.plot([m[2] for m in metrics_GTs], 'b-', label='GTs Overlapping Edges')\n",
    "#     ax2.plot([m[2] for m in metrics_GTs_random], 'r--', label='GTs_random Overlapping Edges')\n",
    "#     ax2.set_title('Edge Overlap Comparison')\n",
    "#     ax2.set_ylabel('Edge Count')\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(True)\n",
    "    \n",
    "#     # Length plot\n",
    "#     ax3.plot([m[3] for m in metrics_GTs], 'g-', label='GTs Overlapping Length')\n",
    "#     ax3.plot([m[3] for m in metrics_GTs_random], 'm--', label='GTs_random Overlapping Length')\n",
    "#     ax3.set_title('Length Overlap Comparison')\n",
    "#     ax3.set_ylabel('Length Units')\n",
    "#     ax3.legend()\n",
    "#     ax3.grid(True)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# metrics_GTs, metrics_GTs_random = compare_against_existing(GTs, GTs_random, G_biketrack)\n",
    "# plot_comparison(metrics_GTs, metrics_GTs_random)\n",
    "\n",
    "\n",
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {}\n",
    "\n",
    "if rerun or 'overlap_size_percent_GTs' not in analysis_results:\n",
    "    def calculate_overlap_percentages(G_ref, G):\n",
    "        overlapping_edges = 0\n",
    "        overlapping_length = 0\n",
    "        total_edges = G_ref.number_of_edges()\n",
    "        total_length = sum(data.get('length', 0) for _, _, data in G_ref.edges(data=True))\n",
    "\n",
    "        for u, v, data in G_ref.edges(data=True):\n",
    "            if G.has_edge(u, v):\n",
    "                overlapping_edges += 1\n",
    "                overlapping_length += data.get('length', 0)\n",
    "\n",
    "        size_percent = (overlapping_edges / total_edges * 100) if total_edges else 0\n",
    "        length_percent = (overlapping_length / total_length * 100) if total_length else 0\n",
    "\n",
    "        return size_percent, length_percent, overlapping_edges, overlapping_length\n",
    "\n",
    "    def get_metrics(graph_list, ref_graph):\n",
    "        return [calculate_overlap_percentages(ref_graph, g) for g in graph_list]\n",
    "\n",
    "    metrics_betweenness = get_metrics(GTs, G_biketrack)\n",
    "    metrics_random = get_metrics(GTs_random, G_biketrack)\n",
    "    metrics_demand = get_metrics(GTs_demand, G_biketrack)\n",
    "    metrics_betweenness_ltn_priority = get_metrics(GTs_betweenness_ltn_priority, G_biketrack)\n",
    "    metrics_demand_ltn_priority = get_metrics(GTs_demand_ltn_priority, G_biketrack)\n",
    "\n",
    "    overlap_metrics = {\n",
    "        # Betweenness\n",
    "        'overlap_size_percent_GTs': [m[0] for m in metrics_betweenness],\n",
    "        'overlap_length_percent_GTs': [m[1] for m in metrics_betweenness],\n",
    "        'overlap_edges_GTs': [m[2] for m in metrics_betweenness],\n",
    "        'overlap_length_GTs': [m[3] for m in metrics_betweenness],\n",
    "\n",
    "        # Random\n",
    "        'overlap_size_percent_random': [m[0] for m in metrics_random],\n",
    "        'overlap_length_percent_random': [m[1] for m in metrics_random],\n",
    "        'overlap_edges_random': [m[2] for m in metrics_random],\n",
    "        'overlap_length_random': [m[3] for m in metrics_random],\n",
    "\n",
    "        # Demand\n",
    "        'overlap_size_percent_demand': [m[0] for m in metrics_demand],\n",
    "        'overlap_length_percent_demand': [m[1] for m in metrics_demand],\n",
    "        'overlap_edges_demand': [m[2] for m in metrics_demand],\n",
    "        'overlap_length_demand': [m[3] for m in metrics_demand],\n",
    "\n",
    "        # Demand LTN Priority\n",
    "        'overlap_size_percent_demand_ltn_priority': [m[0] for m in metrics_demand_ltn_priority],\n",
    "        'overlap_length_percent_demand_ltn_priority': [m[1] for m in metrics_demand_ltn_priority],\n",
    "        'overlap_edges_demand_ltn_priority': [m[2] for m in metrics_demand_ltn_priority],\n",
    "        'overlap_length_demand_ltn_priority': [m[3] for m in metrics_demand_ltn_priority],\n",
    "\n",
    "        # Betweenness LTN Priority\n",
    "        'overlap_size_percent_betweenness_ltn_priority': [m[0] for m in metrics_betweenness_ltn_priority],\n",
    "        'overlap_length_percent_betweenness_ltn_priority': [m[1] for m in metrics_betweenness_ltn_priority],\n",
    "        'overlap_edges_betweenness_ltn_priority': [m[2] for m in metrics_betweenness_ltn_priority],\n",
    "        'overlap_length_betweenness_ltn_priority': [m[3] for m in metrics_betweenness_ltn_priority]\n",
    "    }\n",
    "\n",
    "    analysis_results.update(overlap_metrics)\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()}).to_csv(analysis_res_csv, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison(metric_key_prefix, ylabel, title, filename):\n",
    "    \"\"\"\n",
    "    Plots a single metric across all strategies compared to existing infrastructure.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(analysis_results[f'{metric_key_prefix}_GTs'], color='orange', linestyle='-', label='Betweenness')\n",
    "    plt.plot(analysis_results[f'{metric_key_prefix}_random'], color='blue', linestyle='--', label='Random')\n",
    "    plt.plot(analysis_results[f'{metric_key_prefix}_demand'], color='red', linestyle='-.', label='Demand')\n",
    "    plt.plot(analysis_results[f'{metric_key_prefix}_demand_ltn_priority'], color='green', linestyle=':', label='Demand LTN')\n",
    "    plt.plot(analysis_results[f'{metric_key_prefix}_betweenness_ltn_priority'], color='purple', linestyle='-', label='Betweenness LTN')\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = PATH[\"plots\"] + f\"/{placeid}/{filename}\"\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Call for each metric\n",
    "plot_metric_comparison(\n",
    "    metric_key_prefix='overlap_size_percent',\n",
    "    ylabel='Overlap (%)',\n",
    "    title='Edge Overlap % with Existing Cycle Network',\n",
    "    filename='percentage_overlap_edges.png'\n",
    ")\n",
    "\n",
    "plot_metric_comparison(\n",
    "    metric_key_prefix='overlap_length_percent',\n",
    "    ylabel='Overlap (%)',\n",
    "    title='Length Overlap % with Existing Cycle Network',\n",
    "    filename='percentage_overlap_length.png'\n",
    ")\n",
    "\n",
    "# plot_metric_comparison(\n",
    "#     metric_key_prefix='overlap_edges',\n",
    "#     ylabel='Edge Count',\n",
    "#     title='Overlapping Edge Count with Existing Cycle Network',\n",
    "#     filename='overlapping_edges_count.png'\n",
    "# )\n",
    "\n",
    "# plot_metric_comparison(\n",
    "#     metric_key_prefix='overlap_length',\n",
    "#     ylabel='Length (m)',\n",
    "#     title='Overlapping Length with Existing Cycle Network',\n",
    "#     filename='overlapping_length_total.png'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## against a random baseline\n",
    "def plot_deviation_from_random(metric_key_prefix, ylabel, title, filename):\n",
    "    \"\"\"\n",
    "    Plots deviation from random growth for a given overlap metric.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    baseline = np.array(analysis_results[f'{metric_key_prefix}_random'])\n",
    "\n",
    "    def plot_diff(strategy_key, label, color, linestyle):\n",
    "        values = np.array(analysis_results[f'{metric_key_prefix}_{strategy_key}'])\n",
    "        diff = values - baseline\n",
    "        plt.plot(diff, label=label, color=color, linestyle=linestyle)\n",
    "\n",
    "    plot_diff('GTs', 'Betweenness – Random', 'orange', '-')\n",
    "    plot_diff('demand', 'Demand – Random', 'red', '-.')\n",
    "    plot_diff('demand_ltn_priority', 'Demand LTN – Random', 'green', ':')\n",
    "    plot_diff('betweenness_ltn_priority', 'Betweenness LTN – Random', 'purple', '-')\n",
    "\n",
    "    plt.axhline(0, color='grey', linestyle='--', linewidth=1, alpha=0.6)\n",
    "\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    output_path = PATH[\"plots\"] + f\"/{placeid}/{filename}\"\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "plot_deviation_from_random(\n",
    "    metric_key_prefix='overlap_size_percent',\n",
    "    ylabel='Difference in Overlap (%)',\n",
    "    title='Edge Overlap: Improvement over Random (Baseline)',\n",
    "    filename='deviation_from_random_overlap_edges.png'\n",
    ")\n",
    "\n",
    "plot_deviation_from_random(\n",
    "    metric_key_prefix='overlap_length_percent',\n",
    "    ylabel='Difference in Overlap (%)',\n",
    "    title='Length Overlap: Improvement over Random (Baseline)',\n",
    "    filename='deviation_from_random_overlap_length.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_GTs, metrics_GTs_random = compare_against_existing(GTs, GTs_random, G_biketrack_no_ltn) # no differance?\n",
    "# plot_comparison(metrics_GTs, metrics_GTs_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### to explore it\n",
    "\n",
    "# # work in meters\n",
    "# G_biketrack_edges = G_biketrack_edges.to_crs(epsg=3857)\n",
    "# G_edges = G_edges.to_crs(epsg=3857)\n",
    "# G_biketrack_edges['geometry'] = G_biketrack_edges.geometry.buffer(1)\n",
    "# G_edges['geometry'] = G_edges.geometry.buffer(1)\n",
    "# joined = gpd.sjoin(G_biketrack_edges, G_edges, how=\"inner\", predicate=\"intersects\", lsuffix=\"_biketrack\", rsuffix=\"_edge\")\n",
    "\n",
    "# joined.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directness (Directness=Total Sum of Network Distances/Total Sum of Euclidean Distances​)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_dist = []\n",
    "# eucl_dist = []\n",
    "# directness = []\n",
    "\n",
    "# for G in GT_abstracts:\n",
    "#     total_net_dist = sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True))\n",
    "#     total_eucl_dist = sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True))\n",
    "#     net_dist.append(total_net_dist)\n",
    "#     eucl_dist.append(total_eucl_dist)\n",
    "#     if total_net_dist != 0:\n",
    "#         ratio = total_eucl_dist / total_net_dist\n",
    "#     else:\n",
    "#         ratio = None\n",
    "#     directness.append(ratio)\n",
    "\n",
    "\n",
    "# net_dist_random = []\n",
    "# eucl_dist_random = []\n",
    "# directness_random = []\n",
    "\n",
    "# for G in GT_abstracts_random:\n",
    "#     total_net_dist = sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True))\n",
    "#     total_eucl_dist = sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True))\n",
    "#     net_dist_random.append(total_net_dist)\n",
    "#     eucl_dist_random.append(total_eucl_dist)\n",
    "#     if total_net_dist != 0:\n",
    "#         ratio = total_eucl_dist / total_net_dist\n",
    "#     else:\n",
    "#         ratio = None\n",
    "#     directness_random.append(ratio)\n",
    "\n",
    "\n",
    "\n",
    "# # Plotting \n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(directness, linestyle='-', color='blue', label='Betweeness')\n",
    "# plt.plot(directness_random, linestyle='--', color='orange', label='Random')\n",
    "# plt.xlabel('Graph Index')\n",
    "# plt.ylabel('Directness (Euclidean / Network Distance)')\n",
    "# plt.title('Total Network Directness')\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Directness analysis \n",
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {}\n",
    "\n",
    "if rerun or 'directness_demand' not in analysis_results:\n",
    "    directness_metrics = {\n",
    "        # Betweenness\n",
    "        'directness_net': [\n",
    "            sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts\n",
    "        ],\n",
    "        'directness_eucl': [\n",
    "            sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts\n",
    "        ],\n",
    "        'directness': [\n",
    "            (sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True)) / \n",
    "            sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)))\n",
    "            if sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)) != 0 else None\n",
    "            for G in GT_abstracts\n",
    "        ],\n",
    "\n",
    "        # Random\n",
    "        'directness_net_random': [\n",
    "            sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts_random\n",
    "        ],\n",
    "        'directness_eucl_random': [\n",
    "            sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts_random\n",
    "        ],\n",
    "        'directness_random': [\n",
    "            (sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True)) / \n",
    "             sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)))\n",
    "            if sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)) != 0 else None\n",
    "            for G in GT_abstracts_random\n",
    "        ],\n",
    "\n",
    "        # Demand\n",
    "        'directness_net_demand': [\n",
    "            sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts_demand\n",
    "        ],\n",
    "        'directness_eucl_demand': [\n",
    "            sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts_demand\n",
    "        ],\n",
    "        'directness_demand': [\n",
    "            (sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True)) / \n",
    "             sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)))\n",
    "            if sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)) != 0 else None\n",
    "            for G in GT_abstracts_demand\n",
    "        ],\n",
    "\n",
    "        # Demand LTN Priority\n",
    "        'directness_net_demand_ltn_priority': [\n",
    "            sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts_demand_ltn_priority\n",
    "        ],\n",
    "        'directness_eucl_demand_ltn_priority': [\n",
    "            sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts_demand_ltn_priority\n",
    "        ],\n",
    "        'directness_demand_ltn_priority': [\n",
    "            (sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True)) / \n",
    "             sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)))\n",
    "            if sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)) != 0 else None\n",
    "            for G in GT_abstracts_demand_ltn_priority\n",
    "        ],\n",
    "\n",
    "        # Betweenness LTN Priority\n",
    "        'directness_net_betweenness_ltn_priority': [\n",
    "            sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts_betweenness_ltn_priority\n",
    "        ],\n",
    "        'directness_eucl_betweenness_ltn_priority': [\n",
    "            sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True))\n",
    "            for G in GT_abstracts_betweenness_ltn_priority\n",
    "        ],\n",
    "        'directness_betweenness_ltn_priority': [\n",
    "            (sum(data.get('eucl_dist', 0) for _, _, data in G.edges(data=True)) / \n",
    "             sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)))\n",
    "            if sum(data.get('sp_true_distance', 0) for _, _, data in G.edges(data=True)) != 0 else None\n",
    "            for G in GT_abstracts_betweenness_ltn_priority\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    analysis_results.update(directness_metrics)\n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()}).to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    analysis_results['directness'],\n",
    "    linestyle='--', \n",
    "    color='orange', \n",
    "    label='Betweenness'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['directness_random'],\n",
    "    linestyle='-', \n",
    "    color='blue', \n",
    "    label='Random'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['directness_demand'],\n",
    "    linestyle='-.', \n",
    "    color='red', \n",
    "    label='Demand'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['directness_demand_ltn_priority'],\n",
    "    linestyle=':', \n",
    "    color='green', \n",
    "    label='Demand LTN'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['directness_betweenness_ltn_priority'],\n",
    "    linestyle='-', \n",
    "    color='purple', \n",
    "    label='Betweenness LTN'\n",
    ")\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Directness (Euclidean / Network Distance)')\n",
    "plt.title('Network Directness Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/directness.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcaulate directness of existing network to compare against..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## edit plotting\n",
    "# def calculate_efficiency(G):\n",
    "#     \"\"\"Calculate global network efficiency using formula E = 1/(N(N-1)) * Σ 1/d_ij\"\"\"\n",
    "#     # Convert to undirected graph\n",
    "#     undirected_G = nx.Graph(G)\n",
    "#     try:\n",
    "#         return nx.global_efficiency(undirected_G)\n",
    "#     except nx.NetworkXError:\n",
    "#         return 0  # Handle disconnected graphs\n",
    "\n",
    "# def plot_efficiency_comparison(GTs, GTs_random):\n",
    "#     \"\"\"Calculate and plot global efficiency for both graph lists\"\"\"\n",
    "#     # Calculate efficiencies\n",
    "#     eff_GTs = [calculate_efficiency(G) for G in GTs]\n",
    "#     eff_random = [calculate_efficiency(G) for G in GTs_random]\n",
    "    \n",
    "#     # Create plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(eff_GTs, 'b-', linewidth=2, label='GTs Efficiency')\n",
    "#     plt.plot(eff_random, 'r--', linewidth=2, label='GTs_random Efficiency')\n",
    "    \n",
    "#     plt.title('Global Network Efficiency Comparison\\n$E = \\\\frac{1}{N(N-1)}\\\\sum_{i\\\\neq j} \\\\frac{1}{d_{ij}}$')\n",
    "#     plt.ylabel('Global Efficiency')\n",
    "#     plt.xlabel('Graph Instance Index')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.ylim(0, 1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage example:\n",
    "# plot_efficiency_comparison(GTs, GTs_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_global_efficiency(G, numnodepairs=500, normalized=True, weight='weight', debug=False):\n",
    "    \"\"\"Calculates global network efficiency for a graph G.\"\"\"\n",
    "    if G is None or len(G) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    nodes = list(G.nodes)\n",
    "    N = len(nodes)\n",
    "    \n",
    "    if N > numnodepairs:\n",
    "        sampled_nodes = random.sample(nodes, numnodepairs)\n",
    "    else:\n",
    "        sampled_nodes = nodes\n",
    "    S = len(sampled_nodes)\n",
    "    if S < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    total_efficiency = 0.0\n",
    "    considered_pairs = S * (S - 1)  \n",
    "    \n",
    "    for u in sampled_nodes:\n",
    "        try:\n",
    "            lengths = nx.single_source_dijkstra_path_length(G, u, weight=weight)\n",
    "            for v in sampled_nodes:\n",
    "                if u == v: continue\n",
    "                d = lengths.get(v, float('inf'))\n",
    "                if 0 < d < float('inf'):\n",
    "                    total_efficiency += 1 / d\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "    \n",
    "    if considered_pairs == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Always use considered_pairs for unnormalized\n",
    "    EG = total_efficiency / considered_pairs  # average efficiency\n",
    "    \n",
    "    if not normalized:\n",
    "        return EG  # Directly return average efficiency of sampled pairs\n",
    "    \n",
    "    # Normalisation logic \n",
    "    for node in sampled_nodes:\n",
    "        if 'x' not in G.nodes[node] or 'y' not in G.nodes[node]:\n",
    "            raise KeyError(\"Nodes need 'x' and 'y' for normalization.\")\n",
    "    \n",
    "    ideal_total = 0.0\n",
    "    for u, v in itertools.permutations(sampled_nodes, 2):\n",
    "        x1, y1 = G.nodes[u]['x'], G.nodes[u]['y']\n",
    "        x2, y2 = G.nodes[v]['x'], G.nodes[v]['y']\n",
    "        distance = ((x1-x2)**2 + (y1-y2)**2)**0.5\n",
    "        if distance > 0:\n",
    "            ideal_total += 1 / distance\n",
    "    \n",
    "    if ideal_total == 0:\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "    ideal_avg = ideal_total / considered_pairs\n",
    "    normalized_efficiency = EG / ideal_avg\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Actual Avg: {EG}, Ideal Avg: {ideal_avg}, Normalized: {normalized_efficiency}\")\n",
    "    \n",
    "    return normalized_efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_global_efficiency(G, numnodepairs=500, normalized=True, weight='length', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(analysis_res_pickle):\n",
    "    with open(analysis_res_pickle, 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "else:\n",
    "    analysis_results = {}\n",
    "\n",
    "if rerun or 'efficiency_demand' not in analysis_results:\n",
    "    efficiency_metrics = {\n",
    "        'efficiency': [\n",
    "            calculate_global_efficiency(G, numnodepairs=1000, normalized=True, weight='length')\n",
    "            for G in GT_abstracts\n",
    "        ],\n",
    "        'efficiency_random': [\n",
    "            calculate_global_efficiency(G, numnodepairs=1000, normalized=True, weight='length')\n",
    "            for G in GT_abstracts_random\n",
    "        ],\n",
    "        'efficiency_demand': [\n",
    "            calculate_global_efficiency(G, numnodepairs=1000, normalized=True, weight='length')\n",
    "            for G in GT_abstracts_demand\n",
    "        ],\n",
    "        'efficiency_demand_ltn_priority': [\n",
    "            calculate_global_efficiency(G, numnodepairs=1000, normalized=True, weight='length')\n",
    "            for G in GT_abstracts_demand_ltn_priority\n",
    "        ],\n",
    "        'efficiency_betweenness_ltn_priority': [\n",
    "            calculate_global_efficiency(G, numnodepairs=1000, normalized=True, weight='length')\n",
    "            for G in GT_abstracts_betweenness_ltn_priority\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "\n",
    "    analysis_results.update(efficiency_metrics)  \n",
    "    with open(analysis_res_pickle, 'wb') as f:\n",
    "        pickle.dump(analysis_results, f)\n",
    "    pd.DataFrame({k: pd.Series(v) for k, v in analysis_results.items()}).to_csv(analysis_res_csv, index=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot efficiency from analysis_results\n",
    "plt.plot(\n",
    "    analysis_results['efficiency'],\n",
    "    linestyle='-', \n",
    "    color='orange',\n",
    "    label='Betweenness'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['efficiency_random'],\n",
    "    linestyle='--', \n",
    "    color='blue',\n",
    "    label='Random'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['efficiency_demand'],\n",
    "    linestyle='-.', \n",
    "    color='red',\n",
    "    label='Demand'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['efficiency_demand_ltn_priority'],\n",
    "    linestyle=':', \n",
    "    color='green',\n",
    "    label='Demand LTN'\n",
    ")\n",
    "plt.plot(\n",
    "    analysis_results['efficiency_betweenness_ltn_priority'],\n",
    "    linestyle='-', \n",
    "    color='purple',\n",
    "    label='Betweenness LTN'\n",
    ")\n",
    "\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Global Efficiency', fontsize=12)\n",
    "plt.title('Global Network Efficiency Comparison', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = PATH[\"plots\"] + \"/\" + placeid + \"/global_eff.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot both lines\n",
    "plt.plot(x, eff_GTs, label='GTs',  linestyle='-', color='blue')\n",
    "plt.plot(x_random, eff_GTs_random, label='GTs Random', linestyle='--', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Graph Index', fontsize=12)\n",
    "plt.ylabel('Global Efficiency', fontsize=12)\n",
    "plt.title('Global Network Efficiency Comparison', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Customize ticks\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_efficiencies(G):\n",
    "#     \"\"\"Calculate both global and local efficiencies\"\"\"\n",
    "#     # Convert to undirected graph\n",
    "#     undirected_G = nx.Graph(G)\n",
    "    \n",
    "#     try:\n",
    "#         global_eff = nx.global_efficiency(undirected_G)\n",
    "#     except nx.NetworkXError:\n",
    "#         global_eff = 0\n",
    "        \n",
    "#     try:\n",
    "#         local_eff = nx.local_efficiency(undirected_G)\n",
    "#     except nx.NetworkXError:\n",
    "#         local_eff = 0\n",
    "        \n",
    "#     return global_eff, local_eff\n",
    "\n",
    "# def plot_efficiency_comparison(GTs, GTs_random):\n",
    "#     \"\"\"Plot comparison of both efficiency metrics\"\"\"\n",
    "#     # Calculate efficiencies\n",
    "#     global_GTs, local_GTs = zip(*[calculate_efficiencies(G) for G in GTs])\n",
    "#     global_random, local_random = zip(*[calculate_efficiencies(G) for G in GTs_random])\n",
    "    \n",
    "#     # Create plots\n",
    "#     fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "#     # Global efficiency plot\n",
    "#     ax1.plot(global_GTs, 'b-', linewidth=2, label='GTs Global Eff')\n",
    "#     ax1.plot(global_random, 'r--', linewidth=2, label='GTs_random Global Eff')\n",
    "#     ax1.set_title('Global Network Efficiency Comparison')\n",
    "#     ax1.set_ylabel('Efficiency')\n",
    "#     ax1.legend()\n",
    "#     ax1.grid(True)\n",
    "#     ax1.set_ylim(0, 1)\n",
    "    \n",
    "#     # Local efficiency plot\n",
    "#     ax2.plot(local_GTs, 'g-', linewidth=2, label='GTs Local Eff')\n",
    "#     ax2.plot(local_random, 'm--', linewidth=2, label='GTs_random Local Eff')\n",
    "#     ax2.set_title('Local Network Efficiency Comparison')\n",
    "#     ax2.set_ylabel('Efficiency')\n",
    "#     ax2.legend()\n",
    "#     ax2.grid(True)\n",
    "#     ax2.set_ylim(0, 1)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage example:\n",
    "# plot_efficiency_comparison(GTs, GTs_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Pretty plots of networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_number = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))  # Adjust the width and height as needed\n",
    "\n",
    "\n",
    "\n",
    "G_biketrackcarall_edges = ox.graph_to_gdfs(G_biketrackcarall, nodes=False)\n",
    "G_biketrackcarall_edges = G_biketrackcarall_edges.to_crs(epsg=3857)  # Ensure CRS matches\n",
    "G_biketrackcarall_edges.plot(ax=ax, color='grey', linewidth=0.6, alpha=0.5, zorder = 0)  # Light grey with thin linewidth\n",
    "\n",
    "# Add bike track edges\n",
    "#G_biketrack = {}\n",
    "#G_biketrack[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'biketrack')\n",
    "#G_biketrack[placeid].graph[\"crs\"] = 'epsg:4326'  # Needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "#G_biketrack = copy.deepcopy(G_biketrack[placeid])\n",
    "G_biketrack_edges = ox.graph_to_gdfs(G_biketrack, nodes=False)\n",
    "G_biketrack_edges = G_biketrack_edges.to_crs(epsg=3857)\n",
    "G_biketrack_edges.plot(ax=ax, color='turquoise', linewidth=1.4, alpha=0.9, zorder = 1)  # Light grey with thin linewidth\n",
    "\n",
    "\n",
    "# Plot the main graph and layers\n",
    "GT_nodes, GT_edges = ox.graph_to_gdfs(GTs[iteration_number])\n",
    "GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "GT_edges.plot(ax=ax, color='orange')\n",
    "ltn_points.to_crs(epsg=3857).plot(ax=ax, color='red', markersize=10, zorder=4)\n",
    "tess_points.to_crs(epsg=3857).plot(ax=ax, color='green', markersize=5, zorder = 3)\n",
    "\n",
    "\n",
    "ltns = ltns.to_crs(epsg=3857)  # Ensure the CRS matches\n",
    "ltns.plot(ax=ax, color='blue', alpha=0.5, label=f\"Low Traffic Neighbourhoods\", zorder=2)\n",
    "\n",
    "\n",
    "# Remove x and y axis labels and ticks\n",
    "ax.axis('off')  # This removes the entire axis, including labels and ticks\n",
    "\n",
    "ax.set_title(f\"Iteration: {iteration_number + 1}\")\n",
    "#ax.legend(loc=\"upper left\")\n",
    "\n",
    "output_path = fr\"C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\Conferances etc\\GISRUK 2025\\Plots\\{iteration_number}_network_plot.png\"\n",
    "plt.savefig(output_path, dpi=600, bbox_inches='tight'\n",
    "            #, transparent=True\n",
    "            )\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif_path = r\"C:\\Users\\b8008458\\OneDrive - Newcastle University\\2022 to 2023\\PhD\\networkGrowth\\bikenwgrowth_external\\videos\\newcastle\\investment_animation_pct.gif\"\n",
    "\n",
    "# # Set up a figure for animation\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# def update(idx):\n",
    "#     \"\"\"Update function for each frame in the animation.\"\"\"\n",
    "#     ax.clear()  # Clear previous frame\n",
    "#     G = GTs[idx]\n",
    "#     # Skip empty graphs\n",
    "#     if len(G.edges()) == 0:\n",
    "#         print(f\"Graph {idx + 1} has no edges, skipping plot.\")\n",
    "#         return\n",
    "    \n",
    "#     # Add G_weighted edges\n",
    "#     G_weighted_edges = ox.graph_to_gdfs(G_weighted, nodes=False)\n",
    "#     G_weighted_edges = G_weighted_edges.to_crs(epsg=3857)\n",
    "#     G_weighted_edges.plot(ax=ax, color='grey', linewidth=0.5, alpha=0.6, zorder=0)\n",
    "\n",
    "#     # Add bike track edges\n",
    "#     G_biketrack_nodes, G_biketrack_edges = ox.graph_to_gdfs(G_biketrack)\n",
    "#     G_biketrack_edges = G_biketrack_edges.to_crs(epsg=3857)\n",
    "#     G_biketrack_edges.plot(ax=ax, color='turquoise', linewidth=0.5, alpha=0.8, zorder=1)\n",
    "\n",
    "#     # Plot main graph\n",
    "#     GT_nodes, GT_edges = ox.graph_to_gdfs(G)\n",
    "#     GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "#     GT_edges.plot(ax=ax, color='orange')\n",
    "\n",
    "#     # Plot additional layers\n",
    "#     ltn_gdf.plot(ax=ax, color='red', markersize=10, zorder=4)\n",
    "#     tess_gdf.plot(ax=ax, color='green', markersize=5, zorder=3)\n",
    "\n",
    "#     # Plot the neighbourhood\n",
    "#     placename = \"Newcastle Upon Tyne\"\n",
    "#     if placename in neighbourhoods:\n",
    "#         neighbourhood_gdf = neighbourhoods[placename].to_crs(epsg=3857)\n",
    "#         neighbourhood_gdf.plot(ax=ax, color='blue', alpha=0.5, zorder=2)\n",
    "\n",
    "#     # Remove axis and set title\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f\"Meters of investment: {D/10}\")\n",
    "\n",
    "# # Create animation\n",
    "# ani = animation.FuncAnimation(fig, update, frames=len(GTs), repeat=False)\n",
    "\n",
    "# # Save the animation as a GIF using PillowWriter\n",
    "# ani.save(gif_path, writer=animation.PillowWriter(fps=6))\n",
    "\n",
    "# print(f\"GIF saved to: {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete once happy with cell below\n",
    "# neighbourhoods = load_neighbourhoods(os.path.join(PATH[\"data\"], placeid))\n",
    "# G_biketrackcarall_edges = (\n",
    "#     ox.graph_to_gdfs(G_biketrackcarall, nodes=False)\n",
    "#       .to_crs(epsg=3857)\n",
    "# )\n",
    "# G_biketrack_edges = (\n",
    "#     ox.graph_to_gdfs(G_biketrack, nodes=False)\n",
    "#       .to_crs(epsg=3857)\n",
    "# )\n",
    "\n",
    "# ltn_points_crs = ltn_points.to_crs(epsg=3857)\n",
    "# tess_points_crs = tess_points.to_crs(epsg=3857)\n",
    "# neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "# ltns = neighbourhoods.get(\"ltns\", None)\n",
    "# ltns_gdf = None\n",
    "# if neighbourhoods:\n",
    "#     _, ltns_gdf = next(iter(neighbourhoods.items())) # get the first geodataframe in neighbourhoods. Should fix this to a more elegant solution\n",
    "#     ltns = ltns_gdf.to_crs(epsg=3857) \n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))  \n",
    "\n",
    "# def update(idx):\n",
    "#     \"\"\"Update function called for each animation frame.\"\"\"\n",
    "#     ax.clear()  # clear the axis for the new frame\n",
    "\n",
    "#     # Plot the static background layers first.\n",
    "#     G_biketrackcarall_edges.plot(ax=ax, color='grey', linewidth=0.6, alpha=0.5, zorder=0)\n",
    "#     G_biketrack_edges.plot(ax=ax, color='turquoise', linewidth=1.4, alpha=0.9, zorder=1)\n",
    "\n",
    "#     # Get the current main graph from your list of graphs GTs.\n",
    "#     current_graph = GTs[idx]\n",
    "#     if len(current_graph.edges()) == 0:\n",
    "#         print(f\"Graph {idx + 1} has no edges, skipping plot.\")\n",
    "#         return\n",
    "\n",
    "#     # Convert the main graph to GeoDataFrames and reproject\n",
    "#     GT_nodes, GT_edges = ox.graph_to_gdfs(current_graph)\n",
    "#     GT_edges.to_crs(epsg=3857).plot(ax=ax, color='orange')\n",
    "    \n",
    "#     # Plot additional layers.\n",
    "#     ltn_points_crs.plot(ax=ax, color='red', markersize=10, zorder=4)\n",
    "#     tess_points_crs.plot(ax=ax, color='green', markersize=5, zorder=3)\n",
    "#     ltns.plot(ax=ax, color='blue', alpha=0.5, zorder=2)\n",
    "\n",
    "#     ax.axis('off')\n",
    "#     ax.set_title(f\"Iterations completed: {idx + 1}%\", fontsize=14)\n",
    "\n",
    "\n",
    "# ani = animation.FuncAnimation(fig, update, frames=len(GT_abstracts), repeat=False)\n",
    "\n",
    "# # Construct the output file path flexibly\n",
    "# output_gif = os.path.join(PATH[\"videos\"], placeid + \"/\" f\"investment_animation{prune_measure}.gif\")\n",
    "# output_gif = os.path.join(PATH[\"videos\"], placeid + \"/\" f\"betweenness_greedyTri.gif\")\n",
    "# # Create the directory if it doesn't exist.\n",
    "# os.makedirs(os.path.dirname(output_gif), exist_ok=True)\n",
    "\n",
    "# # Save the animation as a GIF with PillowWriter.\n",
    "# ani.save(output_gif, writer=animation.PillowWriter(fps=6))\n",
    "\n",
    "# print(f\"GIF saved to: {output_gif}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new plotting function\n",
    "def plot_investment_animation(\n",
    "    graph_list,\n",
    "    output_path,\n",
    "    G_biketrackcarall,\n",
    "    G_biketrack,\n",
    "    ltn_points,\n",
    "    tess_points,\n",
    "    neighbourhoods,\n",
    "    fps=4,\n",
    "    title_prefix=\"Iteration number: \",\n",
    "    crs_epsg=3857,\n",
    "    figsize=(12, 8)\n",
    "):\n",
    "    \"\"\"Generate and save an animated GIF showing network growth over time.\"\"\"\n",
    "\n",
    "    G_biketrackcarall_edges = (\n",
    "        ox.graph_to_gdfs(G_biketrackcarall, nodes=False).to_crs(epsg=crs_epsg)\n",
    "    )\n",
    "    G_biketrack_edges = (\n",
    "        ox.graph_to_gdfs(G_biketrack, nodes=False).to_crs(epsg=crs_epsg)\n",
    "    )\n",
    "    ltn_points_crs = ltn_points.to_crs(epsg=crs_epsg)\n",
    "    tess_points_crs = tess_points.to_crs(epsg=crs_epsg)\n",
    "\n",
    "    # Get a neighbourhood GeoDataFrame from the dictionary\n",
    "    ltns = None\n",
    "    if neighbourhoods:\n",
    "        _, ltns_gdf = next(iter(neighbourhoods.items()))\n",
    "        ltns = ltns_gdf.to_crs(epsg=crs_epsg)\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    def update(idx):\n",
    "        ax.clear()\n",
    "        G = graph_list[idx]\n",
    "        if 'crs' not in G.graph:\n",
    "            G.graph['crs'] = f\"epsg:{crs_epsg}\"\n",
    "\n",
    "        # Static background layers\n",
    "        G_biketrackcarall_edges.plot(ax=ax, color='grey', linewidth=0.6, alpha=0.5, zorder=0)\n",
    "        G_biketrack_edges.plot(ax=ax, color='turquoise', linewidth=1.4, alpha=0.9, zorder=1)\n",
    "\n",
    "        # Skip empty graphs\n",
    "        if len(G.edges()) == 0:\n",
    "            print(f\"Graph {idx + 1} has no edges, skipping.\")\n",
    "            return\n",
    "\n",
    "        # Main graph\n",
    "        _, edges = ox.graph_to_gdfs(G)\n",
    "        edges.to_crs(epsg=crs_epsg).plot(ax=ax, color='orange')\n",
    "\n",
    "        # Point layers\n",
    "        ltn_points_crs.plot(ax=ax, color='red', markersize=10, zorder=4)\n",
    "        tess_points_crs.plot(ax=ax, color='green', markersize=5, zorder=3)\n",
    "\n",
    "        # LTN areas\n",
    "        if ltns is not None:\n",
    "            ltns.plot(ax=ax, color='blue', alpha=0.5, zorder=2)\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{title_prefix} - iterations completed: {idx + 1}%\", fontsize=14)\n",
    "\n",
    "    # Create animation\n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(graph_list), repeat=False)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "\n",
    "    ani.save(output_path, writer=animation.PillowWriter(fps=fps), dpi=400)\n",
    "    print(f\"GIF saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run plotting function\n",
    "plot_investment_animation(\n",
    "    graph_list=GT_abstracts_demand,\n",
    "    output_path=os.path.join(PATH[\"videos\"], placeid, f\"demand_abstract_animation.gif\"),\n",
    "    G_biketrackcarall=G_biketrackcarall,\n",
    "    G_biketrack=G_biketrack,\n",
    "    ltn_points=ltn_points,\n",
    "    tess_points=tess_points,\n",
    "    neighbourhoods=load_neighbourhoods(os.path.join(PATH[\"data\"], placeid)),\n",
    "    title_prefix=\"Demand growth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfinshed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_GTs, metrics_GTs_random = compare_against_existing(GTs, GTs_random, G_biketrack_no_ltn) # no differance?\n",
    "plot_comparison(metrics_GTs, metrics_GTs_random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
