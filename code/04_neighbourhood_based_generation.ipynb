{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Points of interest based bicycle network generation\n",
    "## Project: Growing Urban Bicycle Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the transit-oriented development approach of palominos2020ica or a grid approach and applies cardillo2006spp: Take the greedy triangulation between railway/underground stations (or other points of interest created in 02_prepare_pois). This is the cold start bicycle network generation process which creates bicycle networks from scratch.\n",
    "\n",
    "Contact: Chris Larkin (c.larkin@ncl.ac.uk)\n",
    "Created: 2023-10-4  \n",
    "Last modified: 2025-05-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- ltns un-prioritised growth to compare against ltn prioritsed  X DONE\n",
    "- clean triangulation for concave shapes X DONE\n",
    "- methods plotting for removing slivers etc\n",
    "- cycle network investment within LTNs, converting pedestrain streets so that you can cycle through. Could have a big impact on the lcc size\n",
    "- can't take multiple places as an input (my bad coding skills...), need to be able to take several places at a time\n",
    "- save outputs as geopackages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # If True, will produce plots and/or verbose output to double-check\n",
    "%run -i \"../parameters/parameters.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i path.py\n",
    "%run -i setup.py\n",
    "\n",
    "%load_ext watermark\n",
    "#%watermark -n -v -m -g -iv\n",
    "PATH['exports_gpkg'] = '../../bikenwgrowth_external/exports_gpkg/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network weighting tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i tag_lts.py\n",
    "%run -i distance_cost.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function and code below currently routes between the edges of neighbourhoods, rather than from a single point to a single point. We then join the neighbourhoods up first, before considering the wider area. This wider area is derived from hexagonal tesslleations within the city boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "def csv_to_ox(p, placeid, parameterid):\n",
    "    '''\n",
    "    Load graph from csv files (nodes and edge)\n",
    "    Include OSMID, length, highway, x, y attributes\n",
    "    '''\n",
    "\n",
    "    prefix = placeid + '_' + parameterid\n",
    "    compress = check_extract_zip(p, prefix)\n",
    "    \n",
    "    with open(p + prefix + '_edges.csv', 'r') as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "        lines = []\n",
    "        for line in csv.reader(f, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):\n",
    "            line_list = [c for c in line]\n",
    "            osmid = str(eval(line_list[header.index(\"osmid\")])[0]) if isinstance(eval(line_list[header.index(\"osmid\")]), list) else line_list[header.index(\"osmid\")]\n",
    "            length = str(eval(line_list[header.index(\"length\")])[0]) if isinstance(eval(line_list[header.index(\"length\")]), list) else line_list[header.index(\"length\")]\n",
    "            highway = line_list[header.index(\"highway\")]\n",
    "            if highway.startswith(\"[\") and highway.endswith(\"]\"):\n",
    "                highway = highway.strip(\"[]\").split(\",\")[0].strip(\" '\")\n",
    "            line_string = f\"{line_list[header.index('u')]} {line_list[header.index('v')]} {osmid} {length} {highway}\"\n",
    "            lines.append(line_string)\n",
    "        G = nx.parse_edgelist(lines, nodetype=int, data=((\"osmid\", int), (\"length\", float), (\"highway\", str)), create_using=nx.MultiDiGraph)\n",
    "    \n",
    "    with open(p + prefix + '_nodes.csv', 'r') as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "        values_x = {}\n",
    "        values_y = {}\n",
    "        for line in csv.reader(f, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):\n",
    "            line_list = [c for c in line]\n",
    "            osmid = int(line_list[header.index(\"osmid\")])\n",
    "            values_x[osmid] = float(line_list[header.index(\"x\")])\n",
    "            values_y[osmid] = float(line_list[header.index(\"y\")])\n",
    "        nx.set_node_attributes(G, values_x, \"x\")\n",
    "        nx.set_node_attributes(G, values_y, \"y\")\n",
    "    \n",
    "    if compress:\n",
    "        os.remove(p + prefix + '_nodes.csv')\n",
    "        os.remove(p + prefix + '_edges.csv')\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G_caralls - no addtional weighting\n",
    "\n",
    "G_weighted - for routing. lts applied + zero bike infrastucutre cost\n",
    "\n",
    "G_investment distance - for finding distances between points in abstract (greedy_gdf) graph. cycle infrastucre has no cost, everything else has default distance\n",
    "\n",
    "greedy_gdf[distance] - the investment length (routed length - any infrastucture)\n",
    "\n",
    "GT \"length\" - actual distance between points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get graphs\n",
    "\n",
    "locations = {}\n",
    "parameterinfo = osmnxparameters['carall']\n",
    "G_caralls = {}\n",
    "G_caralls_simplified = {}\n",
    "\n",
    "\n",
    "# reload graph to ensure we have highway type information\n",
    "G_caralls[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'biketrackcarall')\n",
    "G_caralls[placeid].graph[\"crs\"] = 'epsg:4326'  # Needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "\n",
    "# set up a graph for weighting by LTS, one without any weighting and one for finding the \"true\" distance of investment per path\n",
    "G_weighted = copy.deepcopy(G_caralls[placeid])\n",
    "G_investment_distance = copy.deepcopy(G_caralls[placeid])\n",
    "G_default = copy.deepcopy(G_caralls[placeid])\n",
    "\n",
    "\n",
    "# weight graphs\n",
    "for u, v, key, data in G_weighted.edges(keys=True, data=True):\n",
    "    highway = data.get('highway') \n",
    "    length = data.get('length')  \n",
    "    lts = tag_lts.get(highway, 1)  # Get the LTS value, default to 1 if highway type not in tag_lts\n",
    "    G_weighted[u][v][key]['length'] *= lts  # Multiply length by LTS value\n",
    "\n",
    "for u, v, key, data in G_investment_distance.edges(keys=True, data=True):\n",
    "    highway = data.get('highway') \n",
    "    length = data.get('length')  \n",
    "    distance = distance_cost.get(highway, 1)  \n",
    "    G_investment_distance[u][v][key]['length'] *= distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set the graph to route on to undirected \n",
    "G_weighted = G_weighted.to_undirected()\n",
    "G_investment_distance = G_investment_distance.to_undirected()\n",
    "G_default = G_default.to_undirected()\n",
    "\n",
    "# get biketrack graph\n",
    "gpkg_path = PATH[\"data\"] + placeid + \"/\" + placeid + '_biketrack.gpkg'\n",
    "G_biketrack = ox_gpkg_to_graph(gpkg_path)\n",
    "G_biketrack.remove_nodes_from(list(nx.isolates(G_biketrack)))\n",
    "\n",
    "\n",
    "# get biketrack graph without LTNs\n",
    "gpkg_path = PATH[\"data\"] + placeid + \"/\" + placeid + '_biketrack_no_ltn.gpkg'\n",
    "G_biketrack_no_ltn = ox_gpkg_to_graph(gpkg_path)\n",
    "G_biketrack_no_ltn.remove_nodes_from(list(nx.isolates(G_biketrack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nodes \n",
    "\n",
    "\n",
    "for placeid, placeinfo in tqdm(cities.items(), desc=\"Cities\"):\n",
    "    print(f\"Processing {placeid}\")\n",
    "    \n",
    "    # Load Tesselation POIs (hard coded for now)\n",
    "    with open(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_' + 'tessellation' + '_nnidsbikeall.csv') as f:\n",
    "        tessellation_nnids = [int(line.rstrip()) for line in f]\n",
    "\n",
    "\n",
    "    # Load LTN POIs\n",
    "    if placeinfo[\"nominatimstring\"] != '':\n",
    "        location = ox.geocoder.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "        if location.geometry[0].geom_type == 'MultiPolygon':\n",
    "            location = location.explode(index_parts=False).reset_index(drop=True)\n",
    "        location = fill_holes(extract_relevant_polygon(placeid, shapely.geometry.shape(location['geometry'][0])))\n",
    "    else:\n",
    "        # https://gis.stackexchange.com/questions/113799/how-to-read-a-shapefile-in-python\n",
    "        shp = fiona.open(PATH[\"data\"] + placeid + \"/\" + placeid + \".shp\")\n",
    "        first = next(iter(shp))\n",
    "        try:\n",
    "            location = Polygon(shapely.geometry.shape(first['geometry']))  # If shape file is given as linestring\n",
    "        except:\n",
    "            location = shapely.geometry.shape(first['geometry'])\n",
    "    locations[placeid] = location\n",
    "    \n",
    "    G_caralls[placeid] = csv_to_ox_highway(PATH[\"data\"] + placeid + \"/\", placeid, 'biketrackcarall')\n",
    "    G_caralls[placeid].graph[\"crs\"] = 'epsg:4326'  # Needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "    G_caralls_simplified[placeid] = csv_to_ox(PATH[\"data\"] + placeid + \"/\", placeid, 'biketrackcarall_simplified')\n",
    "    G_caralls_simplified[placeid].graph[\"crs\"] = 'epsg:4326'  # Needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "\n",
    "    print(f\"{placeid}: Loading and moving POIs\")\n",
    "    # Get the carall graph and location geometry\n",
    "    location = locations[placeid]\n",
    "    G_carall = G_caralls_simplified[placeid]\n",
    "\n",
    "    # Load neighbourhoods and create GeoDataFrame for centroids\n",
    "    neighbourhoods = load_neighbourhoods(PATH[\"data\"] + placeid + \"/\")\n",
    "    all_centroids = gpd.GeoDataFrame(columns=['neighbourhood_id', 'geometry'], crs='EPSG:4326')  \n",
    "    \n",
    "\n",
    "    # load tesselation points \n",
    "    tess_nodes = gpd.read_file(PATH[\"data\"] + placeid + \"/\" + placeid + '_poi_tessellation.gpkg')\n",
    "\n",
    "    exit_points = get_exit_nodes(neighbourhoods, G_biketrack)\n",
    "    if export:\n",
    "        file_path = os.path.join(PATH[\"exports_gpkg\"] + placeid + \"/\" + f\"{placeid}_exit_points.gpkg\"); os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        exit_points.to_file(file_path, driver=\"GPKG\")\n",
    "\n",
    "    \n",
    "        \n",
    "    unique_id = 0  # Counter for unique IDs across neighbourhoods\n",
    "\n",
    "    for name, gdf in neighbourhoods.items():  # Process each neighbourhood GeoDataFrame to get centroids, exit points, and neighbourhood IDs\n",
    "        if gdf.empty:\n",
    "            print(f\"Warning: The GeoDataFrame for {name} is empty. Skipping...\")\n",
    "            continue\n",
    "        print(f\"Processing neighbourhoods in: {name}\")\n",
    "\n",
    "        # Assign a unique ID to each neighbourhood in the GeoDataFrame to reference throughout\n",
    "        gdf['neighbourhood_id'] = range(unique_id, unique_id + len(gdf))\n",
    "        if debug:\n",
    "            print(f\"Assigned neighbourhood_ids from {unique_id} to {unique_id + len(gdf) - 1} for {name}\")\n",
    "\n",
    "        # Get centroids to inherit 'neighbourhood_id'\n",
    "        centroids_gdf = get_neighbourhood_centroids(gdf)\n",
    "        all_centroids = pd.concat([all_centroids, centroids_gdf], ignore_index=True)\n",
    "        unique_id += len(gdf)  # Increment by the number of neighbourhoods processed\n",
    "\n",
    "    # Snap centroids to the closest nodes in the street network\n",
    "    neighbourhood_nnids = set()\n",
    "    for g in all_centroids['geometry']:\n",
    "        n = ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "        if n not in neighbourhood_nnids and haversine((g.y, g.x), (G_carall.nodes[n][\"y\"], G_carall.nodes[n][\"x\"]), unit=\"m\") <= snapthreshold:\n",
    "            neighbourhood_nnids.add(n)\n",
    "    # Add nearest_node column to all_centroids by finding the nearest node for each centroid geometry\n",
    "    all_centroids['nearest_node'] = all_centroids['geometry'].apply(\n",
    "        lambda g: ox.distance.nearest_nodes(G_carall, g.x, g.y))  # We now have all_centroids with 'neighbourhood_id', 'geometry', 'nearest_node' columns\n",
    "    \n",
    "    ltn_nodes = all_centroids\n",
    "\n",
    "    # add nearest node ID from G_carall \n",
    "    tess_nn = get_nearest_nodes_to_gdf(G_carall, tess_nodes)\n",
    "    ltn_nn = get_nearest_nodes_to_gdf(G_carall, ltn_nodes)\n",
    "    tess_nodes['osmid'] = tess_nn\n",
    "    ltn_nodes['osmid'] = ltn_nn\n",
    "\n",
    "    # combine nodes \n",
    "    combined_nodes = pd.concat([tess_nodes, ltn_nodes], ignore_index=True)\n",
    "\n",
    "    # save them\n",
    "    tess_nodes.to_file(PATH[\"data\"] + placeid + \"/\" + placeid + \"_tess_points.gpkg\", driver=\"GPKG\")\n",
    "    ltn_nodes.to_file(PATH[\"data\"] + placeid + \"/\" + placeid + \"_ltn_points.gpkg\", driver=\"GPKG\")\n",
    "    combined_nodes.to_file(PATH[\"data\"] + placeid + \"/\" + placeid + \"_combined_points.gpkg\", driver=\"GPKG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create triangulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LTN triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce triangulation for LTN nodes\n",
    "# only needed to set up full trianuglation, not needed later.\n",
    "# ltn nodes are not ordered (nor do they need to be at this stage)\n",
    "greedy_triangulation_ltns_gdf = greedy_triangulation_ltns(ltn_nodes)\n",
    "ltn_node_pairs = get_ltn_node_pairs(ltn_nodes, greedy_triangulation_ltns_gdf) \n",
    "if export:\n",
    "    file_path = os.path.join(PATH[\"exports_gpkg\"] + placeid + \"/\" + f\"{placeid}_greedy_triangulation_ltns.gpkg\"); os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    greedy_triangulation_ltns_gdf.to_file(file_path, driver=\"GPKG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build full triangulation using greedy triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the greedy triangulation graph using both sets of seed points\n",
    "greedy_gdf, ltn_gdf, tess_gdf = build_greedy_triangulation(ltn_nodes, tess_nodes)\n",
    "max_length = greedy_gdf['distance'].sum() # find the size of the fully connected graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up greedy triangulation to remove silver triangles, and unrealisticly long links.\n",
    "- Removing silvers removes links which would pass along the same route (thus essentially duplicating themselves)\n",
    "- Links longer than 5km are not realistic links to be making, they almost always link rural to rural over routes which can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find very long links\n",
    "greedy_gdf = greedy_gdf[greedy_gdf[\"distance\"] <= 5000]\n",
    "\n",
    "# find slivers and remove the longest edge from them\n",
    "fas = momepy.FaceArtifacts(greedy_gdf) \n",
    "# run FaceArtifacts (cf https://docs.momepy.org/en/stable/api/momepy.FaceArtifacts.html)\n",
    "fas.polygons = fas.polygons.set_crs(greedy_gdf.crs)\n",
    "threshold = 10\n",
    "polys = fas.polygons.copy()\n",
    "slivers = polys[polys['face_artifact_index'] < threshold]\n",
    "slivers[\"centroid\"] = slivers.geometry.centroid\n",
    "centroids = slivers[\"centroid\"]\n",
    "bounding = find_bounding_lines(centroids, greedy_gdf)\n",
    "longest_lines = []\n",
    "for pt_idx, line_idxs in bounding.items():\n",
    "    if not line_idxs:\n",
    "        longest_lines.append(None)  \n",
    "        continue\n",
    "    lines = greedy_gdf.loc[line_idxs]\n",
    "    # Find the longest line\n",
    "    max_line = lines.loc[lines[\"distance\"].idxmax()]\n",
    "    longest_lines.append(max_line.name)\n",
    "lines_to_drop = [idx for idx in longest_lines if idx is not None]\n",
    "greedy_gdf = greedy_gdf.drop(index=lines_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_plotting:\n",
    "    greedy_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Full Triangulation (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get node pairs of ltn-to-tess, tess-to-ltn, and tess-to-tess\n",
    "combined_node_pairs = get_node_pairs(combined_nodes, greedy_gdf)\n",
    "combined_node_pairs = [pair for pair in combined_node_pairs if pair not in ltn_node_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Level of Traffic Stress to triangulation distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the routed distance (using level of traffic stress as a weighting) between each link, prior to calcualting the betweenness centrialty\n",
    "greedy_gdf['sp_lts_route'] = greedy_gdf.apply(lambda row: nx.shortest_path(G_weighted, source=row['start_osmid'], target=row['end_osmid'], weight='length'), axis=1)\n",
    "greedy_gdf['sp_lts_distance'] = greedy_gdf['sp_lts_route'].apply(lambda route: sum(G_weighted[u][v][0]['length'] for u, v in zip(route[:-1], route[1:])))\n",
    "# find normal network distance for directness analysis\n",
    "\n",
    "#G_default = copy.deepcopy(G_weighted) ## remove, G_weighted has lengths as zero so cannot be deweighted\n",
    "#deweight_edges(G_default, tag_lts)\n",
    "\n",
    "greedy_gdf['sp_true_distance'] = greedy_gdf['sp_lts_route'].apply(lambda route: sum(G_default[u][v][0]['length'] for u, v in zip(route[:-1], route[1:])))\n",
    "greedy_gdf['eucl_dist'] = greedy_gdf['distance'] # save for directness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph from the triangulation GeoDataFrame\n",
    "greedy_nx = nx.Graph()\n",
    "for _, row in greedy_gdf.iterrows():\n",
    "    start = row['start_osmid']\n",
    "    end = row['end_osmid']\n",
    "    sp_lts_distance = row['sp_lts_distance']\n",
    "    greedy_nx.add_edge(start, end, geometry=row['geometry'], sp_lts_distance=sp_lts_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ltn prioirty \n",
    "shortest_paths_ltn = []\n",
    "shortest_paths_other = []\n",
    "for node1, node2 in ltn_node_pairs:\n",
    "    try:\n",
    "        path = nx.shortest_path(greedy_nx, source=node1, target=node2, weight='sp_lts_distance')\n",
    "        shortest_paths_ltn.append(path)\n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"No path between {node1} and {node2}\")\n",
    "for node1, node2 in combined_node_pairs:\n",
    "    try:\n",
    "        path = nx.shortest_path(greedy_nx, source=node1, target=node2, weight='sp_lts_distance')\n",
    "        shortest_paths_other.append(path)\n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"No path between {node1} and {node2}\")\n",
    "\n",
    "# for non-ltn priority\n",
    "shortest_paths_all = []\n",
    "for node1, node2 in combined_node_pairs:\n",
    "    try:\n",
    "        path = nx.shortest_path(greedy_nx, source=node1, target=node2, weight='sp_lts_distance')\n",
    "        shortest_paths_all.append(path)\n",
    "    except nx.NetworkXNoPath:\n",
    "        print(f\"No path between {node1} and {node2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find path values for ranking: Get order using sum of { (ebc of edge * (length of edge/sum of length of edges) ) } / number-of-edges.\n",
    "\n",
    "Distance used in these functions is the routed lts distance in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate edge betweenness centrality\n",
    "edge_betweenness = nx.edge_betweenness_centrality(greedy_nx, weight='sp_lts_distance')\n",
    "\n",
    "# find path values\n",
    "ebc_ltn = get_sp_ebc_weights(ltn_node_pairs, shortest_paths_ltn, greedy_nx, edge_betweenness)\n",
    "ebc_other = get_sp_ebc_weights(combined_node_pairs, shortest_paths_other, greedy_nx, edge_betweenness)\n",
    "ebc_all = get_sp_ebc_weights(combined_node_pairs, shortest_paths_all, greedy_nx, edge_betweenness)\n",
    "\n",
    "# order by betweenness path value\n",
    "ebc_ltn = dict(sorted(ebc_ltn.items(), key=lambda item: item[1], reverse=True))\n",
    "ebc_other = dict(sorted(ebc_other.items(), key=lambda item: item[1], reverse=True))\n",
    "ebc_all = dict(sorted(ebc_all.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# set up shortest paths for looping through in budget adjustment\n",
    "shortest_paths_ltn = {\n",
    "    (path[0], path[-1]): [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "    for path in shortest_paths_ltn\n",
    "}\n",
    "shortest_paths_other =  {\n",
    "    (path[0], path[-1]): [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "    for path in shortest_paths_other\n",
    "}\n",
    "\n",
    "shortest_paths_all =  {\n",
    "    (path[0], path[-1]): [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "    for path in shortest_paths_all\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute routed edge lengths for abstract graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures we use our \"budget\" correctly. We first find the optimal route using the LTS tags per connection, then \"deweight\" the path to find the \"true\" length of the connection in meters. Existing infrastucture (bike paths etc) are not included in the \"true\" length, as we do not need to build these facilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the routed distance between edges in abstract GT\n",
    "centroid_osmids = set(all_centroids['osmid'])\n",
    "greedy_gdf['ltn_origin'] = greedy_gdf['start_osmid'].isin(centroid_osmids)\n",
    "greedy_gdf['ltn_destination'] = greedy_gdf['end_osmid'].isin(centroid_osmids)\n",
    "\n",
    "# Build a mapping from centroid osmid to neighbourhood_id\n",
    "osmid_to_neigh = dict(zip(all_centroids['osmid'], all_centroids['neighbourhood_id']))\n",
    "\n",
    "# Build a mapping from neighbourhood_id to list of exit point osmids\n",
    "neigh_to_exits = defaultdict(list)\n",
    "for idx, row in exit_points.iterrows():\n",
    "    neigh_to_exits[row['neighbourhood_id']].append(row['osmid'])\n",
    "\n",
    "# Compute the shortest path and store in 'sp_route'\n",
    "greedy_gdf['sp_route'] = greedy_gdf.apply(lambda row: compute_routed_path_for_GT(row, G_weighted), axis=1)\n",
    "\n",
    "# Find lengths of shortest paths on G_investment_distance\n",
    "## this is the true distance of the investment, without LTS weighting but also without distances for existing infrastructure\n",
    "greedy_gdf['distance'] = greedy_gdf['sp_route'].apply(\n",
    "    lambda route: calculate_sp_route_distance(route, G_investment_distance)\n",
    ")\n",
    "\n",
    "del greedy_gdf['sp_route']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now \"Distance\" is the length of investment required to connect two locations via the best(shortest with weighting of LTS) path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_plotting:\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.axis('off')\n",
    "    location = locations[placeid]\n",
    "    location_gseries = gpd.GeoSeries([location])\n",
    "    location_gdf = gpd.GeoDataFrame(geometry=location_gseries)\n",
    "    location_gdf = location_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    location_gdf.plot(ax=ax, color='lightgrey', zorder=0) \n",
    "\n",
    "    # add seed points\n",
    "    tess_nodes.plot(ax=ax, color='red', markersize=10, zorder=2)\n",
    "    ltn_nodes.plot(ax=ax, color='red', markersize=10, zorder=3)\n",
    "    #greedy_gdf.to_crs(location_gdf.crs).plot(ax=ax, color='blue', linewidth=0.9, zorder=1)\n",
    "\n",
    "    # add ebc\n",
    "    ebc_df = pd.DataFrame([{'start': min(u, v), 'end': max(u, v), 'betw': w} for (u, v), w in edge_betweenness.items()])\n",
    "    merged_gdf = (greedy_gdf.assign(start_norm=greedy_gdf[['start_osmid', 'end_osmid']].min(axis=1),\n",
    "                                    end_norm=greedy_gdf[['start_osmid', 'end_osmid']].max(axis=1))\n",
    "                                    .merge(ebc_df, left_on=['start_norm', 'end_norm'], right_on=['start', 'end'], how='left')).to_crs(location_gdf.crs)\n",
    "    merged_gdf.plot(ax=ax, column='betw', legend=True, cmap='viridis', linewidth=2, zorder=1, legend_kwds={'shrink': 0.6, 'label': \"Normalised Edge Betweenness Centrality\"})\n",
    "    plt.axis('off')\n",
    "    plt.savefig(PATH[\"plots\"] + placeid + \"/greedy_tri_ebc_map.png\", dpi=600,  bbox_inches='tight')\n",
    "    \n",
    "\n",
    "    ############ PLOT DOUBLE GREEDY #############\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.axis('off')\n",
    "    location = locations[placeid]\n",
    "    location_gseries = gpd.GeoSeries([location])\n",
    "    location_gdf = gpd.GeoDataFrame(geometry=location_gseries)\n",
    "    location_gdf = location_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    location_gdf.plot(ax=ax, color='lightgrey', zorder=0) \n",
    "\n",
    "    # add seed points\n",
    "   # tess_nodes.plot(ax=ax, color='red', markersize=10, zorder=4)\n",
    "    ltn_nodes.plot(ax=ax, color='darkorange', markersize=50, zorder=3)\n",
    "    #greedy_gdf.to_crs(location_gdf.crs).plot(ax=ax, olor='blue', linewidth=0.9, zorder=1)\n",
    "    greedy_gdf.to_crs(location_gdf.crs).plot(ax=ax, color='firebrick', linewidth=1, zorder=1)\n",
    "    greedy_triangulation_ltns_gdf.to_crs(location_gdf.crs).plot(ax=ax, color='royalblue', linewidth=2, zorder=2)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(PATH[\"plots\"] + placeid + \"/greedy_tri_neighbours_map.png\", dpi=600,  bbox_inches='tight')\n",
    "\n",
    "    \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export:\n",
    "    # add ebc\n",
    "    ebc_df = pd.DataFrame([{'start': min(u, v), 'end': max(u, v), 'betw': w} for (u, v), w in edge_betweenness.items()])\n",
    "    merged_gdf = (greedy_gdf.assign(start_norm=greedy_gdf[['start_osmid', 'end_osmid']].min(axis=1),\n",
    "                                    end_norm=greedy_gdf[['start_osmid', 'end_osmid']].max(axis=1))\n",
    "                                    .merge(ebc_df, left_on=['start_norm', 'end_norm'], right_on=['start', 'end'], how='left')).to_crs(4326)\n",
    "    file_path = os.path.join(PATH[\"exports_gpkg\"] + placeid + \"/\" + f\"{placeid}_greedy_triangulation.gpkg\"); os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    merged_gdf.to_file(file_path, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the size of a fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the size of the fully connected network\n",
    "total_distance = greedy_gdf['distance'].sum() \n",
    "\n",
    "if debug:\n",
    "    print(\"Total distance of fully connected network: \", total_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set investment max to a fully connected bicycle network, and interval to 1% incremental steps\n",
    "points = np.linspace(0, total_distance, 101)\n",
    "# Drop 0 as we don't need to consider the empty network\n",
    "investment_levels = points[1:].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBC Growth - Set connection order (LTNs prioritised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy triangulation \"distance\" is the length of the shortest path distance between edges once routed on to the network, without including any length from existing infrastucture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create iterations of network growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## betwenness\n",
    "## many combinations\n",
    "previous_selected_edges = set()\n",
    "\n",
    "GT_abstracts = []\n",
    "GT_abstracts_gdf = []\n",
    "GTs = []\n",
    "GTs_gdf = []\n",
    "\n",
    "\n",
    "\n",
    "global_processed_pairs = set()\n",
    "cumulative_GT_indices = set()\n",
    "\n",
    "\n",
    "for D in tqdm(investment_levels, desc=\"Pruning GT abstract and routing on network for meters of investment\"):\n",
    "    # make abstract greedy triangulation graph\n",
    "    GT_abstract_gdf, previous_selected_edges, connected_ltn_pairs, connected_other_pairs = adjust_triangulation_to_budget_ltn_priority(greedy_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges, ltn_node_pairs)\n",
    "    remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "    if debug:\n",
    "        print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "    \n",
    "    GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "    GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "    GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "    if debug:\n",
    "        ax = GT_abstract_gdf.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        for idx, row in ltn_gdf.iterrows():\n",
    "            ax.annotate(\n",
    "                text=str(row['osmid']),  # Use index or another column for labeling\n",
    "                xy=(row.geometry.x, row.geometry.y),  # Get the coordinates of the point\n",
    "                xytext=(3, 3),  # Offset for better readability\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=8,  \n",
    "                color=\"red\"\n",
    "        )\n",
    "        \n",
    "    # poipairs = connected_ltn_pairs | connected_other_pairs\n",
    "    poipairs = list(GT_abstract_nx.edges())\n",
    "    routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Routing on network for investment level: {D} with routdenodepairs\", routenodepairs)\n",
    "    \n",
    "    GT_indices = set()\n",
    "    \n",
    "    ## conditional routing \n",
    "    # ltn --> ltn (all)\n",
    "    # ltn --> tess (all to one)\n",
    "    # tess --> tess (one to one)\n",
    "    # tess --> ltn (one to all)\n",
    "\n",
    "    GT_indices = set()\n",
    "    processed_pairs = set()\n",
    "\n",
    "    for u, v in routenodepairs:\n",
    "        poipair = (u, v)\n",
    "        if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "            continue\n",
    "        \n",
    "        # Determine if nodes are neighbourhood or tessellation\n",
    "        is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "        is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "        \n",
    "        if is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Both nodes are neighbourhoods\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            \n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "            \n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs:\n",
    "                        continue\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    \n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Neighbourhood to Tessellation\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Tessellation to Neighbourhood\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for eb in exit_points_b:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Tessellation to Tessellation\n",
    "            try:\n",
    "                sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                GT_indices.update(sp)\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"This should never happen.\")\n",
    "\n",
    "        global_processed_pairs.add(poipair) # track pairs that we've already routed between\n",
    "    \n",
    "    cumulative_GT_indices.update(GT_indices) # add routes we've already calculated previously to current GT\n",
    "\n",
    "    # Generate subgraph for selected routes\n",
    "    GT = G_caralls[placeid].subgraph(cumulative_GT_indices)\n",
    "    #deweight_edges(GT, tag_lts)\n",
    "    # ensure weight attibute is stored\n",
    "    for u, v, data in GT.edges(data=True):\n",
    "        if 'length' in data:\n",
    "            data['weight'] = data['length']\n",
    "            \n",
    "    GTs.append(GT)\n",
    "    GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "    GTs_gdf.append(GT_edges)\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "        ax = GT_edges.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "\n",
    "\n",
    "   \n",
    "    # convert to igraph for analysis\n",
    "    GTs_igraph = [ig.Graph.from_networkx(gt) for gt in GTs]\n",
    "    GT_abstracts_igraph = [ig.Graph.from_networkx(gt_abstract) for gt_abstract in GT_abstracts]\n",
    "\n",
    "    # write results\n",
    "    #results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": GTs_igraph, \"GT_abstracts\": GT_abstracts_igraph}\n",
    "    #write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files are saved to a pickle. They will be of the form \"newcastle_poi_LTNs_tessellation_betweenness_weighted.pickle\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving as pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_measure = \"betweenness_ltn_priority\"\n",
    "\n",
    "results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": GTs, \"GT_abstracts\": GT_abstracts}\n",
    "write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as geodataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Demand Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading PCT results - Predownloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in lines\n",
    "lines = gpd.read_file(PATH[\"example-data\"] +  \"/lines_lsoa.gpkg\")\n",
    "rnet = gpd.read_file(PATH[\"example-data\"] + \"/rnet_lsoa.gpkg\")\n",
    "lsoa = gpd.read_file(PATH[\"example-data\"] + \"/lsoa.gpkg\")\n",
    "lsoa_bound = gpd.read_file(PATH[\"example-data\"] + \"/lsoa_bound.gpkg\")\n",
    "\n",
    "# clip to boundary\n",
    "boundary = ox.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "#lines = gpd.clip(lines, boundary) # dont clip, otherwise lines which pass temporarily outside the boundary will be removed\n",
    "rnet = gpd.clip(rnet, boundary)\n",
    "lsoa = gpd.clip(lsoa, boundary)\n",
    "lsoa_bound = gpd.clip(lsoa_bound, boundary)\n",
    "valid_lad11cds = lsoa['lad11cd'].unique()\n",
    "lines = lines[lines['lad11cd1'].isin(valid_lad11cds) & lines['lad11cd2'].isin(valid_lad11cds)] # do this instead\n",
    "\n",
    "\n",
    "# test = G_pct_gdfs[99]\n",
    "# test.plot()\n",
    "# total_length_m = test['length_m'].sum()\n",
    "# total_length_m\n",
    "\n",
    "\n",
    "# test1 = GTs_gdf[99]\n",
    "# test1.plot()\n",
    "# total_length_m = test1['length'].sum()\n",
    "# total_length_m\n",
    "\n",
    "\n",
    "# m = test.explore()\n",
    "# test1.explore(m=m, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_plotting:\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.axis('off')\n",
    "    location = locations[placeid]\n",
    "    location_gseries = gpd.GeoSeries([location])\n",
    "    location_gdf = gpd.GeoDataFrame(geometry=location_gseries)\n",
    "    location_gdf = location_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    location_gdf.plot(ax=ax, color='lightgrey', zorder=0)\n",
    "\n",
    "    # add pct lines\n",
    "    lines.plot(ax=ax, column = \"dutch_slc\", scheme='Percentiles', alpha=0.2)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(PATH[\"plots\"] + placeid + \"/pct_raw_demand.png\", dpi=600,  bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.axis('off')\n",
    "    location = locations[placeid]\n",
    "    location_gseries = gpd.GeoSeries([location])\n",
    "    location_gdf = gpd.GeoDataFrame(geometry=location_gseries)\n",
    "    location_gdf = location_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    location_gdf.plot(ax=ax, color='lightgrey', zorder=0)\n",
    "\n",
    "    # add pct lines\n",
    "    lines.plot(ax=ax, column = \"dutch_slc\", scheme='Percentiles', alpha=0.2)\n",
    "    ltn_nodes.plot(ax=ax, color='darkorange', markersize=30, zorder=5)\n",
    "    tess_nodes.plot(ax=ax, color='darkorange', markersize=30, zorder=4)\n",
    "    greedy_gdf.to_crs(lines.crs).plot(ax=ax, color='red', linewidth=0.9, zorder=3)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(PATH[\"plots\"] + placeid + \"/pct_greddy_setup.png\", dpi=600,  bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCT Demand to GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get better demand data, we can transform the desire lines from the PCT's potential demand into the ordering method of links in our greedy triangulation. This can be compared against using betweeness centraility, which up until now has been used as our proxy for demand.\n",
    "\n",
    "PCT desire lines go from LSOA centriods, which are at a different scale to our start and end points. To deal with this, we link each lsoa to its nearest seed point. We can then aggregate the demand between seed points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = gpd.sjoin_nearest(lsoa[['geometry']], combined_nodes[['osmid', 'geometry']], how='left')\n",
    "lsoa['nearest_seed_point'] = join['osmid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method looks to simply spatial join. The issue with this is that some node are not served. Skip this section and run the k-NN approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary mapping geo_code to nearest_seed_point (osmid)\n",
    "# geocode_to_osmid = lsoa.set_index('geo_code')['nearest_seed_point'].to_dict()\n",
    "\n",
    "# # Map geo_code1 and geo_code2 to their corresponding osmids\n",
    "# lines['osmid_start'] = lines['geo_code1'].map(geocode_to_osmid)\n",
    "# lines['osmid_end'] = lines['geo_code2'].map(geocode_to_osmid)\n",
    "\n",
    "# # drop lines which start or finish external to the area\n",
    "# lines = lines.dropna(subset=['osmid_start', 'osmid_end']).query(\"osmid_start != osmid_end\")\n",
    "\n",
    "\n",
    "# # group by osmid pairs\n",
    "# lines['osmid_pairs'] = lines.apply(lambda row: tuple(sorted((row['osmid_start'], row['osmid_end']))), axis=1)\n",
    "\n",
    "# if debug: \n",
    "#     for col in lines.columns:\n",
    "#         print(col)\n",
    "\n",
    "        \n",
    "# # clean\n",
    "# lines = lines[['osmid_start', 'osmid_end', 'geometry', 'length_m', 'osmid_pairs', 'dutch_slc']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines['osmid_start'] = lines['osmid_start'].astype('int64')\n",
    "# lines['osmid_end'] = lines['osmid_end'].astype('int64')\n",
    "# lines['osmid_pairs'] = list(zip(lines['osmid_start'], lines['osmid_end']))\n",
    "\n",
    "\n",
    "# total_dutch_slc = lines.groupby('osmid_pairs')['dutch_slc'].sum().reset_index(name='total_dutch_slc')\n",
    "\n",
    "# # join demand to greedy triangulation\n",
    "# greedy_gdf['osmid_pairs'] = list(zip(greedy_gdf['start_osmid'], greedy_gdf['end_osmid']))\n",
    "# greedy_gdf['start_osmid'] = greedy_gdf['start_osmid'].astype('int64')\n",
    "# greedy_gdf['end_osmid'] = greedy_gdf['end_osmid'].astype('int64')\n",
    "# greedy_gdf = greedy_gdf.merge(total_dutch_slc, on ='osmid_pairs', how='left')\n",
    "\n",
    "# #greedy_gdf['total_dutch_slc'] = greedy_gdf['total_dutch_slc'].fillna(0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a copy of combined_nodes and reproject to Web Mercator (EPSG:3857)\n",
    "# combined_nodes_web_mercator = combined_nodes.to_crs(epsg=3857)\n",
    "\n",
    "# # Buffer the geometries by 200 meters\n",
    "# combined_nodes_web_mercator['geometry'] = combined_nodes_web_mercator['geometry'].buffer(1000)\n",
    "\n",
    "# lsoa_web_mercator = lsoa.to_crs(epsg=3857)\n",
    "# lsoa_web_mercator['geometry'] = lsoa_web_mercator['geometry'].buffer(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m= lsoa.explore()\n",
    "# lsoa_bound.explore(m=m)\n",
    "# greedy_gdf.explore(m=m, color='green', opacity=0.5)\n",
    "# combined_nodes.explore(m=m, color='red')\n",
    "# #lines.explore(m=m, opacity=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the rows where total_dutch_slc is not equal to zero\n",
    "# filtered_greedy_gdf = greedy_gdf[greedy_gdf['total_dutch_slc'].notna() & (greedy_gdf['total_dutch_slc'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the lines with colors based on their groups\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# for group, color in group_color_map.items():\n",
    "#     group_lines = lines[lines['group'] == group]\n",
    "#     group_lines.plot(ax=ax, color=color, label=str(group))\n",
    "\n",
    "\n",
    "# # Plot combined_nodes in blue\n",
    "# combined_nodes.plot(ax=ax, color='blue', markersize=5, label='Combined Nodes')\n",
    "\n",
    "# # Plot lsoa in red\n",
    "# lsoa.plot(ax=ax, color='red', markersize=5, label='LSOA', zorder=1)\n",
    "\n",
    "# # Add legend and show plot\n",
    "# plt.legend(title='Groups (osmid_start, osmid_end)', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find nearest neighbours of seed points, then weight demand by inverse distance from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This joins demand to lines via a k-Nearest Neighbour approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reproject\n",
    "lsoa = lsoa.to_crs(epsg=3857)\n",
    "lines = lines.to_crs(epsg=3857)\n",
    "combined_nodes = combined_nodes.to_crs(epsg=3857)   \n",
    "greedy_gdf = greedy_gdf.to_crs(epsg=3857)\n",
    "\n",
    "\n",
    "\n",
    "# Convert geo_code columns to strings for consistency\n",
    "lsoa['geo_code'] = lsoa['geo_code'].astype(str)\n",
    "lines['geo_code1'] = lines['geo_code1'].astype(str)\n",
    "lines['geo_code2'] = lines['geo_code2'].astype(str)\n",
    "\n",
    "# Compute coordinates\n",
    "lsoa['x'] = lsoa.geometry.x\n",
    "lsoa['y'] = lsoa.geometry.y\n",
    "combined_nodes['x'] = combined_nodes.geometry.x\n",
    "combined_nodes['y'] = combined_nodes.geometry.y\n",
    "\n",
    "# k-NN setup\n",
    "k = 3 # CHANGE\n",
    "combined_coords = combined_nodes[['x', 'y']].values\n",
    "lsoa_coords = lsoa[['x', 'y']].values\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(combined_coords)\n",
    "distances, indices = nbrs.kneighbors(lsoa_coords)\n",
    "\n",
    "# Build mapping dictionary (geo_code -> list of (osmid, normalized_weight))\n",
    "mapping = {}\n",
    "for i, row in enumerate(lsoa.itertuples()):\n",
    "    lsoa_code = row.geo_code\n",
    "    node_indices = indices[i]\n",
    "    dists = distances[i]\n",
    "    # Avoid division by zero by adding a small constant\n",
    "    weights = 1 / (dists + 1e-6)\n",
    "    normalised_weights = weights / weights.sum()\n",
    "    node_ids = combined_nodes.iloc[node_indices]['osmid'].values\n",
    "    mapping[lsoa_code] = list(zip(node_ids, normalised_weights))\n",
    "\n",
    "# Debug: check mapping keys\n",
    "if debug: \n",
    "    print(\"Mapping keys (first 10):\", list(mapping.keys())[:10])\n",
    "    print(\"Total LSOA mapping entries:\", len(mapping))\n",
    "\n",
    "# Generate flow records from desire lines\n",
    "flow_records = []\n",
    "for idx, row in lines.iterrows():\n",
    "    origin_code = row['geo_code1']\n",
    "    dest_code = row['geo_code2']\n",
    "    demand = row['dutch_slc']\n",
    "\n",
    "    # Debug: report missing mappings if any\n",
    "    if debug:\n",
    "        if origin_code not in mapping:\n",
    "            print(f\"Origin code {origin_code} not in mapping\")\n",
    "        if dest_code not in mapping:\n",
    "            print(f\"Destination code {dest_code} not in mapping\")\n",
    "\n",
    "    if origin_code in mapping and dest_code in mapping:\n",
    "        origin_nodes = mapping[origin_code]\n",
    "        dest_nodes = mapping[dest_code]\n",
    "        total_weight_product = sum(w_o * w_d for (_, w_o) in origin_nodes for (_, w_d) in dest_nodes)\n",
    "        for o_node, w_o in origin_nodes:\n",
    "            for d_node, w_d in dest_nodes:\n",
    "                flow_share = demand * (w_o * w_d) / total_weight_product\n",
    "                node_pair = tuple(sorted((o_node, d_node)))\n",
    "                flow_records.append({'osmid_pair': node_pair, 'flow': flow_share})\n",
    "\n",
    "if debug:\n",
    "    print(\"Number of flow records:\", len(flow_records))\n",
    "\n",
    "# Aggregate flows\n",
    "flow_df = pd.DataFrame(flow_records)\n",
    "if flow_df.empty:\n",
    "    print(\"No flow records were generated. Check your mapping and geo_code consistency.\")\n",
    "else:\n",
    "    total_flow = flow_df.groupby('osmid_pair')['flow'].sum().reset_index(name='total_flow')\n",
    "    if debug:\n",
    "        print(total_flow.head())\n",
    "\n",
    "    # Merge with network data (greedy_gdf)\n",
    "    greedy_gdf = greedy_gdf.copy()\n",
    "    \n",
    "    # Ensure osmid_pair is built as sorted tuples to match total_flow keys\n",
    "    greedy_gdf['osmid_pair'] = greedy_gdf.apply(\n",
    "        lambda row: tuple(sorted((int(row['start_osmid']), int(row['end_osmid'])))), axis=1\n",
    "    )\n",
    "    \n",
    "    # Merge the flows\n",
    "    greedy_gdf = greedy_gdf.merge(total_flow, on='osmid_pair', how='left')\n",
    "    greedy_gdf['total_flow'] = greedy_gdf['total_flow'].fillna(0)\n",
    "    if debug: \n",
    "        print(greedy_gdf[['osmid_pair', 'total_flow']].head())\n",
    "\n",
    "\n",
    "lsoa = lsoa.to_crs(epsg=4326)\n",
    "lines = lines.to_crs(epsg=4326)\n",
    "combined_nodes = combined_nodes.to_crs(epsg=4326)   \n",
    "greedy_gdf = greedy_gdf.to_crs(epsg=4326)\n",
    "if debug:\n",
    "    greedy_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if methods_plotting:    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.axis('off')\n",
    "    location = locations[placeid]\n",
    "    location_gseries = gpd.GeoSeries([location])\n",
    "    location_gdf = gpd.GeoDataFrame(geometry=location_gseries)\n",
    "    location_gdf = location_gdf.set_crs(epsg=4326, inplace=True)\n",
    "    location_gdf.plot(ax=ax, color='lightgrey', zorder=0)\n",
    "\n",
    "\n",
    "    ltn_nodes.plot(ax=ax, color='darkorange', markersize=30, zorder=5)\n",
    "    tess_nodes.plot(ax=ax, color='darkorange', markersize=30, zorder=4)\n",
    "    greedy_gdf.to_crs(lines.crs).plot(ax=ax, linewidth=1.5, zorder=3, column='total_flow', cmap='viridis', alpha=0.8)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.savefig(PATH[\"plots\"] + placeid + \"/pct_greedy.png\", dpi=600,  bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demand Growth - Set Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp_demand_weights(node_pairs, shortest_paths, graph, edge_length_key='sp_lts_distance'):\n",
    "    \"\"\"\n",
    "    Calculate the demand from the PCT for each node pair's shortest path.\n",
    "    For each edge in the path, weight its total_flow by the proportion of its length\n",
    "    relative to the total length of the path and average across the number of edges.\n",
    "    \"\"\"\n",
    "    result_dict = {}\n",
    "    \n",
    "    for node_pair in node_pairs:\n",
    "        # Try to retrieve the path using the node pair as key (or its reverse if undirected)\n",
    "        if node_pair in shortest_paths:\n",
    "            path = shortest_paths[node_pair]\n",
    "        elif tuple(reversed(node_pair)) in shortest_paths:\n",
    "            path = shortest_paths[tuple(reversed(node_pair))]\n",
    "        else:\n",
    "            raise KeyError(f\"Path for node pair {node_pair} not found in shortest_paths\")\n",
    "        \n",
    "        # If the path is already a list of edges, use it directly.\n",
    "        # Otherwise, assume it is a list of nodes and convert it to edge pairs.\n",
    "        if path and isinstance(path[0], tuple) and len(path[0]) == 2:\n",
    "            edges_in_path = path\n",
    "        else:\n",
    "            edges_in_path = [(path[j], path[j + 1]) for j in range(len(path) - 1)]\n",
    "        \n",
    "        edges_info = []\n",
    "        for u, v in edges_in_path:\n",
    "            edge_data = graph.get_edge_data(u, v)\n",
    "            if edge_data is None:\n",
    "                edge_data = graph.get_edge_data(v, u)\n",
    "            if edge_data is None:\n",
    "                raise ValueError(f\"Edge ({u}, {v}) not found in graph for path {path}\")\n",
    "            \n",
    "            length = edge_data.get(edge_length_key, 0)\n",
    "            flow = edge_data.get('total_flow', 0)\n",
    "            edges_info.append((length, flow))\n",
    "        \n",
    "        total_length = sum(length for length, _ in edges_info)\n",
    "        num_edges = len(edges_info)\n",
    "        \n",
    "        if num_edges == 0:\n",
    "            result_dict[node_pair] = 0.0\n",
    "            continue\n",
    "        \n",
    "        weighted_sum = sum(flow * (length / total_length) for length, flow in edges_info) if total_length > 0 else 0.0\n",
    "        result_dict[node_pair] = weighted_sum / num_edges\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "greedy_nx = nx.Graph()\n",
    "for _, row in greedy_gdf.iterrows():\n",
    "    start = row['start_osmid']\n",
    "    end = row['end_osmid']\n",
    "    total_flow = row['total_flow']\n",
    "    sp_lts_distance = row.get('sp_lts_distance', 0)  # Retrieve the distance value to ensure it's the average flow across sp rather than total\n",
    "    greedy_nx.add_edge(\n",
    "        start, end,\n",
    "        geometry=row['geometry'],\n",
    "        total_flow=total_flow,\n",
    "        sp_lts_distance=sp_lts_distance\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "demand_ltn = get_sp_demand_weights(ltn_node_pairs, shortest_paths_ltn, greedy_nx)\n",
    "demand_other = get_sp_demand_weights(combined_node_pairs, shortest_paths_other, greedy_nx)\n",
    "demand_all = get_sp_demand_weights(combined_node_pairs, shortest_paths_all, greedy_nx)\n",
    "\n",
    "# order by betweenness path value\n",
    "demand_ltn = dict(sorted(demand_ltn.items(), key=lambda item: item[1], reverse=True))\n",
    "demand_other = dict(sorted(demand_other.items(), key=lambda item: item[1], reverse=True))\n",
    "demand_all = dict(sorted(demand_all.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "greedy_gdf = greedy_gdf.to_crs(3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## demand\n",
    "## many combinations\n",
    "previous_selected_edges = set()\n",
    "\n",
    "GT_abstracts = []\n",
    "GT_abstracts_gdf = []\n",
    "GTs = []\n",
    "GTs_gdf = []\n",
    "\n",
    "\n",
    "\n",
    "global_processed_pairs = set()\n",
    "cumulative_GT_indices = set()\n",
    "\n",
    "\n",
    "for D in tqdm(investment_levels, desc=\"Pruning GT abstract and routing on network for meters of investment\"):\n",
    "    # make abstract greedy triangulation graph\n",
    "    GT_abstract_gdf, previous_selected_edges, connected_ltn_pairs, connected_other_pairs =  adjust_triangulation_to_budget_ltn_priority(greedy_gdf, D, shortest_paths_ltn, demand_ltn, shortest_paths_other, demand_other, previous_selected_edges, ltn_node_pairs)\n",
    "    remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "    if debug:\n",
    "        print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "    \n",
    "    GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "    GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "    GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "    if debug:\n",
    "        ax = GT_abstract_gdf.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        for idx, row in ltn_gdf.iterrows():\n",
    "            ax.annotate(\n",
    "                text=str(row['osmid']),  # Use index or another column for labeling\n",
    "                xy=(row.geometry.x, row.geometry.y),  # Get the coordinates of the point\n",
    "                xytext=(3, 3),  # Offset for better readability\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=8,  \n",
    "                color=\"red\"\n",
    "        )\n",
    "        \n",
    "    # poipairs = connected_ltn_pairs | connected_other_pairs\n",
    "    poipairs = list(GT_abstract_nx.edges())\n",
    "    routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Routing on network for investment level: {D} with routdenodepairs\", routenodepairs)\n",
    "    \n",
    "    GT_indices = set()\n",
    "    \n",
    "    ## conditional routing \n",
    "    # ltn --> ltn (all)\n",
    "    # ltn --> tess (all to one)\n",
    "    # tess --> tess (one to one)\n",
    "    # tess --> ltn (one to all)\n",
    "\n",
    "    GT_indices = set()\n",
    "    processed_pairs = set()\n",
    "\n",
    "    for u, v in routenodepairs:\n",
    "        poipair = (u, v)\n",
    "        if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "            continue\n",
    "        \n",
    "        # Determine if nodes are neighbourhood or tessellation\n",
    "        is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "        is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "        \n",
    "        if is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Both nodes are neighbourhoods\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            \n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "            \n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs:\n",
    "                        continue\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    \n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Neighbourhood to Tessellation\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Tessellation to Neighbourhood\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for eb in exit_points_b:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Tessellation to Tessellation\n",
    "            try:\n",
    "                sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                GT_indices.update(sp)\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"This should never happen.\")\n",
    "\n",
    "        global_processed_pairs.add(poipair) # track pairs that we've already routed between\n",
    "    \n",
    "    cumulative_GT_indices.update(GT_indices) # add routes we've already calculated previously to current GT\n",
    "\n",
    "    # Generate subgraph for selected routes\n",
    "    GT = G_caralls[placeid].subgraph(cumulative_GT_indices)\n",
    "    #deweight_edges(GT, tag_lts)\n",
    "    # ensure weight attibute is stored\n",
    "    for u, v, data in GT.edges(data=True):\n",
    "        if 'length' in data:\n",
    "            data['weight'] = data['length']\n",
    "            \n",
    "    GTs.append(GT)\n",
    "    GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "    GTs_gdf.append(GT_edges)\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "        ax = GT_edges.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "\n",
    "\n",
    "   \n",
    "    # convert to igraph for analysis\n",
    "    GTs_igraph = [ig.Graph.from_networkx(gt) for gt in GTs]\n",
    "    GT_abstracts_igraph = [ig.Graph.from_networkx(gt_abstract) for gt_abstract in GT_abstracts]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "prune_measure = \"demand_ltn_priority\"\n",
    "\n",
    "results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": GTs, \"GT_abstracts\": GT_abstracts}\n",
    "write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTNs Not Prioritised "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section of code has produced bike networks where the growth between LTNs has been prioiritsed over any other link. This section produces growth plans where LTNs aren't given explict priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a lookup table for shortest paths ###\n",
    "\n",
    "# Shuffle edges to get random order\n",
    "shuffled_edges = greedy_gdf.sample(frac=1, random_state=42)\n",
    "\n",
    "# Prepare lookup table for shortest paths\n",
    "exit_nodes = set(exit_points['osmid'])\n",
    "endpoints = set(greedy_gdf['start_osmid']).union(greedy_gdf['end_osmid'])\n",
    "lookup_nodes = exit_nodes.union(endpoints)\n",
    "\n",
    "# Precompute all-pairs shortest paths among lookup nodes\n",
    "sp_length = {}   # (source, target) -> distance\n",
    "sp_path   = {}   # (source, target) -> list of nodes\n",
    "for source in lookup_nodes:\n",
    "    dist_dict, path_dict = nx.single_source_dijkstra(G_weighted, source=source, weight='length')\n",
    "    for target in lookup_nodes:\n",
    "        if target in dist_dict:\n",
    "            sp_length[(source, target)] = dist_dict[target]\n",
    "            sp_path[(source, target)]   = path_dict[target]\n",
    "\n",
    "# Static cache for reuse in repeated random runs\n",
    "global_cache = {'shuffled_edges': shuffled_edges, 'sp_length': sp_length, 'sp_path': sp_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make many runs of random growth and save them.\n",
    "iterations = range(1, 101)  \n",
    "for run_id in tqdm(iterations, desc=\"Random Growth Runs\", unit=\"Run\"):\n",
    "    results = run_random_growth(\n",
    "        placeid=placeid,\n",
    "        poi_source=poi_source,\n",
    "        investment_levels=investment_levels,\n",
    "        weighting=weighting,\n",
    "        greedy_gdf=greedy_gdf,\n",
    "        G_caralls=G_caralls,\n",
    "        G_weighted=G_weighted,\n",
    "        all_centroids=all_centroids,\n",
    "        exit_points=exit_points,\n",
    "        sp_length=sp_length,\n",
    "        sp_path=sp_path,\n",
    "        ltn_gdf=ltn_gdf,\n",
    "        tess_gdf=tess_gdf,\n",
    "        debug=False  \n",
    "    )\n",
    "\n",
    "    suffix = f\"_run{run_id:02d}.pickle\"\n",
    "    write_result(\n",
    "        res=results,\n",
    "        mode=\"pickle\",\n",
    "        placeid=placeid,\n",
    "        poi_source=poi_source,\n",
    "        prune_measure=\"random\",\n",
    "        suffix=suffix,\n",
    "        weighting=weighting\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## betwenness\n",
    "## many combinations\n",
    "previous_selected_edges = set()\n",
    "GT_abstracts = []\n",
    "GT_abstracts_gdf = []\n",
    "GTs = []\n",
    "GTs_gdf = []\n",
    "global_processed_pairs = set()\n",
    "cumulative_GT_indices = set()\n",
    "\n",
    "for D in tqdm(investment_levels, desc=\"Pruning GT abstract and routing on network for meters of investment\"):\n",
    "    GT_abstract_gdf, previous_selected_edges, connected_pairs = adjust_triangulation_to_budget(greedy_gdf, D, shortest_paths_all, ebc_all, previous_selected_edges)\n",
    "    remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "    if debug:\n",
    "        print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "    \n",
    "    GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "    GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "    GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "    if debug:\n",
    "        ax = GT_abstract_gdf.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        for idx, row in ltn_gdf.iterrows():\n",
    "            ax.annotate(\n",
    "                text=str(row['osmid']),  # Use index or another column for labeling\n",
    "                xy=(row.geometry.x, row.geometry.y),  # Get the coordinates of the point\n",
    "                xytext=(3, 3),  # Offset for better readability\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=8,  \n",
    "                color=\"red\"\n",
    "        )\n",
    "        \n",
    "    # poipairs = connected_ltn_pairs | connected_other_pairs\n",
    "    poipairs = list(GT_abstract_nx.edges())\n",
    "    routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Routing on network for investment level: {D} with routdenodepairs\", routenodepairs)\n",
    "    \n",
    "    GT_indices = set()\n",
    "    \n",
    "    ## conditional routing \n",
    "    # ltn --> ltn (all)\n",
    "    # ltn --> tess (all to one)\n",
    "    # tess --> tess (one to one)\n",
    "    # tess --> ltn (one to all)\n",
    "\n",
    "    GT_indices = set()\n",
    "    processed_pairs = set()\n",
    "\n",
    "    for u, v in routenodepairs:\n",
    "        poipair = (u, v)\n",
    "        if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "            continue\n",
    "        \n",
    "        # Determine if nodes are neighbourhood or tessellation\n",
    "        is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "        is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "        \n",
    "        if is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Both nodes are neighbourhoods\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            \n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "            \n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs:\n",
    "                        continue\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    \n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Neighbourhood to Tessellation\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Tessellation to Neighbourhood\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for eb in exit_points_b:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Tessellation to Tessellation\n",
    "            try:\n",
    "                sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                GT_indices.update(sp)\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"This should never happen.\")\n",
    "\n",
    "        global_processed_pairs.add(poipair) # track pairs that we've already routed between\n",
    "    \n",
    "    cumulative_GT_indices.update(GT_indices) # add routes we've already calculated previously to current GT\n",
    "\n",
    "    # Generate subgraph for selected routes\n",
    "    GT = G_caralls[placeid].subgraph(cumulative_GT_indices)\n",
    "    #deweight_edges(GT, tag_lts)\n",
    "    # ensure weight attibute is stored\n",
    "    for u, v, data in GT.edges(data=True):\n",
    "        if 'length' in data:\n",
    "            data['weight'] = data['length']\n",
    "            \n",
    "    GTs.append(GT)\n",
    "    GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "    GTs_gdf.append(GT_edges)\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "        ax = GT_edges.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "\n",
    "\n",
    "   \n",
    "    # convert to igraph for analysis\n",
    "    GTs_igraph = [ig.Graph.from_networkx(gt) for gt in GTs]\n",
    "    GT_abstracts_igraph = [ig.Graph.from_networkx(gt_abstract) for gt_abstract in GT_abstracts]\n",
    "\n",
    "    # write results\n",
    "    #results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": GTs_igraph, \"GT_abstracts\": GT_abstracts_igraph}\n",
    "    #write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_measure = \"betweenness\"\n",
    "\n",
    "results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": GTs, \"GT_abstracts\": GT_abstracts}\n",
    "write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## demand\n",
    "## many combinations\n",
    "previous_selected_edges = set()\n",
    "\n",
    "GT_abstracts = []\n",
    "GT_abstracts_gdf = []\n",
    "GTs = []\n",
    "GTs_gdf = []\n",
    "\n",
    "\n",
    "\n",
    "global_processed_pairs = set()\n",
    "cumulative_GT_indices = set()\n",
    "\n",
    "\n",
    "for D in tqdm(investment_levels, desc=\"Pruning GT abstract and routing on network for meters of investment\"):\n",
    "    # make abstract greedy triangulation graph\n",
    "    GT_abstract_gdf, previous_selected_edges, connected_pairs  =  adjust_triangulation_to_budget(greedy_gdf, D, shortest_paths_all, demand_all, previous_selected_edges)\n",
    "    remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "    if debug:\n",
    "        print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "    \n",
    "    GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "    GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "    GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "    if debug:\n",
    "        ax = GT_abstract_gdf.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        for idx, row in ltn_gdf.iterrows():\n",
    "            ax.annotate(\n",
    "                text=str(row['osmid']),  # Use index or another column for labeling\n",
    "                xy=(row.geometry.x, row.geometry.y),  # Get the coordinates of the point\n",
    "                xytext=(3, 3),  # Offset for better readability\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=8,  \n",
    "                color=\"red\"\n",
    "        )\n",
    "        \n",
    "    # poipairs = connected_ltn_pairs | connected_other_pairs\n",
    "    poipairs = list(GT_abstract_nx.edges())\n",
    "    routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Routing on network for investment level: {D} with routdenodepairs\", routenodepairs)\n",
    "    \n",
    "    GT_indices = set()\n",
    "    \n",
    "    ## conditional routing \n",
    "    # ltn --> ltn (all)\n",
    "    # ltn --> tess (all to one)\n",
    "    # tess --> tess (one to one)\n",
    "    # tess --> ltn (one to all)\n",
    "\n",
    "    GT_indices = set()\n",
    "    processed_pairs = set()\n",
    "\n",
    "    for u, v in routenodepairs:\n",
    "        poipair = (u, v)\n",
    "        if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "            continue\n",
    "        \n",
    "        # Determine if nodes are neighbourhood or tessellation\n",
    "        is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "        is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "        \n",
    "        if is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Both nodes are neighbourhoods\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            \n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "            \n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs:\n",
    "                        continue\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    \n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Neighbourhood to Tessellation\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Tessellation to Neighbourhood\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for eb in exit_points_b:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Tessellation to Tessellation\n",
    "            try:\n",
    "                sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                GT_indices.update(sp)\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"This should never happen.\")\n",
    "\n",
    "        global_processed_pairs.add(poipair) # track pairs that we've already routed between\n",
    "    \n",
    "    cumulative_GT_indices.update(GT_indices) # add routes we've already calculated previously to current GT\n",
    "\n",
    "    # Generate subgraph for selected routes\n",
    "    GT = G_caralls[placeid].subgraph(cumulative_GT_indices)\n",
    "    #deweight_edges(GT, tag_lts)\n",
    "    # ensure weight attibute is stored\n",
    "    for u, v, data in GT.edges(data=True):\n",
    "        if 'length' in data:\n",
    "            data['weight'] = data['length']\n",
    "            \n",
    "    GTs.append(GT)\n",
    "    GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "    GTs_gdf.append(GT_edges)\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "        ax = GT_edges.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "\n",
    "\n",
    "   \n",
    "    # convert to igraph for analysis\n",
    "    GTs_igraph = [ig.Graph.from_networkx(gt) for gt in GTs]\n",
    "    GT_abstracts_igraph = [ig.Graph.from_networkx(gt_abstract) for gt_abstract in GT_abstracts]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "prune_measure = \"demand\"\n",
    "\n",
    "results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": GTs, \"GT_abstracts\": GT_abstracts}\n",
    "write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - You don't need to run any of this code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expermient with determining k in pct aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of neighbours used in in the k-nn has an impact on how accurate assigning demand will be. Currently testing adaptive demand and using an elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "k_test = 5  # Test with different values\n",
    "nbrs = NearestNeighbors(n_neighbors=k_test).fit(lsoa[['x', 'y']])\n",
    "distances, _ = nbrs.kneighbors(lsoa[['x', 'y']])\n",
    "\n",
    "# Sort distances to the k-th neighbor\n",
    "sorted_distances = np.sort(distances[:, k_test-1])\n",
    "\n",
    "# Plot the sorted k-distances\n",
    "plt.plot(sorted_distances)\n",
    "plt.xlabel(\"Points Sorted by Distance\")\n",
    "plt.ylabel(f\"Distance to {k_test}-th Nearest Neighbor\")\n",
    "plt.title(\"k-Distance Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Compute average distance to nearest 5 neighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(lsoa[['x', 'y']])\n",
    "distances, _ = nbrs.kneighbors(lsoa[['x', 'y']])\n",
    "avg_dist = distances.mean(axis=1)\n",
    "\n",
    "# Set adaptive k: more neighbors in sparse regions, fewer in dense regions\n",
    "adaptive_k = np.round(1 + (avg_dist / avg_dist.max()) * 10).astype(int)\n",
    "\n",
    "print(\"Adaptive k values (first 100):\", adaptive_k[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_gdf.explore(column='total_flow', cmap='viridis', legend=True, legend_name='Dutch SLC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce a network using routed PCT dutch scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the GeoDataFrame by 'dutch_slc' descending - most potential first\n",
    "sorted_rnet = rnet.sort_values('dutch_slc', ascending=False)\n",
    "cumulative_lengths = sorted_rnet['length_m'].cumsum().values\n",
    "\n",
    "G_pct_gdfs = []\n",
    "\n",
    "# Track the maximum index reached to avoid reprocessing\n",
    "max_idx = 0\n",
    "\n",
    "for D in investment_levels:\n",
    "    # Find the furthest index where cumulative length <= D\n",
    "    idx = np.searchsorted(cumulative_lengths[max_idx:], D, side='right') + max_idx\n",
    "    idx = min(idx, len(sorted_rnet))  # Ensure we don't exceed the dataframe\n",
    "    \n",
    "    # Select edges up to this index\n",
    "    subset = sorted_rnet.iloc[:idx]\n",
    "    G_pct_gdfs.append(subset)\n",
    "    \n",
    "    # Update max_idx to reflect edges already included\n",
    "    max_idx = idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt at getting population into buildings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ukcensusapi.Nomisweb as census_api\n",
    "\n",
    "os.environ[\"NOMIS_API_KEY\"] = \"0x98f2ffbbe685f2623fa5c201d4ff86a8c9c46dee\"\n",
    "\n",
    "api = census_api.Nomisweb(cache_dir = PATH[\"data\"]+ \"/\" + placeid)\n",
    "\n",
    "# Define the dataset and geography\n",
    "dataset_id = \"NM_2010_1\"  # Ensure this is the correct dataset ID\n",
    "geography = \"LSOA11\"      # 2021 LSOA geography type\n",
    "measures = [\"OBS_VALUE\"]  # Population measure\n",
    "\n",
    "# List of LSOA codes you're interested in\n",
    "lsoa_codes = lsoa_bound[\"geo_code\"].tolist()  # Assuming lsoa_bound is your GeoDataFrame\n",
    "\n",
    "# Fetch data for the specified LSOA codes\n",
    "population_data = api.get_data(\n",
    "    dataset_id,\n",
    "    {\"geography\": geography, \"measures\": measures, \"geography_code\": lsoa_codes, \"date\": \"latest\"}\n",
    ")\n",
    "\n",
    "# Display the fetched data\n",
    "print(population_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_levels = [5503.54106,\n",
    " 369233.16903,\n",
    " 372964.797]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## many combinations\n",
    "random_edges = pd.Series(False, index=greedy_gdf.index)  \n",
    "distance = 0.0 # needed for budget\n",
    "\n",
    "Random_GT_abstracts = []\n",
    "Random_GTs = []\n",
    "\n",
    "# reset global variables\n",
    "global_processed_pairs_random = set()\n",
    "cumulative_GT_indices_random = set()\n",
    "\n",
    "\n",
    "for D in tqdm(investment_levels, desc=\"Pruning GT abstract randomly and routing on network for meters of investment\"):\n",
    "    # make abstract greedy triangulation graph\n",
    "    # Calculate remaining budget for new edges\n",
    "    remaining_budget = D - distance\n",
    "    \n",
    "    if remaining_budget > 0:\n",
    "        unselected_edges = greedy_gdf[~random_edges]  # Get edges not yet selected\n",
    "        if not unselected_edges.empty:\n",
    "            shuffled_edges = unselected_edges.sample(frac=1) # Shuffle unselected edges to randomize selection\n",
    "            cumulative_distances = shuffled_edges['distance'].cumsum()\n",
    "            within_budget = cumulative_distances <= remaining_budget  # Find edges that fit within the remaining budget\n",
    "            new_indices = shuffled_edges[within_budget].index   # Get indices of edges to add\n",
    "            random_edges[new_indices] = True\n",
    "            remaining_edges = (~random_edges).sum()\n",
    "            print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "            if within_budget.any():\n",
    "                distance += cumulative_distances[within_budget].iloc[-1]\n",
    "    # save edges\n",
    "    GT_abstract_gdf = greedy_gdf[random_edges].copy()\n",
    "    GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "    Random_GT_abstracts.append(GT_abstract_nx)\n",
    "    poipairs = list(GT_abstract_nx.edges()) # store currently connected points for routing\n",
    "\n",
    "    if debug:\n",
    "        ax = GT_abstract_gdf.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        for idx, row in ltn_gdf.iterrows():\n",
    "            ax.annotate(\n",
    "                text=str(row['osmid']),  # Use index or another column for labeling\n",
    "                xy=(row.geometry.x, row.geometry.y),  # Get the coordinates of the point\n",
    "                xytext=(3, 3),  # Offset for better readability\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=8,  \n",
    "                color=\"red\"\n",
    "        )\n",
    "        \n",
    "    \n",
    "    routenodepairs = [(u, v) for u, v in poipairs]\n",
    "   \n",
    "\n",
    "    if debug:\n",
    "        print(f\"Routing on network for investment level: {D} with routdenodepairs\", routenodepairs)\n",
    "    \n",
    "    \n",
    "    ## conditional routing \n",
    "    # ltn --> ltn (all)\n",
    "    # ltn --> tess (all to one)\n",
    "    # tess --> tess (one to one)\n",
    "    # tess --> ltn (one to all)\n",
    "\n",
    "    GT_indices = set()\n",
    "    processed_pairs = set()\n",
    "\n",
    "    for u, v in routenodepairs:\n",
    "        poipair = (u, v)\n",
    "        if poipair in global_processed_pairs_random or tuple(reversed(poipair)) in global_processed_pairs_random:\n",
    "            continue\n",
    "        \n",
    "        # Determine if nodes are neighbourhood or tessellation\n",
    "        is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "        is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "        \n",
    "        if is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Both nodes are neighbourhoods\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            \n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "            \n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs:\n",
    "                        continue\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    \n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "                if debug:\n",
    "                    print(\"Routed between:\", u, v,\"on path: \", best_path)\n",
    "\n",
    "        elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Neighbourhood to Tessellation\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "                if debug:\n",
    "                    print(\"Routed between:\", u, v,\"with a path length of\", shortest_path_length , \"on path: \", best_path)\n",
    "                    print(\"The exit points were:\", exit_points_a)\n",
    "\n",
    "        elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Tessellation to Neighbourhood\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for eb in exit_points_b:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "                if debug:\n",
    "                    print(\"Routed between:\", u, v,\"on path: \", best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Tessellation to Tessellation\n",
    "            try:\n",
    "                sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                GT_indices.update(sp)\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"This should never happen.\")\n",
    "\n",
    "        global_processed_pairs.add(poipair) # track pairs that we've already routed between\n",
    "    \n",
    "    cumulative_GT_indices_random.update(GT_indices) # add routes we've already calculated previously to current GT\n",
    "\n",
    "    # Generate subgraph for selected routes\n",
    "    GT = G_caralls[placeid].subgraph(cumulative_GT_indices_random)\n",
    "    #deweight_edges(GT, tag_lts)\n",
    "    # ensure weight attibute is stored\n",
    "    for u, v, data in GT.edges(data=True):\n",
    "        if 'length' in data:\n",
    "            data['weight'] = data['length']\n",
    "            \n",
    "    Random_GTs.append(GT)\n",
    "        \n",
    "    \n",
    "\n",
    "    if debug:\n",
    "        GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "        ax = GT_edges.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "\n",
    "\n",
    "   \n",
    "    # convert to igraph for analysis\n",
    "    Random_GTs_igraph = [ig.Graph.from_networkx(gt) for gt in Random_GTs]\n",
    "    Random_GT_abstracts_igraph = [ig.Graph.from_networkx(gt_abstract) for gt_abstract in Random_GT_abstracts]\n",
    "\n",
    "    # write results\n",
    "    #results = {\"placeid\": placeid, \"prune_measure\": random, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": Random_GTs_igraph, \"GT_abstracts\": Random_GT_abstracts_igraph}\n",
    "    #write_result(results, \"pickle\", placeid, poi_source, random, \".pickle\", weighting=weighting)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "final_remaining = (~random_edges).sum()\n",
    "print(f\"Final remaining edges: {final_remaining}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(previous_selected_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_unused_edges(gdf, selected_edges):\n",
    "#     def normalize_edge(row):\n",
    "#         return tuple(sorted((row['start_osmid'], row['end_osmid'])))\n",
    "#     return gdf[~gdf.apply(lambda row: normalize_edge(row) in selected_edges, axis=1)]\n",
    "\n",
    "# adjusted_gdf, selected_edges, _, _ = adjust_triangulation_to_budget_ltn_priority(greedy_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges, ltn_node_pairs)\n",
    "# remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "# unused_edges_gdf = get_unused_edges(greedy_gdf, selected_edges)\n",
    "\n",
    "# # Plotting the results\n",
    "# ax = greedy_gdf.plot(color='lightgrey', linewidth=1, alpha=0.5)  # background context\n",
    "# unused_edges_gdf.plot(ax=ax, color='red', linewidth=2)\n",
    "\n",
    "# unused_edges_gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebc_ltn.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_triangulation_to_budget_ltn_priority(triangulation_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges=None, ltn_node_pairs=None):\n",
    "    \"\"\"\n",
    "    Adjust a given triangulation to fit within the specified budget D,\n",
    "    ensuring that previously selected edges are always included.\n",
    "    Only after all ltns are connected do we move to include the growth of other areas.\n",
    "    \"\"\"\n",
    "    # make a graph\n",
    "    G = nx.Graph()\n",
    "    for _, row in triangulation_gdf.iterrows():\n",
    "        G.add_edge(\n",
    "            row['start_osmid'],\n",
    "            row['end_osmid'],\n",
    "            geometry=row['geometry'],\n",
    "            distance=row['distance']\n",
    "        )\n",
    "\n",
    "    total_length = 0\n",
    "    selected_edges = set(tuple(sorted(edge)) for edge in (previous_selected_edges or [])) # use tuple to ensure we don't double count edges\n",
    "\n",
    "    # Include previously selected edges so that we aren't starting from stratch each loop through\n",
    "    for u, v in selected_edges:\n",
    "        if G.has_edge(u, v):\n",
    "            total_length += G[u][v]['distance']\n",
    "\n",
    "    # Track the ltns which are connected\n",
    "    connected_ltn_pairs = set()\n",
    "\n",
    "    # Track all other connected pairs\n",
    "    connected_other_pairs = set()\n",
    "\n",
    "    # Prune for ltn node pairs first\n",
    "    for (node1, node2), centrality in ebc_ltn.items():\n",
    "        if node1 in G.nodes and node2 in G.nodes:\n",
    "            edges = shortest_paths_ltn.get((node1, node2), [])\n",
    "            if edges:  # If a valid path exists\n",
    "                # Calculate new edges and their length\n",
    "                new_edges = [tuple(sorted((u, v))) for u, v in edges if tuple(sorted((u, v))) not in selected_edges]\n",
    "                new_length = sum(G[min(u, v)][max(u, v)]['distance'] for u, v in new_edges)\n",
    "                # Check if adding this path exceeds the budget D\n",
    "                if total_length + new_length > D:\n",
    "                    continue\n",
    "                # Add the edges to selected_edges\n",
    "                selected_edges.update(new_edges)\n",
    "                total_length += new_length\n",
    "                connected_ltn_pairs.add((node1, node2))\n",
    "\n",
    "    \n",
    "    # Check if all ltn node pairs are connected\n",
    "    if set(ltn_node_pairs).issubset(connected_ltn_pairs):\n",
    "        # Now move to all other connections (ltn to tess, tess to tess, tess to ltn etc)\n",
    "        for (node1, node2), centrality in ebc_other.items():\n",
    "            if node1 in G.nodes and node2 in G.nodes:\n",
    "                edges = shortest_paths_other.get((node1, node2), [])\n",
    "                if edges:  # If a valid path exists\n",
    "                    new_edges = [tuple(sorted((u, v))) for u, v in edges if tuple(sorted((u, v))) not in selected_edges]\n",
    "                    new_length = sum(G[min(u, v)][max(u, v)]['distance'] for u, v in new_edges)\n",
    "                    # Check if adding this path exceeds the budget D\n",
    "                    if total_length + new_length > D:\n",
    "                        continue\n",
    "                    # Add the edges to selected_edges\n",
    "                    selected_edges.update(new_edges)\n",
    "                    total_length += new_length\n",
    "                    connected_other_pairs.add((node1, node2))\n",
    "    # missing_pairs = [pair for pair in ltn_node_pairs if pair not in connected_ltn_pairs]\n",
    "\n",
    "\n",
    "    # edges which aren't in a shortest path won't have been selected\n",
    "    # we will add these last, as they are the least important\n",
    "    unused_edges = []\n",
    "    for _, row in triangulation_gdf.iterrows():\n",
    "        e = tuple(sorted((row['start_osmid'], row['end_osmid'])))\n",
    "        dist = row['distance']\n",
    "        if e not in selected_edges:\n",
    "            unused_edges.append((dist, e))\n",
    "    unused_edges.sort(key=lambda x: x[0])\n",
    "    for dist, edge in unused_edges:\n",
    "        if total_length + dist <= D:\n",
    "            selected_edges.add(edge)\n",
    "            total_length += dist\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Build the adjusted GeoDataFrame\n",
    "    lines = []\n",
    "    distances = []\n",
    "    start_osmids = []\n",
    "    end_osmids = []\n",
    "\n",
    "\n",
    "    for u, v in selected_edges:\n",
    "        lines.append(G[u][v]['geometry'])\n",
    "        distances.append(G[u][v]['distance'])\n",
    "        start_osmids.append(u)\n",
    "        end_osmids.append(v)\n",
    "\n",
    "    adjusted_gdf = gpd.GeoDataFrame({\n",
    "        'geometry': lines,\n",
    "        'start_osmid': start_osmids,\n",
    "        'end_osmid': end_osmids,\n",
    "        'distance': distances,\n",
    "    }, crs=triangulation_gdf.crs)\n",
    "\n",
    "    return adjusted_gdf, selected_edges, connected_ltn_pairs, connected_other_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# PLOT EVERY GT_ABSTRACT  if debug #############\n",
    "\n",
    "\n",
    "# ## betwenness\n",
    "# ## many combinations\n",
    "# previous_selected_edges = set()\n",
    "\n",
    "# GT_abstracts = []\n",
    "# GTs = []\n",
    "\n",
    "# global_processed_pairs = set()\n",
    "# cumulative_GT_indices = set()\n",
    "\n",
    "\n",
    "# for D in tqdm(investment_levels, desc=\"Pruning GT abstract and routing on network for meters of investment\"):\n",
    "#     # make abstract greedy triangulation graph\n",
    "#     GT_abstract_gdf, previous_selected_edges, connected_ltn_pairs, connected_other_pairs = adjust_triangulation_to_budget_ltn_priority(greedy_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges, ltn_node_pairs)\n",
    "#     remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "#     if debug:\n",
    "#         print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "    \n",
    "#     GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "#     GT_abstracts.append(GT_abstract_nx)\n",
    "#     if debug:\n",
    "#         # Plot GT_abstract_gdf with a title indicating the number of meters used to invest at that level\n",
    "#         ax = GT_abstract_gdf.plot()\n",
    "#         ax.set_title(f\"Investment level: {D} meters\")\n",
    "#         ax.set_title(f\"Investment level: {D} meters, Number of edges: {len(GT_abstract_gdf)}\")\n",
    "#         plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = GT_abstracts[2]\n",
    "Test.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List to store total lengths of each graph in GT_abstracts\n",
    "# total_lengths_abstracts = []\n",
    "\n",
    "# for G in GT_abstracts:\n",
    "#     # Calculate total edge length of the graph\n",
    "#     lengths = nx.get_edge_attributes(G, 'weight')\n",
    "#     total_length = sum(lengths.values())\n",
    "#     total_lengths_abstracts.append(total_length)\n",
    "\n",
    "# # Create a line plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(total_lengths_abstracts, marker='o', linestyle='-')\n",
    "# plt.title('Total Length of Abstract Graphs')\n",
    "# plt.xlabel('Graph Index in GT_abstracts')\n",
    "# plt.ylabel('Total Length (meters)')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set investment max to a fully connected bicycle network, and interval to 1% incremental steps\n",
    "points = np.linspace(0, total_distance, 101)\n",
    "# Drop 0 as we don't need to consider the empty network\n",
    "investment_levels = points[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(GT_abstract_nx.edges())\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## betwenness\n",
    "## many combinations\n",
    "previous_selected_edges = set()\n",
    "\n",
    "GT_abstracts = []\n",
    "GT_abstracts_gdf = []\n",
    "GTs = []\n",
    "GTs_gdf = []\n",
    "\n",
    "\n",
    "\n",
    "global_processed_pairs = set()\n",
    "cumulative_GT_indices = set()\n",
    "\n",
    "\n",
    "for D in tqdm(investment_levels, desc=\"Pruning GT abstract and routing on network for meters of investment\"):\n",
    "    # make abstract greedy triangulation graph\n",
    "    GT_abstract_gdf, previous_selected_edges, connected_ltn_pairs, connected_other_pairs = adjust_triangulation_to_budget_ltn_priority(greedy_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges, ltn_node_pairs)\n",
    "    remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "    if debug:\n",
    "        print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "    \n",
    "    GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "    GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "    GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "    if debug:\n",
    "        ax = GT_abstract_gdf.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        for idx, row in ltn_gdf.iterrows():\n",
    "            ax.annotate(\n",
    "                text=str(row['osmid']),  # Use index or another column for labeling\n",
    "                xy=(row.geometry.x, row.geometry.y),  # Get the coordinates of the point\n",
    "                xytext=(3, 3),  # Offset for better readability\n",
    "                textcoords=\"offset points\",\n",
    "                fontsize=8,  \n",
    "                color=\"red\"\n",
    "        )\n",
    "        \n",
    "    # poipairs = connected_ltn_pairs | connected_other_pairs\n",
    "    poipairs = list(GT_abstract_nx.edges())\n",
    "    routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Routing on network for investment level: {D} with routdenodepairs\", routenodepairs)\n",
    "    \n",
    "    GT_indices = set()\n",
    "    \n",
    "    ## conditional routing \n",
    "    # ltn --> ltn (all)\n",
    "    # ltn --> tess (all to one)\n",
    "    # tess --> tess (one to one)\n",
    "    # tess --> ltn (one to all)\n",
    "\n",
    "    GT_indices = set()\n",
    "    processed_pairs = set()\n",
    "\n",
    "    for u, v in routenodepairs:\n",
    "        poipair = (u, v)\n",
    "        if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "            continue\n",
    "        \n",
    "        # Determine if nodes are neighbourhood or tessellation\n",
    "        is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "        is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "        \n",
    "        if is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Both nodes are neighbourhoods\n",
    "            neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            \n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "            \n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                for eb in exit_points_b:\n",
    "                    pair_id = tuple(sorted((ea, eb)))\n",
    "                    if pair_id in processed_pairs:\n",
    "                        continue\n",
    "                    processed_pairs.add(pair_id)\n",
    "                    \n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Neighbourhood to Tessellation\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "            exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for ea in exit_points_a:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "            # Tessellation to Neighbourhood\n",
    "            neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "            shortest_path_length, best_path = float('inf'), None\n",
    "            for eb in exit_points_b:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                    sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                    if sp_length < shortest_path_length:\n",
    "                        shortest_path_length, best_path = sp_length, sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "            \n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "\n",
    "        elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "            # Tessellation to Tessellation\n",
    "            try:\n",
    "                sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                GT_indices.update(sp)\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"This should never happen.\")\n",
    "\n",
    "        global_processed_pairs.add(poipair) # track pairs that we've already routed between\n",
    "    \n",
    "    cumulative_GT_indices.update(GT_indices) # add routes we've already calculated previously to current GT\n",
    "\n",
    "    # Generate subgraph for selected routes\n",
    "    GT = G_caralls[placeid].subgraph(cumulative_GT_indices)\n",
    "    #deweight_edges(GT, tag_lts)\n",
    "    # ensure weight attibute is stored\n",
    "    for u, v, data in GT.edges(data=True):\n",
    "        if 'length' in data:\n",
    "            data['weight'] = data['length']\n",
    "            \n",
    "    GTs.append(GT)\n",
    "    GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "    GTs_gdf.append(GT_edges)\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "        ax = GT_edges.plot()\n",
    "        ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "        tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "        ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "\n",
    "\n",
    "   \n",
    "    # convert to igraph for analysis\n",
    "    GTs_igraph = [ig.Graph.from_networkx(gt) for gt in GTs]\n",
    "    GT_abstracts_igraph = [ig.Graph.from_networkx(gt_abstract) for gt_abstract in GT_abstracts]\n",
    "\n",
    "    # write results\n",
    "    #results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": GTs_igraph, \"GT_abstracts\": GT_abstracts_igraph}\n",
    "    #write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "\n",
    "ax = GTs_gdf[iteration].to_crs(epsg=3857).plot()\n",
    "GT_abstracts_gdf[iteration].to_crs(epsg=3857).plot(ax=ax, color='red')\n",
    "\n",
    "ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "tess_gdf.plot(ax=ax, color='green', markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_nx\n",
    "\n",
    "# Calculate edge betweenness centrality\n",
    "edge_betweenness = nx.edge_betweenness_centrality(greedy_nx, weight='sp_lts_distance')\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "edges = []\n",
    "for u, v, data in greedy_nx.edges(data=True):\n",
    "    edge_data = {\n",
    "        'start_osmid': u,\n",
    "        'end_osmid': v,\n",
    "        'geometry': data['geometry'],\n",
    "        'betweenness_centrality': edge_betweenness[(u, v)]\n",
    "    }\n",
    "    edges.append(edge_data)\n",
    "\n",
    "greedy_gdf_with_betweenness = gpd.GeoDataFrame(edges).set_crs(3857)\n",
    "\n",
    "# Calculate the rank of betweenness centrality\n",
    "greedy_gdf_with_betweenness['betweenness_rank'] = greedy_gdf_with_betweenness['betweenness_centrality'].rank(ascending=False).astype(int)\n",
    "\n",
    "greedy_gdf_with_betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the top 20 rows based on betweenness_rank\n",
    "top_20_betweenness = greedy_gdf_with_betweenness.nsmallest(20, 'betweenness_rank')\n",
    "\n",
    "# Use .explore() to visualize the top 20 betweenness ranking\n",
    "top_20_betweenness.explore(cmap='viridis', column='betweenness_rank', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_gdf_with_betweenness.explore(cmap='viridis', column='betweenness_rank', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 50\n",
    "m= GT_abstracts_gdf[iteration].to_crs(epsg=3857).explore(color='red')\n",
    "neighbourhoods['Newcastle Upon Tyne'].explore(m=m, color='red', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate total lengths for each graph in GTs\n",
    "total_lengths = []\n",
    "for G in GTs:\n",
    "    lengths = nx.get_edge_attributes(G, 'length')\n",
    "    total_length = sum(lengths.values())\n",
    "    total_lengths.append(total_length)\n",
    "\n",
    "# Plot the total lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_lengths, marker='o', linestyle='-')\n",
    "plt.xlabel('Graph Index')\n",
    "plt.ylabel('Total Length (meters)')\n",
    "plt.title('Total Length of Graphs in GTs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "ax = GT_edges.plot()\n",
    "\n",
    "GT_abs_nodes, GT_abs_edges = ox.graph_to_gdfs(GT_abs)\n",
    "GT_abs_edges = GT_abs_edges.to_crs(epsg=3857)\n",
    "GT_abs_edges.plot(ax=ax, color='red', linewidth=2)\n",
    "\n",
    "ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpkg_path = PATH[\"data\"] + placeid + \"/\" + placeid + '_biketrack.gpkg'\n",
    "# G_biketrack = ox_gpkg_to_graph(gpkg_path)\n",
    "# G_biketrack.remove_nodes_from(list(nx.isolates(G_biketrack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# import geopandas as gpd\n",
    "\n",
    "# # Assuming G_biketrack_edges and exit_points_3857 are already defined as GeoDataFrames\n",
    "\n",
    "# # Ensure both GeoDataFrames are in the same CRS (e.g., EPSG:4326 for folium compatibility)\n",
    "# if G_biketrack_edges.crs != 'EPSG:4326':\n",
    "#     G_biketrack_edges = G_biketrack_edges.to_crs('EPSG:4326')\n",
    "# if exit_points_3857.crs != 'EPSG:4326':\n",
    "#     exit_points_3857 = exit_points_3857.to_crs('EPSG:4326')\n",
    "\n",
    "# # Create a folium map centered on the first point of G_biketrack_edges\n",
    "# map_center = [G_biketrack_edges.geometry.centroid.y.iloc[0], G_biketrack_edges.geometry.centroid.x.iloc[0]]\n",
    "# m = folium.Map(location=map_center, zoom_start=14)\n",
    "\n",
    "# # Add G_biketrack_edges to the map as a line layer\n",
    "# for _, row in G_biketrack_edges.iterrows():\n",
    "#     folium.PolyLine(\n",
    "#         locations=[(point[1], point[0]) for point in row.geometry.coords],  # Swap lat/lon for folium\n",
    "#         color='blue',\n",
    "#         weight=2,\n",
    "#         opacity=0.7\n",
    "#     ).add_to(m)\n",
    "\n",
    "# # Add exit_points_3857 to the map as a point layer\n",
    "# for _, row in exit_points_3857.iterrows():\n",
    "#     folium.CircleMarker(\n",
    "#         location=(row.geometry.y, row.geometry.x),  # Swap lat/lon for folium\n",
    "#         radius=5,\n",
    "#         color='red',\n",
    "#         fill=True,\n",
    "#         fill_color='red',\n",
    "#         fill_opacity=0.7\n",
    "#     ).add_to(m)\n",
    "\n",
    "# # Display the map\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## investigate if exit points match with neighbourhoods?\n",
    "\n",
    "# # Set up a larger figure\n",
    "# fig, ax = plt.subplots(figsize=(20, 14))  # Adjust the width and height as needed\n",
    "\n",
    "\n",
    "# # Add G_weighted edges\n",
    "# G_weighted_nodes, G_weighted_edges = ox.graph_to_gdfs(G_weighted)\n",
    "# G_weighted_edges = G_weighted_edges.to_crs(epsg=3857)  # Ensure CRS matches\n",
    "# G_weighted_edges.plot(ax=ax, color='lightgrey', linewidth=0.5, alpha=0.8, zorder = 0)  # Light grey with thin linewidth\n",
    "\n",
    "# # Add bike track edges\n",
    "# G_biketrack_nodes, G_biketrack_edges = ox.graph_to_gdfs(G_biketrack)\n",
    "# G_biketrack_edges = G_biketrack_edges.to_crs(epsg=3857)\n",
    "# G_biketrack_edges.plot(ax=ax, color='turquoise', linewidth=0.5, alpha=0.8, zorder = 2)  # Light grey with thin linewidth\n",
    "\n",
    "\n",
    "# # Plot the main graph and layers\n",
    "# GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "# GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "# GT_edges.plot(ax=ax, color='orange', zorder = 1)\n",
    "\n",
    "# # plot exit points\n",
    "# exit_points_3857 = exit_points.to_crs(epsg=3857)\n",
    "# exit_points_3857.plot(ax=ax, color='red', markersize=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Remove x and y axis labels and ticks\n",
    "# ax.axis('off')  # This removes the entire axis, including labels and ticks\n",
    "\n",
    "# # Enhance plot aesthetics\n",
    "# ax.set_title(f\"Meters of investment: {D/10}\")\n",
    "# ax.legend(loc=\"upper left\")\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
