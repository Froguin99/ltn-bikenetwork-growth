{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Points of interest based bicycle network generation\n",
    "## Project: Growing Urban Bicycle Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the transit-oriented development approach of palominos2020ica or a grid approach and applies cardillo2006spp: Take the greedy triangulation between railway/underground stations (or other points of interest created in 02_prepare_pois). This is the cold start bicycle network generation process which creates bicycle networks from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- ltns un-prioritised growth to compare against ltn prioritsed  X DONE\n",
    "- clean triangulation for concave shapes X DONE\n",
    "- methods plotting for removing slivers etc\n",
    "- cycle network investment within LTNs, converting pedestrain streets so that you can cycle through. Could have a big impact on the lcc size X Done in script 03\n",
    "- can't take multiple places as an input (my bad coding skills...), need to be able to take several places at a time\n",
    "- save outputs as geopackages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from src import utils\n",
    "PATH = utils.PATH # shortening the var name so that we don't have to change it below\n",
    "\n",
    "# System\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import dill as pickle\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from copy import deepcopy\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "# Math/Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Network\n",
    "import networkx as nx\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation # for gifs\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "\n",
    "# Geo\n",
    "import osmnx as ox\n",
    "ox.settings.log_file = True\n",
    "ox.settings.requests_timeout = 300\n",
    "ox.settings.logs_folder = PATH[\"logs\"]\n",
    "import fiona\n",
    "import shapely\n",
    "from haversine import haversine\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import momepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "gifs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yaml.load(\n",
    "    open(\"../parameters/parameters.yml\"), \n",
    "    Loader=yaml.FullLoader)\n",
    "osmnxparameters = json.load(open(\"../parameters/osmnxparameters.json\", \"r\"))\n",
    "plotparam = json.load(open(\"../parameters/plotparam.json\", \"r\"))\n",
    "plotparam_analysis = json.load(open(\"../parameters/plotparam_analysis.json\", \"r\"))\n",
    "\n",
    "# load cities\n",
    "cities = utils.load_cities(PATH, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network weighting tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_lts = json.load(open(\"../parameters/tag_lts.json\", \"r\"))\n",
    "distance_cost = json.load(open(\"../parameters/distance_cost.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function and code below currently routes between the edges of neighbourhoods, rather than from a single point to a single point. We then join the neighbourhoods up first, before considering the wider area. This wider area is derived from hexagonal tesslleations within the city boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "def csv_to_ox(p, placeid, parameterid):\n",
    "    '''\n",
    "    Load graph from csv files (nodes and edge)\n",
    "    Include OSMID, length, highway, x, y attributes\n",
    "    '''\n",
    "\n",
    "    prefix = placeid + '_' + parameterid\n",
    "    compress = check_extract_zip(p, prefix)\n",
    "    \n",
    "    with open(p + prefix + '_edges.csv', 'r') as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "        lines = []\n",
    "        for line in csv.reader(f, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):\n",
    "            line_list = [c for c in line]\n",
    "            osmid = str(eval(line_list[header.index(\"osmid\")])[0]) if isinstance(eval(line_list[header.index(\"osmid\")]), list) else line_list[header.index(\"osmid\")]\n",
    "            length = str(eval(line_list[header.index(\"length\")])[0]) if isinstance(eval(line_list[header.index(\"length\")]), list) else line_list[header.index(\"length\")]\n",
    "            highway = line_list[header.index(\"highway\")]\n",
    "            if highway.startswith(\"[\") and highway.endswith(\"]\"):\n",
    "                highway = highway.strip(\"[]\").split(\",\")[0].strip(\" '\")\n",
    "            line_string = f\"{line_list[header.index('u')]} {line_list[header.index('v')]} {osmid} {length} {highway}\"\n",
    "            lines.append(line_string)\n",
    "        G = nx.parse_edgelist(lines, nodetype=int, data=((\"osmid\", int), (\"length\", float), (\"highway\", str)), create_using=nx.MultiDiGraph)\n",
    "    \n",
    "    with open(p + prefix + '_nodes.csv', 'r') as f:\n",
    "        header = f.readline().strip().split(\",\")\n",
    "        values_x = {}\n",
    "        values_y = {}\n",
    "        for line in csv.reader(f, quotechar='\"', delimiter=',', quoting=csv.QUOTE_ALL, skipinitialspace=True):\n",
    "            line_list = [c for c in line]\n",
    "            osmid = int(line_list[header.index(\"osmid\")])\n",
    "            values_x[osmid] = float(line_list[header.index(\"x\")])\n",
    "            values_y[osmid] = float(line_list[header.index(\"y\")])\n",
    "        nx.set_node_attributes(G, values_x, \"x\")\n",
    "        nx.set_node_attributes(G, values_y, \"y\")\n",
    "    \n",
    "    if compress:\n",
    "        os.remove(p + prefix + '_nodes.csv')\n",
    "        os.remove(p + prefix + '_edges.csv')\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G_caralls - no addtional weighting\n",
    "\n",
    "G_weighted - for routing. lts applied + zero bike infrastucutre cost\n",
    "\n",
    "G_investment distance - for finding distances between points in abstract (greedy_gdf) graph. cycle infrastucre has no cost, everything else has default distance\n",
    "\n",
    "greedy_gdf[distance] - the investment length (routed length - any infrastucture)\n",
    "\n",
    "GT \"length\" - actual distance between points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data structures\n",
    "locations = {}\n",
    "G_caralls = {}\n",
    "G_caralls_simplified = {}\n",
    "G_weighteds = {}\n",
    "G_investment_distances = {}\n",
    "G_defaults = {}\n",
    "G_biketracks = {}\n",
    "G_biketracks_no_ltn = {}\n",
    "ltn_nodes_dict = {}\n",
    "tess_nodes_dict = {}\n",
    "combined_nodes_dict = {}\n",
    "all_centroids_dict = {}\n",
    "edge_betweenness_dict = {}\n",
    "ebc_ltn_dict = {}\n",
    "ebc_other_dict = {}\n",
    "ebc_all_dict = {}\n",
    "shortest_paths_ltn_pairs_dict = {}\n",
    "shortest_paths_other_pairs_dict = {}\n",
    "shortest_paths_all_pairs_dict = {}\n",
    "shortest_paths_ltn_dict = {}\n",
    "shortest_paths_other_dict = {}\n",
    "shortest_paths_all_dict = {}\n",
    "combined_node_pairs_dict = {}\n",
    "greedy_gdfs_dict = {}\n",
    "tess_gdfs_dict = {}\n",
    "ltn_gdfs_dict = {}\n",
    "greedy_triangulation_ltns_dict = {}\n",
    "ltn_node_pairs_dict = {}\n",
    "exit_points_dict = {}\n",
    "greedy_nx_dict = {}\n",
    "\n",
    "parameterinfo = osmnxparameters['carall']\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid in cities.keys():\n",
    "        print(f\"Processing: {placeid} - {scenario}\")\n",
    "\n",
    "        data_path = os.path.join(PATH[\"data\"], placeid, scenario)\n",
    "\n",
    "        # Load road graph\n",
    "        G_carall = utils.csv_to_ox(data_path + \"/\", placeid, 'biketrackcarall')\n",
    "        G_carall.graph[\"crs\"] = 'epsg:4326'  # Needed for compatibility with GeoDataFrames\n",
    "        G_caralls[(placeid, scenario)] = G_carall\n",
    "\n",
    "        # Make graph copies for different weighting strategies\n",
    "        G_weighted = copy.deepcopy(G_carall)\n",
    "        G_investment_distance = copy.deepcopy(G_carall)\n",
    "        G_default = copy.deepcopy(G_carall)\n",
    "\n",
    "        # Apply LTS weighting\n",
    "        for u, v, key, data in G_weighted.edges(keys=True, data=True):\n",
    "            highway = data.get('highway')\n",
    "            lts = tag_lts.get(highway, 1)\n",
    "            G_weighted[u][v][key]['length'] *= lts\n",
    "\n",
    "        # Apply investment cost weighting\n",
    "        for u, v, key, data in G_investment_distance.edges(keys=True, data=True):\n",
    "            highway = data.get('highway')\n",
    "            distance = distance_cost.get(highway, 1)\n",
    "            G_investment_distance[u][v][key]['length'] *= distance\n",
    "\n",
    "        # Convert to undirected\n",
    "        G_weighteds[(placeid, scenario)] = G_weighted.to_undirected()\n",
    "        G_investment_distances[(placeid, scenario)] = G_investment_distance.to_undirected()\n",
    "        G_defaults[(placeid, scenario)] = G_default.to_undirected()\n",
    "\n",
    "        # Load biketrack graph\n",
    "        gpkg_path = os.path.join(data_path, f\"{placeid}_biketrack.gpkg\")\n",
    "        G_bike = utils.ox_gpkg_to_graph(gpkg_path)\n",
    "        G_bike.remove_nodes_from(list(nx.isolates(G_bike)))\n",
    "        G_biketracks[(placeid, scenario)] = G_bike\n",
    "\n",
    "        # Load biketrack graph without LTNs\n",
    "        gpkg_path_no_ltn = os.path.join(data_path, f\"{placeid}_biketrack_no_ltn.gpkg\")\n",
    "        G_bike_no_ltn = utils.ox_gpkg_to_graph(gpkg_path_no_ltn)\n",
    "        G_bike_no_ltn.remove_nodes_from(list(nx.isolates(G_bike_no_ltn)))\n",
    "        G_biketracks_no_ltn[(placeid, scenario)] = G_bike_no_ltn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid, placeinfo in tqdm(cities.items(), desc=f\"Cities ({scenario})\"):\n",
    "        print(f\"Processing: {placeid} - {scenario}\")\n",
    "\n",
    "        # Load Tesselation POIs (hard coded for now)\n",
    "        with open(os.path.join(PATH[\"data\"], placeid, scenario, f\"{placeid}_poi_tessellation_nnidsbikeall.csv\")) as f:\n",
    "            tessellation_nnids = [int(line.rstrip()) for line in f]\n",
    "\n",
    "        # Load LTN POIs\n",
    "        if placeinfo[\"nominatimstring\"] != '':\n",
    "            location = ox.geocoder.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "            if location.geometry[0].geom_type == 'MultiPolygon':\n",
    "                location = location.explode(index_parts=False).reset_index(drop=True)\n",
    "            location = utils.fill_holes(utils.extract_relevant_polygon(placeid, shapely.geometry.shape(location['geometry'][0])))\n",
    "        else:\n",
    "            # https://gis.stackexchange.com/questions/113799/how-to-read-a-shapefile-in-python\n",
    "            shp = fiona.open(os.path.join(PATH[\"data\"], placeid, scenario, f\"{placeid}.shp\"))\n",
    "            first = next(iter(shp))\n",
    "            try:\n",
    "                location = Polygon(shapely.geometry.shape(first['geometry']))  # If shape file is given as linestring\n",
    "            except:\n",
    "                location = shapely.geometry.shape(first['geometry'])\n",
    "\n",
    "        locations[(placeid, scenario)] = location\n",
    "\n",
    "        G_caralls[(placeid, scenario)] = utils.csv_to_ox_highway(PATH[\"data\"] + placeid + \"/\" + scenario + \"/\", placeid, 'biketrackcarall')\n",
    "        G_caralls[(placeid, scenario)].graph[\"crs\"] = 'epsg:4326'  # Needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "        G_caralls_simplified[(placeid, scenario)] = utils.csv_to_ox(PATH[\"data\"] + placeid + \"/\" + scenario + \"/\", placeid, 'biketrackcarall_simplified')\n",
    "        G_caralls_simplified[(placeid, scenario)].graph[\"crs\"] = 'epsg:4326'  # Needed for OSMNX's graph_to_gdfs in utils_graph.py\n",
    "\n",
    "        print(f\"{placeid}: Loading and moving POIs\")\n",
    "        # Get the carall graph and location geometry\n",
    "        location = locations[(placeid, scenario)]\n",
    "        G_carall = G_caralls_simplified[(placeid, scenario)]\n",
    "\n",
    "        # Load neighbourhoods and create GeoDataFrame for centroids\n",
    "        neighbourhoods = utils.load_neighbourhoods(os.path.join(PATH[\"data\"], placeid, scenario))\n",
    "        all_centroids = gpd.GeoDataFrame(columns=['neighbourhood_id', 'geometry'], crs='EPSG:4326')  \n",
    "\n",
    "        # load tesselation points \n",
    "        tess_nodes = gpd.read_file(os.path.join(PATH[\"data\"], placeid, scenario, f\"{placeid}_poi_tessellation.gpkg\"))\n",
    "        # load ltns and get exit points of ltns\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            exit_points = utils.get_exit_nodes(neighbourhoods, G_biketracks[(placeid, scenario)])\n",
    "            exit_points_dict[(placeid, scenario)] = exit_points.copy()\n",
    "            if params[\"export\"]:\n",
    "                file_path = os.path.join(PATH[\"exports_gpkg\"], placeid, scenario, f\"{placeid}_exit_points.gpkg\")\n",
    "                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "                exit_points.to_file(file_path, driver=\"GPKG\")\n",
    "\n",
    "\n",
    "        unique_id = 0  # Counter for unique IDs across neighbourhoods\n",
    "        for name, gdf in neighbourhoods.items():  # Process each neighbourhood GeoDataFrame to get centroids, exit points, and neighbourhood IDs\n",
    "            if gdf.empty:\n",
    "                print(f\"Warning: The GeoDataFrame for {name} is empty. Skipping...\")\n",
    "                continue\n",
    "            print(f\"Processing neighbourhoods in: {name}\")\n",
    "\n",
    "            # Assign a unique ID to each neighbourhood in the GeoDataFrame to reference throughout\n",
    "            gdf['neighbourhood_id'] = range(unique_id, unique_id + len(gdf))\n",
    "            if debug:\n",
    "                print(f\"Assigned neighbourhood_ids from {unique_id} to {unique_id + len(gdf) - 1} for {name}\")\n",
    "\n",
    "            # Get centroids to inherit 'neighbourhood_id'\n",
    "            centroids_gdf = utils.get_neighbourhood_centroids(gdf)\n",
    "            all_centroids = pd.concat([all_centroids, centroids_gdf], ignore_index=True)\n",
    "            unique_id += len(gdf)  # Increment by the number of neighbourhoods processed\n",
    "\n",
    "        # Snap centroids to the closest nodes in the street network\n",
    "        neighbourhood_nnids = set()\n",
    "        for g in all_centroids['geometry']:\n",
    "            n = ox.distance.nearest_nodes(G_carall, g.x, g.y)\n",
    "            if n not in neighbourhood_nnids and haversine((g.y, g.x), (G_carall.nodes[n][\"y\"], G_carall.nodes[n][\"x\"]), unit=\"m\") <= params[\"snapthreshold\"]:\n",
    "                neighbourhood_nnids.add(n)\n",
    "        # Add nearest_node column to all_centroids by finding the nearest node for each centroid geometry\n",
    "        all_centroids['nearest_node'] = all_centroids['geometry'].apply(\n",
    "            lambda g: ox.distance.nearest_nodes(G_carall, g.x, g.y))  # We now have all_centroids with 'neighbourhood_id', 'geometry', 'nearest_node' columns\n",
    "        all_centroids['osmid'] = all_centroids['nearest_node']\n",
    "        ltn_nodes = all_centroids\n",
    "        all_centroids_dict[(placeid, scenario)] = all_centroids.copy()\n",
    "\n",
    "        # add nearest node ID from G_carall \n",
    "        tess_nn = utils.get_nearest_nodes_to_gdf(G_carall, tess_nodes)\n",
    "        tess_nodes['osmid'] = tess_nn\n",
    "        tess_nodes_dict[(placeid, scenario)] = tess_nodes\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            ltn_nodes = all_centroids\n",
    "            ltn_nn = utils.get_nearest_nodes_to_gdf(G_carall, ltn_nodes)\n",
    "            ltn_nodes['osmid'] = ltn_nn\n",
    "            combined_nodes = pd.concat([tess_nodes, ltn_nodes], ignore_index=True)\n",
    "            combined_nodes_dict[(placeid, scenario)] = combined_nodes\n",
    "            ltn_nodes_dict[(placeid, scenario)] = ltn_nodes\n",
    "        else:\n",
    "            combined_nodes = tess_nodes\n",
    "            combined_nodes_dict[(placeid, scenario)] = tess_nodes\n",
    "\n",
    "        # save them\n",
    "        out_dir = os.path.join(PATH[\"data\"], placeid, scenario)\n",
    "        tess_nodes.to_file(os.path.join(out_dir, f\"{placeid}_tess_points.gpkg\"), driver=\"GPKG\")\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            ltn_nodes.to_file(os.path.join(out_dir, f\"{placeid}_ltn_points.gpkg\"), driver=\"GPKG\")\n",
    "        combined_nodes.to_file(os.path.join(out_dir, f\"{placeid}_combined_points.gpkg\"), driver=\"GPKG\")\n",
    "        print(f\"Saved tessellation and LTN points for {placeid} in {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create triangulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LTN triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce triangulation for LTN nodes\n",
    "# only needed to set up full trianuglation, not needed later.\n",
    "# ltn nodes are not ordered (nor do they need to be at this stage)\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    if scenario == \"no_ltn_scenario\":\n",
    "        print(f\"Skipping triangulation for {scenario} as it does not contain LTNs.\")\n",
    "        continue  # Skip triangulation for 'no_ltn_scenario'\n",
    "\n",
    "    for placeid in cities:\n",
    "        print(f\"Triangulating LTN nodes for {placeid} ({scenario})\")\n",
    "        ltn_nodes = ltn_nodes_dict[(placeid, scenario)]  \n",
    "        greedy_triangulation_ltns_gdf = utils.greedy_triangulation_ltns(ltn_nodes)\n",
    "        ltn_node_pairs = utils.get_ltn_node_pairs(ltn_nodes, greedy_triangulation_ltns_gdf)\n",
    "        greedy_triangulation_ltns_dict[(placeid, scenario)] = greedy_triangulation_ltns_gdf\n",
    "        ltn_node_pairs_dict[(placeid, scenario)] = ltn_node_pairs\n",
    "\n",
    "\n",
    "        if params[\"export\"]:\n",
    "            file_path = os.path.join(PATH[\"exports_gpkg\"], placeid, scenario, f\"{placeid}_greedy_triangulation_ltns.gpkg\")\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            greedy_triangulation_ltns_gdf.to_file(file_path, driver=\"GPKG\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build full triangulation using greedy triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    if scenario == \"no_ltn_scenario\":\n",
    "        tess_nodes = tess_nodes_dict[(placeid, scenario)]\n",
    "        greedy_gdf, tess_gdf = utils.build_greedy_triangulation_no_ltns(tess_nodes)\n",
    "        max_length = greedy_gdf['distance'].sum()\n",
    "        greedy_gdfs_dict[(placeid, scenario)] = greedy_gdf\n",
    "        tess_gdfs_dict[(placeid, scenario)] = tess_gdf\n",
    "        continue\n",
    "    ltn_nodes = ltn_nodes_dict[(placeid, scenario)]\n",
    "    tess_nodes = tess_nodes_dict[(placeid, scenario)]\n",
    "\n",
    "    greedy_gdf, ltn_gdf, tess_gdf = utils.build_greedy_triangulation(ltn_nodes, tess_nodes)\n",
    "    max_length = greedy_gdf['distance'].sum()\n",
    "    greedy_gdfs_dict[(placeid, scenario)] = greedy_gdf\n",
    "    tess_gdfs_dict[(placeid, scenario)] = tess_gdf\n",
    "    ltn_gdfs_dict[(placeid, scenario)] = ltn_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up greedy triangulation to remove silver triangles, and unrealisticly long links.\n",
    "- Removing silvers removes links which would pass along the same route (thus essentially duplicating themselves)\n",
    "- Links longer than 5km are not realistic links to be making, they almost always link rural to rural over routes which can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid in cities:\n",
    "        greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]  \n",
    "        # find very long links\n",
    "        greedy_gdf = greedy_gdf[greedy_gdf[\"distance\"] <= 5000]\n",
    "        # find slivers and remove the longest edge from them\n",
    "        fas = momepy.FaceArtifacts(greedy_gdf) \n",
    "        # run FaceArtifacts (cf https://docs.momepy.org/en/stable/api/momepy.FaceArtifacts.html)\n",
    "        fas.polygons = fas.polygons.set_crs(greedy_gdf.crs)\n",
    "        threshold = 10\n",
    "        polys = fas.polygons.copy()\n",
    "        slivers = polys[polys['face_artifact_index'] < threshold]\n",
    "        slivers[\"centroid\"] = slivers.geometry.centroid\n",
    "        centroids = slivers[\"centroid\"]\n",
    "        bounding = utils.find_bounding_lines(centroids, greedy_gdf)\n",
    "        longest_lines = []\n",
    "        for pt_idx, line_idxs in bounding.items():\n",
    "            if not line_idxs:\n",
    "                longest_lines.append(None)  \n",
    "                continue\n",
    "            lines = greedy_gdf.loc[line_idxs]\n",
    "            # Find the longest line\n",
    "            max_line = lines.loc[lines[\"distance\"].idxmax()]\n",
    "            longest_lines.append(max_line.name)\n",
    "        lines_to_drop = [idx for idx in longest_lines if idx is not None]\n",
    "        greedy_gdf = greedy_gdf.drop(index=lines_to_drop)\n",
    "        # Save back the cleaned graph\n",
    "        greedy_gdfs_dict[(placeid, scenario)] = greedy_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"methods_plotting\"]:\n",
    "    greedy_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Full Triangulation (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get node pairs of ltn-to-tess, tess-to-ltn, and tess-to-tess\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    if scenario == \"no_ltn_scenario\":\n",
    "        combined_nodes = combined_nodes_dict[(placeid, scenario)]\n",
    "        greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "        combined_node_pairs = utils.get_node_pairs_no_ltn(combined_nodes, greedy_gdf)\n",
    "        combined_node_pairs_dict[(placeid, scenario)] = combined_node_pairs\n",
    "        continue  # Skip non-LTN scenarios\n",
    "    for placeid in cities:\n",
    "        combined_nodes = combined_nodes_dict[(placeid, scenario)]\n",
    "        greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "        ltn_node_pairs = ltn_node_pairs_dict[(placeid, scenario)]\n",
    "        # get node pairs of ltn-to-tess, tess-to-ltn, and tess-to-tess\n",
    "        combined_node_pairs = utils.get_node_pairs(combined_nodes, greedy_gdf)\n",
    "        # remove ltn-ltn pairs \n",
    "        combined_node_pairs = [pair for pair in combined_node_pairs if pair not in ltn_node_pairs]\n",
    "        combined_node_pairs_dict[(placeid, scenario)] = combined_node_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Level of Traffic Stress to triangulation distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid in cities:\n",
    "        greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "        G_weighted = G_weighteds[(placeid, scenario)]\n",
    "        G_default = G_defaults[(placeid, scenario)]\n",
    "        greedy_gdf['sp_lts_route'] = greedy_gdf.apply(lambda row: nx.shortest_path(G_weighted, source=row['start_osmid'], target=row['end_osmid'], weight='length'),axis=1)\n",
    "        greedy_gdf['sp_lts_distance'] = greedy_gdf['sp_lts_route'].apply(lambda route: sum(G_weighted[u][v][0]['length'] for u, v in zip(route[:-1], route[1:])))\n",
    "        # find normal network distance for directness analysis\n",
    "        #G_default = copy.deepcopy(G_weighted) ## remove, G_weighted has lengths as zero so cannot be deweighted\n",
    "        #deweight_edges(G_default, tag_lts)\n",
    "        greedy_gdf['sp_true_distance'] = greedy_gdf['sp_lts_route'].apply(lambda route: sum(G_default[u][v][0]['length'] for u, v in zip(route[:-1], route[1:])))\n",
    "        greedy_gdf['eucl_dist'] = greedy_gdf['distance']  # save for directness analysis\n",
    "        greedy_gdfs_dict[(placeid, scenario)] = greedy_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid in cities:\n",
    "        greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "        # Create the graph from the triangulation GeoDataFrame\n",
    "        greedy_nx = nx.Graph()\n",
    "        for _, row in greedy_gdf.iterrows():\n",
    "            start = row['start_osmid']\n",
    "            end = row['end_osmid']\n",
    "            sp_lts_distance = row['sp_lts_distance']\n",
    "            greedy_nx.add_edge(start, end, geometry=row['geometry'], sp_lts_distance=sp_lts_distance)\n",
    "        greedy_nx_dict[(placeid, scenario)] = greedy_nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid in cities:\n",
    "        greedy_nx = greedy_nx_dict[(placeid, scenario)]\n",
    "        combined_node_pairs = combined_node_pairs_dict[(placeid, scenario)]\n",
    "\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            # for ltn prioirty \n",
    "            ltn_node_pairs = ltn_node_pairs_dict[(placeid, scenario)]\n",
    "\n",
    "            shortest_paths_ltn = []\n",
    "            shortest_paths_other = []\n",
    "            for node1, node2 in ltn_node_pairs:\n",
    "                try:\n",
    "                    path = nx.shortest_path(greedy_nx, source=node1, target=node2, weight='sp_lts_distance')\n",
    "                    shortest_paths_ltn.append(path)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    print(f\"No path between {node1} and {node2}\")\n",
    "            for node1, node2 in combined_node_pairs:\n",
    "                try:\n",
    "                    path = nx.shortest_path(greedy_nx, source=node1, target=node2, weight='sp_lts_distance')\n",
    "                    shortest_paths_other.append(path)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    print(f\"No path between {node1} and {node2}\")\n",
    "            shortest_paths_ltn_dict[(placeid, scenario)] = shortest_paths_ltn\n",
    "            shortest_paths_other_dict[(placeid, scenario)] = shortest_paths_other\n",
    "            \n",
    "\n",
    "        # for non-ltn priority\n",
    "        shortest_paths_all = []\n",
    "        for node1, node2 in combined_node_pairs:\n",
    "            try:\n",
    "                path = nx.shortest_path(greedy_nx, source=node1, target=node2, weight='sp_lts_distance')\n",
    "                shortest_paths_all.append(path)\n",
    "            except nx.NetworkXNoPath:\n",
    "                print(f\"No path between {node1} and {node2}\")\n",
    "        shortest_paths_all_dict[(placeid, scenario)] = shortest_paths_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find path values for ranking: Get order using sum of { (ebc of edge * (length of edge/sum of length of edges) ) } / number-of-edges.\n",
    "\n",
    "Distance used in these functions is the routed lts distance in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate edge betweenness centrality\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid in cities:\n",
    "        greedy_nx = greedy_nx_dict[(placeid, scenario)]\n",
    "        combined_node_pairs = combined_node_pairs_dict[(placeid, scenario)]\n",
    "        shortest_paths_all = shortest_paths_all_dict[(placeid, scenario)]\n",
    "\n",
    "        edge_betweenness = nx.edge_betweenness_centrality(greedy_nx, weight='sp_lts_distance')\n",
    "        edge_betweenness_dict[(placeid, scenario)] = edge_betweenness\n",
    "\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            ltn_node_pairs = ltn_node_pairs_dict[(placeid, scenario)]\n",
    "            shortest_paths_ltn = shortest_paths_ltn_dict[(placeid, scenario)]\n",
    "            shortest_paths_other = shortest_paths_other_dict[(placeid, scenario)]\n",
    "\n",
    "            # find path values\n",
    "            ebc_ltn = utils.get_sp_ebc_weights(ltn_node_pairs, shortest_paths_ltn, greedy_nx, edge_betweenness)\n",
    "            ebc_other = utils.get_sp_ebc_weights(combined_node_pairs, shortest_paths_other, greedy_nx, edge_betweenness)\n",
    "\n",
    "            # order by betweenness path value\n",
    "            ebc_ltn = dict(sorted(ebc_ltn.items(), key=lambda item: item[1], reverse=True))\n",
    "            ebc_other = dict(sorted(ebc_other.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "            # set up shortest paths for looping through in budget adjustment\n",
    "            shortest_paths_ltn = {\n",
    "                (path[0], path[-1]): [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "                for path in shortest_paths_ltn\n",
    "            }\n",
    "            shortest_paths_other = {\n",
    "                (path[0], path[-1]): [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "                for path in shortest_paths_other\n",
    "            }\n",
    "\n",
    "            ebc_ltn_dict[(placeid, scenario)] = ebc_ltn\n",
    "            ebc_other_dict[(placeid, scenario)] = ebc_other\n",
    "            shortest_paths_ltn_pairs_dict[(placeid, scenario)] = shortest_paths_ltn\n",
    "            shortest_paths_other_pairs_dict[(placeid, scenario)] = shortest_paths_other\n",
    "\n",
    "        # for all paths (non-ltn priority)\n",
    "        ebc_all = utils.get_sp_ebc_weights(combined_node_pairs, shortest_paths_all, greedy_nx, edge_betweenness)\n",
    "        ebc_all = dict(sorted(ebc_all.items(), key=lambda item: item[1], reverse=True))\n",
    "        shortest_paths_all = {\n",
    "            (path[0], path[-1]): [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "            for path in shortest_paths_all\n",
    "        }\n",
    "\n",
    "        ebc_all_dict[(placeid, scenario)] = ebc_all\n",
    "        shortest_paths_all_pairs_dict[(placeid, scenario)] = shortest_paths_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute routed edge lengths for abstract graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures we use our \"budget\" correctly. We first find the optimal route using the LTS tags per connection, then \"deweight\" the path to find the \"true\" length of the connection in meters. Existing infrastucture (bike paths etc) are not included in the \"true\" length, as we do not need to build these facilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this cell can take up to 30 mins to run\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid in cities:\n",
    "        print(f\"Computing shortest paths for {placeid} ({scenario})\")\n",
    "        greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "        G_weighted = G_weighteds[(placeid, scenario)]\n",
    "        G_investment_distance = G_defaults[(placeid, scenario)]\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            all_centroids = all_centroids_dict[(placeid, scenario)]\n",
    "            exit_points = exit_points_dict[(placeid, scenario)]\n",
    "\n",
    "            ## find the routed distance between edges in abstract GT\n",
    "            centroid_osmids = set(all_centroids['osmid'])\n",
    "            greedy_gdf['ltn_origin'] = greedy_gdf['start_osmid'].isin(centroid_osmids)\n",
    "            greedy_gdf['ltn_destination'] = greedy_gdf['end_osmid'].isin(centroid_osmids)\n",
    "\n",
    "            # Build a mapping from centroid osmid to neighbourhood_id\n",
    "            osmid_to_neigh = dict(zip(all_centroids['osmid'], all_centroids['neighbourhood_id']))\n",
    "\n",
    "            # Build a mapping from neighbourhood_id to list of exit point osmids\n",
    "            neigh_to_exits = defaultdict(list)\n",
    "            for idx, row in exit_points.iterrows():\n",
    "                neigh_to_exits[row['neighbourhood_id']].append(row['osmid'])\n",
    "\n",
    "        else:\n",
    "            # Make these columns/empty mappings to avoid getting an error in the apply function\n",
    "            greedy_gdf['ltn_origin'] = False\n",
    "            greedy_gdf['ltn_destination'] = False\n",
    "            osmid_to_neigh = {}\n",
    "            neigh_to_exits = defaultdict(list)\n",
    "        \n",
    "        # Compute the shortest path and store in 'sp_route'\n",
    "        sp_routes = []\n",
    "        for idx, row in greedy_gdf.iterrows():\n",
    "            route = utils.compute_routed_path_for_GT(row, G_weighted, osmid_to_neigh, neigh_to_exits) # <-- this seems to be very slow!?\n",
    "            sp_routes.append(route)\n",
    "        greedy_gdf['sp_route'] = sp_routes\n",
    "        \n",
    "        # Find lengths of shortest paths on G_investment_distance\n",
    "        # greedy_gdf['distance'] = greedy_gdf['sp_route'].apply(lambda route: calculate_sp_route_distance(route, G_investment_distance))\n",
    "        distances = []\n",
    "        for route in greedy_gdf['sp_route']:\n",
    "            if route is not None:\n",
    "                distance = utils.calculate_sp_route_distance(route, G_investment_distance)\n",
    "            else:\n",
    "                distance = None\n",
    "            distances.append(distance)\n",
    "\n",
    "        del greedy_gdf['sp_route']\n",
    "        greedy_gdfs_dict[(placeid, scenario)] = greedy_gdf.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now \"Distance\" is the length of investment required to connect two locations via the best(shortest with weighting of LTS) path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    for placeid in cities:\n",
    "        if params[\"methods_plotting\"]:\n",
    "\n",
    "            print(f\"Plotting for {placeid} ({scenario})\")\n",
    "\n",
    "            # Setup\n",
    "            greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "            location = locations[(placeid, scenario)]\n",
    "            location_gdf = gpd.GeoDataFrame(geometry=gpd.GeoSeries([location]), crs=\"EPSG:4326\")\n",
    "\n",
    "            plot_path = PATH[\"plots\"] + placeid + \"/\" + scenario + \"/\"\n",
    "\n",
    "            #### PLOT EBC MAP ####\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            ax.axis('off')\n",
    "            location_gdf.plot(ax=ax, color='lightgrey', zorder=0)\n",
    "\n",
    "            tess_nodes_dict[(placeid, scenario)].plot(ax=ax, color='red', markersize=10, zorder=2)\n",
    "\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                ltn_nodes_dict[(placeid, scenario)].plot(ax=ax, color='red', markersize=10, zorder=3)\n",
    "\n",
    "            edge_betweenness = edge_betweenness_dict[(placeid, scenario)]\n",
    "            ebc_df = pd.DataFrame([\n",
    "                {'start': min(u, v), 'end': max(u, v), 'betw': w}\n",
    "                for (u, v), w in edge_betweenness.items()\n",
    "            ])\n",
    "\n",
    "            merged_gdf = (\n",
    "                greedy_gdf\n",
    "                .assign(\n",
    "                    start_norm=greedy_gdf[['start_osmid', 'end_osmid']].min(axis=1),\n",
    "                    end_norm=greedy_gdf[['start_osmid', 'end_osmid']].max(axis=1)\n",
    "                )\n",
    "                .merge(ebc_df, left_on=['start_norm', 'end_norm'], right_on=['start', 'end'], how='left')\n",
    "                .to_crs(location_gdf.crs)\n",
    "            )\n",
    "\n",
    "            merged_gdf.plot(\n",
    "                ax=ax, column='betw', legend=True, cmap='viridis', linewidth=2, zorder=1,\n",
    "                legend_kwds={'shrink': 0.6, 'label': \"Normalised Edge Betweenness Centrality\"}\n",
    "            )\n",
    "\n",
    "            plt.savefig(plot_path + \"greedy_tri_ebc_map.png\", dpi=600, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            #### PLOT DOUBLE GREEDY MAP ####\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            ax.axis('off')\n",
    "            location_gdf.plot(ax=ax, color='lightgrey', zorder=0)\n",
    "\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                ltn_nodes_dict[(placeid, scenario)].plot(ax=ax, color='darkorange', markersize=50, zorder=3)\n",
    "\n",
    "            greedy_gdf.to_crs(location_gdf.crs).plot(ax=ax, color='firebrick', linewidth=1, zorder=1)\n",
    "\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                greedy_triangulation_ltns_dict[(placeid, scenario)].to_crs(location_gdf.crs).plot(\n",
    "                    ax=ax, color='royalblue', linewidth=2, zorder=2\n",
    "                )\n",
    "\n",
    "            plt.savefig(plot_path + \"greedy_tri_neighbours_map.png\", dpi=600, bbox_inches='tight')\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"export\"]:\n",
    "    for scenario in params[\"scenarios\"]:\n",
    "        for placeid in cities:\n",
    "            print(f\"Exporting GPKG for {placeid} ({scenario})\")\n",
    "            gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "            ebc = edge_betweenness_dict[(placeid, scenario)]\n",
    "            ebc_df = pd.DataFrame([{'start': min(u, v), 'end': max(u, v), 'betw': w} for (u, v), w in ebc.items()])\n",
    "            gdf['start_norm'] = gdf[['start_osmid', 'end_osmid']].min(axis=1) # norm means normilsed direction\n",
    "            gdf['end_norm']   = gdf[['start_osmid', 'end_osmid']].max(axis=1)\n",
    "\n",
    "            merged = (\n",
    "                gdf.merge(ebc_df, left_on=['start_norm', 'end_norm'], right_on=['start', 'end'], how='left')\n",
    "                .to_crs(4326)\n",
    "            )\n",
    "            path = os.path.join(PATH[\"exports_gpkg\"], placeid, scenario)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            merged.to_file(os.path.join(path, f\"{placeid}_greedy_triangulation.gpkg\"), driver=\"GPKG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the size of a fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_levels_dict = {}\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    if scenario != \"current_ltn_scenario\":\n",
    "        continue  # Skip other scenarios\n",
    "    for placeid in cities:\n",
    "        gdf = greedy_gdfs_dict.get((placeid, scenario))\n",
    "        if gdf is None or 'distance' not in gdf.columns:\n",
    "            if debug:\n",
    "                print(f\"Skipping {placeid} ({scenario}): missing data.\")\n",
    "            continue\n",
    "        total_distance = gdf['distance'].sum()\n",
    "        if debug:\n",
    "            print(f\"{placeid} ({scenario}): {total_distance:.2f} meters\")\n",
    "        # Set investment max to a fully connected bicycle network and define 1% incremental steps\n",
    "        points = np.linspace(0, total_distance, 101)\n",
    "        investment_levels = points[1:].tolist()  # Drop 0 as we don't consider the empty network\n",
    "        investment_levels_dict[(placeid, scenario)] = investment_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute investment levels for other scenarios using same intervals\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    if scenario == \"current_ltn_scenario\":\n",
    "        continue  # Skip current_ltn_scenario here\n",
    "    for placeid in cities:\n",
    "        gdf = greedy_gdfs_dict.get((placeid, scenario))\n",
    "        if gdf is None or 'distance' not in gdf.columns:\n",
    "            if debug:\n",
    "                print(f\"Skipping {placeid} ({scenario}): missing data.\")\n",
    "            continue\n",
    "        total_distance = gdf['distance'].sum()\n",
    "        if debug:\n",
    "            print(f\"{placeid} ({scenario}): {total_distance:.2f} meters\")\n",
    "        current_levels = investment_levels_dict.get((placeid, \"current_ltn_scenario\"))\n",
    "        if current_levels is None:\n",
    "            if debug:\n",
    "                print(f\"Skipping {placeid} ({scenario}): missing current scenario for step size.\")\n",
    "            continue\n",
    "\n",
    "        step_size = current_levels[0]  # use investment step size from current_ltn_scenario\n",
    "        investment_levels = []\n",
    "        next_level = step_size\n",
    "        while next_level < total_distance:\n",
    "            investment_levels.append(next_level)\n",
    "            next_level += step_size\n",
    "        investment_levels.append(min(total_distance, next_level))  \n",
    "\n",
    "        investment_levels_dict[(placeid, scenario)] = investment_levels\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBC Growth - Set connection order (LTNs prioritised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy triangulation \"distance\" is the length of the shortest path distance between edges once routed on to the network, without including any length from existing infrastucture. \n",
    "\n",
    "This cell creates a growth order based on **betweeness centrality**. Priority is given to Low Traffic Neighbouhoods, only once LTNs have been connected are any other links added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create iterations of network growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## betwenness LTN priority\n",
    "## many combinations\n",
    "GTs_dict = {}\n",
    "GT_abstracts_dict = {}\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    if scenario == \"no_ltn_scenario\":\n",
    "        print(f\"Skipping scenario: {scenario}, got no LTNs\")\n",
    "        continue\n",
    "\n",
    "    previous_selected_edges = set()\n",
    "\n",
    "    GT_abstracts = []\n",
    "    GT_abstracts_gdf = []\n",
    "    GTs = []\n",
    "    GTs_gdf = []\n",
    "\n",
    "    global_processed_pairs = set()\n",
    "    cumulative_GT_indices = set()\n",
    "\n",
    "    G_weighted = G_weighteds[(placeid, scenario)]\n",
    "    G_carall = G_caralls[(placeid, scenario)]\n",
    "\n",
    "    greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "    shortest_paths_ltn_pairs = shortest_paths_ltn_pairs_dict[(placeid, scenario)]\n",
    "    ebc_ltn = ebc_ltn_dict[(placeid, scenario)]\n",
    "    shortest_paths_other_pairs = shortest_paths_other_pairs_dict[(placeid, scenario)]\n",
    "    ebc_other = ebc_other_dict[(placeid, scenario)]\n",
    "\n",
    "    all_centroids = all_centroids_dict[(placeid, scenario)]\n",
    "    tess_gdf = tess_gdfs_dict[(placeid, scenario)]\n",
    "    ltn_gdf = ltn_gdfs_dict[(placeid, scenario)]\n",
    "    exit_points = exit_points_dict[(placeid, scenario)]\n",
    "\n",
    "    if ltn_extra is not None and not ltn_extra.empty:\n",
    "        ltn_extra_edges = ltn_extra.reset_index()[['u', 'v']]\n",
    "        ltn_extra_edge_set = set(map(tuple, ltn_extra_edges.to_numpy()))\n",
    "        ltn_extra_edge_set |= {(v, u) for u, v in ltn_extra_edge_set}  # make undirected\n",
    "    else:\n",
    "        ltn_extra_edge_set = set()\n",
    "\n",
    "\n",
    "    for D in tqdm(investment_levels_dict[(placeid, scenario)], desc=f\"Pruning GT abstract and routing on network for meters of investment - {scenario}\"):\n",
    "        GT_abstract_gdf, previous_selected_edges, connected_ltn_pairs, connected_other_pairs = utils.adjust_triangulation_to_budget_ltn_priority(\n",
    "            greedy_gdf, D,\n",
    "            shortest_paths_ltn_pairs,\n",
    "            ebc_ltn,\n",
    "            shortest_paths_other_pairs,\n",
    "            ebc_other,\n",
    "            previous_selected_edges,\n",
    "            ltn_node_pairs_dict[(placeid, scenario)])\n",
    "\n",
    "        remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Remaining edges to add: {remaining_edges}\")\n",
    "\n",
    "        GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "        GT_abstract_nx = utils.gdf_to_nx_graph(GT_abstract_gdf)\n",
    "        GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "        poipairs = list(GT_abstract_nx.edges())\n",
    "        routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Routing {len(routenodepairs)} pairs for investment level {D}\")\n",
    "\n",
    "        GT_indices = set()\n",
    "        processed_pairs = set()\n",
    "        \n",
    "        ## conditional routing \n",
    "        # ltn --> ltn (all)\n",
    "        # ltn --> tess (all to one)\n",
    "        # tess --> tess (one to one)\n",
    "        # tess --> ltn (one to all)\n",
    "\n",
    "        for u, v in routenodepairs:\n",
    "            poipair = (u, v)\n",
    "            if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "                continue\n",
    "\n",
    "            is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "            is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "\n",
    "            if debug:\n",
    "                print(f\"[{scenario}] Routing pair {u} -> {v} | Neighbourhood status: u={is_u_neighbourhood}, v={is_v_neighbourhood}\")\n",
    "\n",
    "            best_path = None\n",
    "            shortest_path_length = float('inf')\n",
    "\n",
    "            if is_u_neighbourhood and is_v_neighbourhood:\n",
    "                neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "                neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "                exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"[{scenario}] Available exit points for u ({u}, neighbourhood {neighbourhood_a}): {list(exit_points_a)}\")\n",
    "                    print(f\"[{scenario}] Available exit points for v ({v}, neighbourhood {neighbourhood_b}): {list(exit_points_b)}\")\n",
    "\n",
    "                processed_pairs = set()\n",
    "                for ea in exit_points_a:\n",
    "                    for eb in exit_points_b:\n",
    "                        pair_id = tuple(sorted((ea, eb)))\n",
    "                        if pair_id in processed_pairs:\n",
    "                            continue\n",
    "                        processed_pairs.add(pair_id)\n",
    "                        try:\n",
    "                            sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                            sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "\n",
    "                            if sp_length < shortest_path_length:\n",
    "                                shortest_path_length = sp_length\n",
    "                                best_path = sp\n",
    "                        except nx.NetworkXNoPath:\n",
    "                            continue\n",
    "\n",
    "            elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "                neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"[{scenario}] Available exit points for u ({u}, neighbourhood {neighbourhood_id}): {list(exit_points_a)}\")\n",
    "\n",
    "                for ea in exit_points_a:\n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length = sp_length\n",
    "                            best_path = sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "\n",
    "            elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "                neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "                exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"[{scenario}] Available exit points for v ({v}, neighbourhood {neighbourhood_id}): {list(exit_points_b)}\")\n",
    "\n",
    "                for eb in exit_points_b:\n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length = sp_length\n",
    "                            best_path = sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                    shortest_path_length = nx.shortest_path_length(G_weighted, source=u, target=v, weight='length')\n",
    "                    best_path = sp\n",
    "                except nx.NetworkXNoPath:\n",
    "                    if debug:\n",
    "                        print(f\"[{scenario}] No path found between {u} and {v}\")\n",
    "\n",
    "            if best_path:\n",
    "                GT_indices.update(best_path)\n",
    "                if debug:\n",
    "                    print(f\"[{scenario}] Shortest path from {u} to {v} (length: {shortest_path_length:.2f} m): {best_path}\")\n",
    "    \n",
    "\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"[{scenario}] No path found between {u} and {v}\")\n",
    "\n",
    "            global_processed_pairs.add(poipair)\n",
    "\n",
    "        cumulative_GT_indices.update(GT_indices)\n",
    "\n",
    "        GT = G_carall.subgraph(cumulative_GT_indices)\n",
    "        for u, v, data in GT.edges(data=True):\n",
    "            if 'length' in data:\n",
    "                data['weight'] = data['length']\n",
    "\n",
    "        GTs.append(GT)\n",
    "        _, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GTs_gdf.append(GT_edges)\n",
    "\n",
    "        if debug:\n",
    "            ax = GT_edges.to_crs(epsg=3857).plot()\n",
    "            ltn_gdf.to_crs(epsg=3857).plot(ax=ax, color='red', markersize=10)\n",
    "            tess_gdf.to_crs(epsg=3857).plot(ax=ax, color='green', markersize=5)\n",
    "            ax.set_title(f\"[{scenario}] Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "            plt.show()\n",
    "\n",
    "    # save to plot gifs\n",
    "    GTs_dict[scenario] = GTs\n",
    "    GT_abstracts_dict[scenario] = GT_abstracts\n",
    "\n",
    "    # write results\n",
    "    prune_measure = \"betweenness_ltn_priority\"\n",
    "    results = {\n",
    "        \"placeid\": placeid,\n",
    "        \"prune_measure\": prune_measure,\n",
    "        \"poi_source\": params[\"poi_source\"],\n",
    "        \"prune_quantiles\": investment_levels_dict[(placeid, scenario)],\n",
    "        \"GTs\": GTs,\n",
    "        \"GT_abstracts\": GT_abstracts}\n",
    "\n",
    "    # Save files. Files are saved to a pickle. They will be of the form \"newcastle_poi_LTNs_tessellation_betweenness_weighted_current_ltn_scenario.pickle\". \n",
    "    utils.write_result(results, \"pickle\", placeid, params[\"poi_source\"], prune_measure, \".pickle\", weighting=params[\"weighting\"], scenario=scenario)\n",
    "\n",
    "\n",
    "if gifs:\n",
    "    # plot gifs\n",
    "    growth_metric = \"betweeness_ltn_priority\"  \n",
    "    for scenario in params[\"scenarios\"]:\n",
    "        if scenario == \"no_ltn_scenario\":\n",
    "            print(f\"Skipping GIF generation for scenario: {scenario}\")\n",
    "            continue\n",
    "\n",
    "        ltn_points = ltn_gdfs_dict.get((placeid, scenario))\n",
    "        tess_points = tess_gdfs_dict.get((placeid, scenario))\n",
    "        neighbourhoods = utils.load_neighbourhoods(os.path.join(PATH[\"data\"], placeid, scenario)) if scenario != \"no_ltn_scenario\" else None\n",
    "\n",
    "        for graph_type, graph_list in [(\"routed\", GTs_dict[scenario]), (\"abstract\", GT_abstracts_dict[scenario])]:\n",
    "            output_file = os.path.join(PATH[\"videos\"], placeid, scenario, f\"{placeid}_{scenario}_{graph_type}_{growth_metric}_growth.gif\")\n",
    "            title_prefix = f\"{scenario} - {graph_type.capitalize()} ({growth_metric.replace('_', ' ').capitalize()}) iteration\"\n",
    "            utils.plot_investment_growth_gifs(\n",
    "                graph_list=graph_list,\n",
    "                output_path=output_file,\n",
    "                G_biketrackcarall=G_caralls[(placeid, scenario)],\n",
    "                G_biketrack=G_biketracks[(placeid, scenario)],\n",
    "                ltn_points=ltn_points,\n",
    "                tess_points=tess_points,\n",
    "                neighbourhoods=neighbourhoods,\n",
    "                investment_levels=investment_levels_dict[(placeid, scenario)],\n",
    "                title_prefix=title_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Demand Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading PCT results - Predownloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in lines\n",
    "lines = gpd.read_file(PATH[\"example-data\"] +  \"/lines_lsoa.gpkg\")\n",
    "rnet = gpd.read_file(PATH[\"example-data\"] + \"/rnet_lsoa.gpkg\")\n",
    "lsoa = gpd.read_file(PATH[\"example-data\"] + \"/lsoa.gpkg\")\n",
    "lsoa_bound = gpd.read_file(PATH[\"example-data\"] + \"/lsoa_bound.gpkg\")\n",
    "\n",
    "# clip to boundary\n",
    "boundary = ox.geocode_to_gdf(placeinfo[\"nominatimstring\"])\n",
    "#lines = gpd.clip(lines, boundary) # dont clip, otherwise lines which pass temporarily outside the boundary will be removed\n",
    "rnet = gpd.clip(rnet, boundary)\n",
    "lsoa = gpd.clip(lsoa, boundary)\n",
    "lsoa_bound = gpd.clip(lsoa_bound, boundary)\n",
    "valid_lad11cds = lsoa['lad11cd'].unique()\n",
    "lines = lines[lines['lad11cd1'].isin(valid_lad11cds) & lines['lad11cd2'].isin(valid_lad11cds)] # do this instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"methods_plotting\"]:\n",
    "    for scenario in params[\"scenarios\"]:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.axis('off')\n",
    "        location = locations[(placeid, scenario)]\n",
    "        location_gseries = gpd.GeoSeries([location])\n",
    "        location_gdf = gpd.GeoDataFrame(geometry=location_gseries)\n",
    "        location_gdf.set_crs(epsg=4326, inplace=True)\n",
    "        location_gdf.plot(ax=ax, color='lightgrey', zorder=0)\n",
    "\n",
    "        # add pct lines\n",
    "        lines.plot(ax=ax, column=\"dutch_slc\", scheme='Percentiles', alpha=0.2)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(PATH[\"plots\"] + placeid + \"/\" + scenario + \"/pct_raw_demand.png\", dpi=600, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    for scenario in params[\"scenarios\"]:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.axis('off')\n",
    "        location = locations[(placeid, scenario)]\n",
    "        location_gseries = gpd.GeoSeries([location])\n",
    "        location_gdf = gpd.GeoDataFrame(geometry=location_gseries)\n",
    "        location_gdf.set_crs(epsg=4326, inplace=True)\n",
    "        location_gdf.plot(ax=ax, color='lightgrey', zorder=0)\n",
    "\n",
    "        # add pct lines\n",
    "        lines.plot(ax=ax, column=\"dutch_slc\", scheme='Percentiles', alpha=0.2, label=scenario)\n",
    "\n",
    "        tess_nodes = tess_nodes_dict[(placeid, scenario)]\n",
    "        tess_nodes.plot(ax=ax, color='darkorange', markersize=30, zorder=5)\n",
    "\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            ltn_nodes = ltn_nodes_dict[(placeid, scenario)]\n",
    "            ltn_nodes.plot(ax=ax, color='darkorange', markersize=30, zorder=4)\n",
    "\n",
    "            greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "            greedy_gdf.to_crs(lines.crs).plot(ax=ax, color='red', linewidth=0.9, zorder=3, label=scenario)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.savefig(PATH[\"plots\"] + placeid + \"/\" + scenario + \"/pct_greedy_setup.png\", dpi=600, bbox_inches='tight')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCT Demand to GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get better demand data, we can transform the desire lines from the PCT's potential demand into the ordering method of links in our greedy triangulation. This can be compared against using betweeness centraility, which up until now has been used as our proxy for demand.\n",
    "\n",
    "PCT desire lines go from LSOA centriods, which are at a different scale to our start and end points. To deal with this, we link each lsoa to its nearest seed point. We can then aggregate the demand between seed points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find nearest neighbours of seed points, then weight demand by inverse distance from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This joins demand to lines via a k-Nearest Neighbour approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    combined_nodes = combined_nodes_dict[(placeid, scenario)]\n",
    "    greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "\n",
    "    # double check crs\n",
    "    if lsoa.crs != combined_nodes.crs:\n",
    "        combined_nodes = combined_nodes.to_crs(lsoa.crs)\n",
    "    # Nearest seed point for each LSOA\n",
    "    join = gpd.sjoin_nearest(lsoa[['geometry']], combined_nodes[['osmid', 'geometry']], how='left')\n",
    "    lsoa['nearest_seed_point'] = join['osmid']\n",
    "\n",
    "    # Reproject to work in meters\n",
    "    lsoa = lsoa.to_crs(epsg=3857)\n",
    "    lines = lines.to_crs(epsg=3857)\n",
    "    combined_nodes = combined_nodes.to_crs(epsg=3857)\n",
    "    greedy_gdf = greedy_gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Convert geo_code columns to strings\n",
    "    lsoa['geo_code'] = lsoa['geo_code'].astype(str)\n",
    "    lines['geo_code1'] = lines['geo_code1'].astype(str)\n",
    "    lines['geo_code2'] = lines['geo_code2'].astype(str)\n",
    "\n",
    "    # Compute coordinates\n",
    "    lsoa['x'] = lsoa.geometry.x\n",
    "    lsoa['y'] = lsoa.geometry.y\n",
    "    combined_nodes['x'] = combined_nodes.geometry.x\n",
    "    combined_nodes['y'] = combined_nodes.geometry.y\n",
    "\n",
    "    # k-NN setup\n",
    "    k = 3  # CHANGE \n",
    "    combined_coords = combined_nodes[['x', 'y']].values\n",
    "    lsoa_coords = lsoa[['x', 'y']].values\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(combined_coords)\n",
    "    distances, indices = nbrs.kneighbors(lsoa_coords)\n",
    "\n",
    "    # Build mapping \n",
    "    mapping = {}\n",
    "    for i, row in enumerate(lsoa.itertuples()):\n",
    "        lsoa_code = row.geo_code\n",
    "        node_indices = indices[i]\n",
    "        dists = distances[i]\n",
    "        weights = 1 / (dists + 1e-6)  # Avoid division by zero\n",
    "        normalised_weights = weights / weights.sum()\n",
    "        node_ids = combined_nodes.iloc[node_indices]['osmid'].values\n",
    "        mapping[lsoa_code] = list(zip(node_ids, normalised_weights))\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[{scenario}] Mapping keys (first 10):\", list(mapping.keys())[:10])\n",
    "        print(f\"[{scenario}] Total LSOA mapping entries:\", len(mapping))\n",
    "\n",
    "    # Generate flow records from desire lines\n",
    "    flow_records = []\n",
    "    for idx, row in lines.iterrows():\n",
    "        origin_code = row['geo_code1']\n",
    "        dest_code = row['geo_code2']\n",
    "        demand = row['dutch_slc']\n",
    "\n",
    "        if debug:\n",
    "            if origin_code not in mapping:\n",
    "                print(f\"[{scenario}] Origin code {origin_code} not in mapping\")\n",
    "            if dest_code not in mapping:\n",
    "                print(f\"[{scenario}] Destination code {dest_code} not in mapping\")\n",
    "\n",
    "        if origin_code in mapping and dest_code in mapping:\n",
    "            origin_nodes = mapping[origin_code]\n",
    "            dest_nodes = mapping[dest_code]\n",
    "            total_weight_product = sum(w_o * w_d for (_, w_o) in origin_nodes for (_, w_d) in dest_nodes)\n",
    "            for o_node, w_o in origin_nodes:\n",
    "                for d_node, w_d in dest_nodes:\n",
    "                    flow_share = demand * (w_o * w_d) / total_weight_product\n",
    "                    node_pair = tuple(sorted((o_node, d_node)))\n",
    "                    flow_records.append({'osmid_pair': node_pair, 'flow': flow_share})\n",
    "\n",
    "    if debug:\n",
    "        print(f\"[{scenario}] Number of flow records:\", len(flow_records))\n",
    "\n",
    "    # Aggregate flows\n",
    "    flow_df = pd.DataFrame(flow_records)\n",
    "    if flow_df.empty:\n",
    "        print(f\"[{scenario}] No flow records were generated. Check mapping and geo_code consistency.\")\n",
    "    else:\n",
    "        total_flow = flow_df.groupby('osmid_pair')['flow'].sum().reset_index(name='total_flow')\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Total flow (head):\\n\", total_flow.head())\n",
    "\n",
    "        greedy_gdf = greedy_gdf.copy()\n",
    "        greedy_gdf['osmid_pair'] = greedy_gdf.apply(\n",
    "            lambda row: tuple(sorted((int(row['start_osmid']), int(row['end_osmid'])))), axis=1\n",
    "        )\n",
    "        greedy_gdf = greedy_gdf.merge(total_flow, on='osmid_pair', how='left')\n",
    "        # shouldn't happen, but just in case\n",
    "        if 'total_flow_y' in greedy_gdf.columns and debug:\n",
    "            greedy_gdf['total_flow'] = greedy_gdf['total_flow_y']\n",
    "        elif 'total_flow' in greedy_gdf.columns and debug:\n",
    "            pass  # already fine\n",
    "        else:\n",
    "            print(f\"[{scenario}] Warning: total_flow column not found after merge.\")\n",
    "        greedy_gdf['total_flow'] = greedy_gdf['total_flow'].fillna(0)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Merged greedy_gdf (head):\\n\", greedy_gdf[['osmid_pair', 'total_flow']].head())\n",
    "\n",
    "    # Reproject back to WGS84\n",
    "    lsoa = lsoa.to_crs(epsg=4326)\n",
    "    lines = lines.to_crs(epsg=4326)\n",
    "    combined_nodes = combined_nodes.to_crs(epsg=4326)\n",
    "    greedy_gdf = greedy_gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # Optionally store updated greedy_gdf\n",
    "    greedy_gdfs_dict[(placeid, scenario)] = greedy_gdf\n",
    "\n",
    "    if params[\"methods_plotting\"]:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.axis('off')\n",
    "        location = locations[(placeid, scenario)]\n",
    "        location_gseries = gpd.GeoSeries([location])\n",
    "        location_gdf = gpd.GeoDataFrame(geometry=location_gseries)\n",
    "        location_gdf = location_gdf.set_crs(epsg=4326, inplace=True)\n",
    "        location_gdf.plot(ax=ax, color='lightgrey', zorder=0)\n",
    "\n",
    "        if scenario != \"no_ltn_scenario\":\n",
    "            ltn_nodes = ltn_nodes_dict[(placeid, scenario)]\n",
    "            ltn_nodes.plot(ax=ax, color='darkorange', markersize=30, zorder=5)\n",
    "        tess_nodes = tess_nodes_dict[(placeid, scenario)]\n",
    "        tess_nodes.plot(ax=ax, color='darkorange', markersize=30, zorder=4)\n",
    "        greedy_gdf.to_crs(lines.crs).plot(ax=ax, linewidth=1.5, zorder=3, column='total_flow', cmap='viridis', alpha=0.8)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.savefig(PATH[\"plots\"] + placeid + \"/\" + scenario + \"/pct_greedy.png\", dpi=600,  bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demand Growth - Set Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_ltn_dict = {}\n",
    "demand_other_dict = {}\n",
    "demand_all_dict = {}\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    key = (placeid, scenario)\n",
    "\n",
    "    greedy_gdf = greedy_gdfs_dict[key]\n",
    "    greedy_nx = nx.Graph()\n",
    "\n",
    "    for _, row in greedy_gdf.iterrows():\n",
    "        start = row['start_osmid']\n",
    "        end = row['end_osmid']\n",
    "        total_flow = row['total_flow']\n",
    "        sp_lts_distance = row.get('sp_lts_distance', 0)\n",
    "\n",
    "        greedy_nx.add_edge(\n",
    "            start, end,\n",
    "            geometry=row['geometry'],\n",
    "            total_flow=total_flow,\n",
    "            sp_lts_distance=sp_lts_distance\n",
    "        )\n",
    "\n",
    "    greedy_nx_dict[key] = greedy_nx  # Save graph\n",
    "\n",
    "    # Prepare node pairs and shortest path dicts for this scenario\n",
    "    combined_pairs = list(map(tuple, combined_node_pairs_dict[key]))\n",
    "    sp_all = utils.make_sp_dict(shortest_paths_all_dict[key])\n",
    "\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        ltn_pairs = list(map(tuple, ltn_node_pairs_dict[key]))\n",
    "        sp_ltn = utils.make_sp_dict(shortest_paths_ltn_dict[key])\n",
    "        sp_other = utils.make_sp_dict(shortest_paths_other_dict[key])\n",
    "\n",
    "    # Compute demands\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        d_ltn = utils.get_sp_demand_weights(ltn_pairs, sp_ltn, greedy_nx)\n",
    "        d_other = utils.get_sp_demand_weights(combined_pairs, sp_other, greedy_nx)\n",
    "    d_all = utils.get_sp_demand_weights(combined_pairs, sp_all, greedy_nx)\n",
    "\n",
    "    # Sort and store results\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        demand_ltn_dict[key] = dict(sorted(d_ltn.items(), key=lambda x: x[1], reverse=True))\n",
    "        demand_other_dict[key] = dict(sorted(d_other.items(), key=lambda x: x[1], reverse=True))\n",
    "    demand_all_dict[key] = dict(sorted(d_all.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "\n",
    "    # Make sure CRS is updated\n",
    "    greedy_gdfs_dict[key] = greedy_gdf.to_crs(3857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demand Growth - Prune Greedy Triangulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates a growth order based on **potential demand**. Priority is given to Low Traffic Neighbouhoods, only once LTNs have been connected are any other links added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demand-based triangulation and routing for multiple scenarios\n",
    "GTs_dict = {}\n",
    "GT_abstracts_dict = {}\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    # Skip scenario without LTNs\n",
    "    if scenario == \"no_ltn_scenario\":\n",
    "        print(f\"Skipping scenario: {scenario}, no LTNs to route\")\n",
    "        continue\n",
    "\n",
    "    key = (placeid, scenario)\n",
    "    previous_selected_edges = set()\n",
    "\n",
    "    GT_abstracts = []\n",
    "    GT_abstracts_gdf = []\n",
    "    GTs = []\n",
    "    GTs_gdf = []\n",
    "\n",
    "    global_processed_pairs = set()\n",
    "    cumulative_GT_indices = set()\n",
    "\n",
    "    # retrieve demand and shortest-path edge lists for this scenario\n",
    "    greedy_gdf = greedy_gdfs_dict[key]\n",
    "    sp_ltn = utils.make_sp_edge_dict(shortest_paths_ltn_dict[key])\n",
    "    sp_other = utils.make_sp_edge_dict(shortest_paths_other_dict[key])\n",
    "    demand_ltn = demand_ltn_dict[key]\n",
    "    demand_other = demand_other_dict[key]\n",
    "    G_weighted = G_weighteds[key]\n",
    "    all_centroids = all_centroids_dict[(placeid, scenario)]\n",
    "    tess_gdf = tess_gdfs_dict[(placeid, scenario)]\n",
    "    ltn_gdf = ltn_gdfs_dict[(placeid, scenario)]\n",
    "    exit_points = exit_points_dict[(placeid, scenario)]\n",
    "\n",
    "\n",
    "    for D in tqdm(investment_levels_dict[(placeid, scenario)], desc=f\"Pruning GT abstract and routing on network for demand-based budget - {scenario}\"):\n",
    "        # abstract triangulation prioritized by demand\n",
    "        GT_abstract_gdf, previous_selected_edges, connected_ltn_pairs, connected_other_pairs = utils.adjust_triangulation_to_budget_ltn_priority(\n",
    "                greedy_gdf, D,\n",
    "                sp_ltn, demand_ltn,\n",
    "                sp_other, demand_other,\n",
    "                previous_selected_edges,\n",
    "                ltn_node_pairs_dict[key])\n",
    "\n",
    "        GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "        GT_abstract_nx = utils.gdf_to_nx_graph(GT_abstract_gdf)\n",
    "        GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "        if debug:\n",
    "            ax = GT_abstract_gdf.plot()\n",
    "            ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "            tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "            for idx, row in ltn_gdf.iterrows():\n",
    "                ax.annotate(\n",
    "                    text=str(row['osmid']),\n",
    "                    xy=(row.geometry.x, row.geometry.y),\n",
    "                    xytext=(3, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    fontsize=8,\n",
    "                    color=\"red\"\n",
    "                )\n",
    "\n",
    "        poipairs = list(GT_abstract_nx.edges())\n",
    "        routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Routing on network for investment level: {D} with routenodepairs\", routenodepairs)\n",
    "\n",
    "        GT_indices = set()\n",
    "        processed_pairs = set()\n",
    "\n",
    "        for u, v in routenodepairs:\n",
    "            poipair = (u, v)\n",
    "            if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "                continue\n",
    "\n",
    "            is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "            is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "\n",
    "            shortest_path_length = float('inf')\n",
    "            best_path = None\n",
    "\n",
    "            if is_u_neighbourhood and is_v_neighbourhood:\n",
    "                neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "                neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "                exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "                for ea in exit_points_a:\n",
    "                    for eb in exit_points_b:\n",
    "                        pair_id = tuple(sorted((ea, eb)))\n",
    "                        if pair_id in processed_pairs:\n",
    "                            continue\n",
    "                        processed_pairs.add(pair_id)\n",
    "                        try:\n",
    "                            sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                            sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                            if sp_length < shortest_path_length:\n",
    "                                shortest_path_length = sp_length\n",
    "                                best_path = sp\n",
    "                        except nx.NetworkXNoPath:\n",
    "                            continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "                neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "                for ea in exit_points_a:\n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length = sp_length\n",
    "                            best_path = sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "                neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "                exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "                for eb in exit_points_b:\n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length = sp_length\n",
    "                            best_path = sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                    GT_indices.update(sp)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "            global_processed_pairs.add(poipair)\n",
    "\n",
    "        cumulative_GT_indices.update(GT_indices)\n",
    "\n",
    "        GT = G_caralls[key].subgraph(cumulative_GT_indices)\n",
    "\n",
    "        for u, v, data in GT.edges(data=True):\n",
    "            if 'length' in data:\n",
    "                data['weight'] = data['length']\n",
    "\n",
    "        GTs.append(GT)\n",
    "        _, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GTs_gdf.append(GT_edges)\n",
    "\n",
    "        if debug:\n",
    "            GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "            GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "            ax = GT_edges.plot()\n",
    "            ltn_gdf = ltn_nodes_dict[(placeid, scenario)].to_crs(epsg=3857)\n",
    "            tess_gdf = tess_nodes_dict[(placeid, scenario)].to_crs(epsg=3857)\n",
    "            ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "            tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "            ax.set_title(f\"[{scenario}] Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "            plt.show()\n",
    "\n",
    "    GTs_dict[scenario] = GTs\n",
    "    GT_abstracts_dict[scenario] = GT_abstracts\n",
    "\n",
    "    # save results for this scenario\n",
    "    prune_measure = \"demand_ltn_priority\"\n",
    "    results = {\n",
    "        \"placeid\": placeid,\n",
    "        \"prune_measure\": prune_measure,\n",
    "        \"poi_source\": params[\"poi_source\"],\n",
    "        \"prune_quantiles\": investment_levels_dict[(placeid, scenario)],\n",
    "        \"GTs\": GTs,\n",
    "        \"GT_abstracts\": GT_abstracts\n",
    "    }\n",
    "    utils.write_result(results, \"pickle\", placeid, params[\"poi_source\"], prune_measure, \".pickle\", weighting=params[\"weighting\"], scenario=scenario)\n",
    "\n",
    "if gifs:\n",
    "    # plot gifs\n",
    "    growth_metric = \"demand_ltn_priority\"  \n",
    "    for scenario in params[\"scenarios\"]:\n",
    "        if scenario == \"no_ltn_scenario\":\n",
    "            print(f\"Skipping GIF generation for scenario: {scenario}\")\n",
    "            continue\n",
    "\n",
    "        ltn_points = ltn_gdfs_dict.get((placeid, scenario))\n",
    "        tess_points = tess_gdfs_dict.get((placeid, scenario))\n",
    "        neighbourhoods = utils.load_neighbourhoods(os.path.join(PATH[\"data\"], placeid, scenario)) if scenario != \"no_ltn_scenario\" else None\n",
    "\n",
    "        for graph_type, graph_list in [(\"routed\", GTs_dict[scenario]), (\"abstract\", GT_abstracts_dict[scenario])]:\n",
    "            output_file = os.path.join(PATH[\"videos\"], placeid, scenario, f\"{placeid}_{scenario}_{graph_type}_{growth_metric}_growth.gif\")\n",
    "            title_prefix = f\"{scenario} - {graph_type.capitalize()} ({growth_metric.replace('_', ' ').capitalize()}) iteration\"\n",
    "            utils.plot_investment_growth_gifs(\n",
    "                graph_list=graph_list,\n",
    "                output_path=output_file,\n",
    "                G_biketrackcarall=G_caralls[(placeid, scenario)],\n",
    "                G_biketrack=G_biketracks[(placeid, scenario)],\n",
    "                ltn_points=ltn_points,\n",
    "                tess_points=tess_points,\n",
    "                neighbourhoods=neighbourhoods,\n",
    "                investment_levels=investment_levels_dict[(placeid, scenario)],\n",
    "                title_prefix=title_prefix\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTNs Not Prioritised "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section of code has produced bike networks where the growth between LTNs has been prioiritsed over any other link. This section produces growth plans where LTNs aren't given explict priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Growth - Prune Greedy Triangulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates many iterations of a **random** growth order. No priority is given to Low Traffic Neighbouhoods. Many runs are required to ensure we get a true sense of what a random strategy would look like. \n",
    "\n",
    "**Note** - creating many runs can take some time, especially if you are running more than one scenario. Good things come to those who wait :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario in params[\"scenarios\"]:\n",
    "    key = (placeid, scenario)\n",
    "    greedy_gdf = greedy_gdfs_dict[key]\n",
    "    G_weighted = G_weighteds[key]\n",
    "    G_caralls_single = G_caralls[key]\n",
    "    all_centroids = all_centroids_dict[key]\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        exit_points = exit_points_dict[key]\n",
    "        ltn_gdf = ltn_gdfs_dict[key]\n",
    "    tess_gdf = tess_gdfs_dict[key]\n",
    "\n",
    "    shuffled_edges = greedy_gdf.sample(frac=1, random_state=42)\n",
    "\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        exit_nodes = set(exit_points['osmid'])\n",
    "        endpoints = set(greedy_gdf['start_osmid']).union(greedy_gdf['end_osmid'])\n",
    "        lookup_nodes = exit_nodes.union(endpoints)\n",
    "    else:\n",
    "        endpoints = set(greedy_gdf['start_osmid']).union(greedy_gdf['end_osmid'])\n",
    "        lookup_nodes = endpoints\n",
    "\n",
    "    print(\"Creating shortest paths lookup for\", scenario)\n",
    "    sp_length = {}\n",
    "    sp_path = {}\n",
    "    for source in lookup_nodes:\n",
    "        dist_dict, path_dict = nx.single_source_dijkstra(G_weighted, source=source, weight='length')\n",
    "        for target in lookup_nodes:\n",
    "            if target in dist_dict:\n",
    "                sp_length[(source, target)] = dist_dict[target]\n",
    "                sp_path[(source, target)] = path_dict[target]\n",
    "\n",
    "    global_cache = {'shuffled_edges': shuffled_edges, 'sp_length': sp_length, 'sp_path': sp_path}\n",
    "\n",
    "    iterations = range(1, 101)\n",
    "    if scenario != \"no_ltn_scenario\":\n",
    "        for run_id in tqdm(iterations, desc=f\"Random Growth Runs - {scenario}\", unit=\"Run\"):\n",
    "            results = utils.run_random_growth(\n",
    "                placeid=placeid,\n",
    "                poi_source=params[\"poi_source\"],\n",
    "                investment_levels=investment_levels_dict[(placeid, scenario)],\n",
    "                weighting=params[\"weighting\"],\n",
    "                greedy_gdf=greedy_gdf,\n",
    "                G_caralls=G_caralls_single,\n",
    "                G_weighted=G_weighted,\n",
    "                all_centroids=all_centroids,\n",
    "                exit_points=exit_points,\n",
    "                sp_length=sp_length,\n",
    "                sp_path=sp_path,\n",
    "                ltn_gdf=ltn_gdf,\n",
    "                tess_gdf=tess_gdf,\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            suffix = f\"_run{run_id:02d}.pickle\"\n",
    "            utils.write_result(\n",
    "                results=results,\n",
    "                file_format = \"pickle\",\n",
    "                placeid=placeid,\n",
    "                poi_source=params[\"poi_source\"],\n",
    "                prune_measure=\"random\",\n",
    "                extension=suffix,\n",
    "                weighting=params[\"weighting\"],\n",
    "                scenario=scenario)\n",
    "    else:\n",
    "        for run_id in tqdm(iterations, desc=f\"Random Growth Runs - {scenario}\", unit=\"Run\"):\n",
    "            results = utils.run_random_growth_no_ltn(\n",
    "                placeid=placeid,\n",
    "                poi_source=params[\"poi_source\"],\n",
    "                investment_levels=investment_levels_dict[(placeid, scenario)],\n",
    "                weighting=params[\"weighting\"],\n",
    "                greedy_gdf=greedy_gdf,\n",
    "                G_caralls=G_caralls_single,\n",
    "                G_weighted=G_weighted,\n",
    "                all_centroids=all_centroids,\n",
    "                sp_length=sp_length,\n",
    "                sp_path=sp_path,\n",
    "                tess_gdf=tess_gdf,\n",
    "                debug=False\n",
    "            )\n",
    "\n",
    "            suffix = f\"_run{run_id:02d}.pickle\"\n",
    "            utils.write_result(\n",
    "                results=results,\n",
    "                file_format = \"pickle\",\n",
    "                placeid=placeid,\n",
    "                poi_source=params[\"poi_source\"],\n",
    "                prune_measure=\"random\",\n",
    "                extension=suffix,\n",
    "                weighting=params[\"weighting\"],\n",
    "                scenario=scenario )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betweenness Growth - Prune Greedy Triangulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates a growth order based on **betweeness centrailty**. No priority is given to Low Traffic Neighbouhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betweenness\n",
    "GTs_dict = {}\n",
    "GT_abstracts_dict = {}\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    previous_selected_edges = set()\n",
    "    GT_abstracts = []\n",
    "    GT_abstracts_gdf = []\n",
    "    GTs = []\n",
    "    GTs_gdf = []\n",
    "    global_processed_pairs = set()\n",
    "    cumulative_GT_indices = set()\n",
    "    G_weighted = G_weighteds[(placeid, scenario)]\n",
    "    G_carall = G_caralls[(placeid, scenario)]\n",
    "\n",
    "    greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "    shortest_paths_all = shortest_paths_all_pairs_dict[(placeid, scenario)]\n",
    "    ebc_all = ebc_all_dict[(placeid, scenario)]\n",
    "    ltn_gdf = ltn_gdfs_dict[(placeid, scenario)] if scenario != \"no_ltn_scenario\" else None\n",
    "    tess_gdf = tess_gdfs_dict[(placeid, scenario)]\n",
    "    all_centroids = all_centroids_dict[(placeid, scenario)]\n",
    "    exit_points = exit_points_dict[(placeid, scenario)] if scenario != \"no_ltn_scenario\" else None\n",
    "\n",
    "    for D in tqdm(investment_levels_dict[(placeid, scenario)], desc=f\"Pruning GT abstract and routing on network for meters of investment - {scenario}\"):\n",
    "        # make abstract greedy triangulation graph\n",
    "        GT_abstract_gdf, previous_selected_edges, connected_pairs = utils.adjust_triangulation_to_budget(\n",
    "            greedy_gdf, D, shortest_paths_all, ebc_all, previous_selected_edges\n",
    "        )\n",
    "\n",
    "        remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Remaining edges to add: {remaining_edges}\")\n",
    "\n",
    "        GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "        GT_abstract_nx = utils.gdf_to_nx_graph(GT_abstract_gdf)\n",
    "        GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "        if debug:\n",
    "            ax = GT_abstract_gdf.to_crs(epsg=3857).plot()\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                ltn_gdf.to_crs(epsg=3857).plot(ax=ax, color='red', markersize=10)\n",
    "            tess_gdf.to_crs(epsg=3857).plot(ax=ax, color='green', markersize=5)\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                for idx, row in ltn_gdf.iterrows():\n",
    "                    ax.annotate(\n",
    "                        text=str(row['osmid']),\n",
    "                        xy=(row.geometry.x, row.geometry.y),\n",
    "                        xytext=(3, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        fontsize=8,\n",
    "                        color=\"red\"\n",
    "                    )\n",
    "\n",
    "        poipairs = list(GT_abstract_nx.edges())\n",
    "        routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Routing on network for investment level: {D} with routenodepairs\", routenodepairs)\n",
    "\n",
    "        GT_indices = set()\n",
    "        processed_pairs = set()\n",
    "\n",
    "        for u, v in routenodepairs:\n",
    "            poipair = (u, v)\n",
    "            if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "                continue\n",
    "\n",
    "            is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "            is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "\n",
    "            if is_u_neighbourhood and is_v_neighbourhood:\n",
    "                neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "                neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "                exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for ea in exit_points_a:\n",
    "                    for eb in exit_points_b:\n",
    "                        pair_id = tuple(sorted((ea, eb)))\n",
    "                        if pair_id in processed_pairs:\n",
    "                            continue\n",
    "                        processed_pairs.add(pair_id)\n",
    "                        try:\n",
    "                            sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                            sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                            if sp_length < shortest_path_length:\n",
    "                                shortest_path_length, best_path = sp_length, sp\n",
    "                        except nx.NetworkXNoPath:\n",
    "                            continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "                neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for ea in exit_points_a:\n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "                neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "                exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for eb in exit_points_b:\n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                    GT_indices.update(sp)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "            global_processed_pairs.add(poipair)\n",
    "        \n",
    "    \n",
    "        cumulative_GT_indices.update(GT_indices)\n",
    "     \n",
    "\n",
    "        GT = G_carall.subgraph(cumulative_GT_indices)\n",
    "        for u, v, data in GT.edges(data=True):\n",
    "            if 'length' in data:\n",
    "                data['weight'] = data['length']\n",
    "\n",
    "\n",
    "        GTs.append(GT)\n",
    "        GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GTs_gdf.append(GT_edges)\n",
    "\n",
    "        if debug:\n",
    "            ax = GT_edges.to_crs(epsg=3857).plot()\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                ltn_gdfs_dict[(placeid, scenario)].to_crs(epsg=3857).plot(ax=ax, color='red', markersize=10)\n",
    "            tess_gdfs_dict[(placeid, scenario)].to_crs(epsg=3857).plot(ax=ax, color='green', markersize=5)\n",
    "            ax.set_title(f\"[{scenario}] Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    # save to plot gifs\n",
    "    GTs_dict[scenario] = GTs\n",
    "    GT_abstracts_dict[scenario] = GT_abstracts\n",
    "\n",
    "    prune_measure = \"betweenness\"\n",
    "    results = {\n",
    "        \"placeid\": placeid,\n",
    "        \"prune_measure\": prune_measure,\n",
    "        \"poi_source\": params[\"poi_source\"],\n",
    "        \"prune_quantiles\": investment_levels_dict[(placeid, scenario)],\n",
    "        \"GTs\": GTs,\n",
    "        \"GT_abstracts\": GT_abstracts\n",
    "    }\n",
    "    utils.write_result(results, \"pickle\", placeid, params[\"poi_source\"], prune_measure, \".pickle\", weighting=params[\"weighting\"], scenario=scenario)\n",
    "\n",
    "\n",
    "if gifs:\n",
    "    growth_metric = \"betweeness\"  \n",
    "    for scenario in params[\"scenarios\"]:\n",
    "        ltn_points = ltn_gdfs_dict.get((placeid, scenario))\n",
    "        tess_points = tess_gdfs_dict.get((placeid, scenario))\n",
    "        neighbourhoods = utils.load_neighbourhoods(os.path.join(PATH[\"data\"], placeid, scenario)) if scenario != \"no_ltn_scenario\" else None\n",
    "\n",
    "        for graph_type, graph_list in [(\"routed\", GTs_dict[scenario]), (\"abstract\", GT_abstracts_dict[scenario])]:\n",
    "            output_file = os.path.join(PATH[\"videos\"], placeid, scenario, f\"{placeid}_{scenario}_{graph_type}_{growth_metric}_growth.gif\")\n",
    "            title_prefix = f\"{scenario} - {graph_type.capitalize()} ({growth_metric.replace('_', ' ').capitalize()}) iteration\"\n",
    "            utils.plot_investment_growth_gifs(\n",
    "                graph_list=graph_list,\n",
    "                output_path=output_file,\n",
    "                G_biketrackcarall=G_caralls[(placeid, scenario)],\n",
    "                G_biketrack=G_biketracks[(placeid, scenario)],\n",
    "                ltn_points=ltn_points,\n",
    "                tess_points=tess_points,\n",
    "                neighbourhoods=neighbourhoods,\n",
    "                investment_levels=investment_levels_dict[(placeid, scenario)],\n",
    "                title_prefix=title_prefix\n",
    "            )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_ltn = set(debug_routing_pairs_dict.get((\"ltn_priority\", scenario), []))\n",
    "pairs_noprio = set(debug_routing_pairs_dict.get((\"no_priority\", scenario), []))\n",
    "\n",
    "extra_in_ltn = pairs_ltn - pairs_noprio\n",
    "extra_in_noprio = pairs_noprio - pairs_ltn\n",
    "\n",
    "print(f\"\\nPairs routed in LTN-priority but not in non-priority: {len(extra_in_ltn)}\")\n",
    "print(list(extra_in_ltn)[:10])\n",
    "\n",
    "print(f\"\\nPairs routed in non-priority but not in LTN-priority: {len(extra_in_noprio)}\")\n",
    "print(list(extra_in_noprio)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "extra_edges = ltn_extra.reset_index()\n",
    "gdf = gpd.GeoDataFrame(extra_edges, geometry=\"geometry\", crs=gt_edges_ltn.crs)\n",
    "\n",
    "gdf.plot(figsize=(8, 8), linewidth=2, color=\"red\")\n",
    "plt.title(\"Edges in LTN-priority but not in non-priority\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the final routed graphs\n",
    "scenario = \"current_ltn_scenario\"\n",
    "gt_edges_ltn = GT_edges_debug_dict.get((\"ltn_priority\", scenario))\n",
    "gt_edges_noprio = GT_edges_debug_dict.get((\"no_priority\", scenario))\n",
    "\n",
    "if gt_edges_ltn is not None and gt_edges_noprio is not None:\n",
    "    print(\"\\n[Comparison] Edges in LTN-priority but not in non-priority:\")\n",
    "    ltn_extra = gt_edges_ltn[~gt_edges_ltn.index.isin(gt_edges_noprio.index)]\n",
    "    print(ltn_extra.reset_index()[[\"u\", \"v\", \"length\"]])\n",
    "\n",
    "\n",
    "    print(\"\\n[Comparison] Edges in non-priority but not in LTN-priority:\")\n",
    "    noprio_extra = gt_edges_noprio[~gt_edges_noprio.index.isin(gt_edges_ltn.index)]\n",
    "    print(noprio_extra.reset_index()[[\"u\", \"v\", \"length\"]])\n",
    "\n",
    "else:\n",
    "    print(\"Could not compare: one or both GTs missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand Growth - Prune Greedy Triangulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates a growth order based on **potential demand**. No priority is given to Low Traffic Neighbouhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demand\n",
    "GTs_dict = {}\n",
    "GT_abstracts_dict = {}\n",
    "\n",
    "for scenario in params[\"scenarios\"]:\n",
    "    previous_selected_edges = set()\n",
    "    GT_abstracts = []\n",
    "    GT_abstracts_gdf = []\n",
    "    GTs = []\n",
    "    GTs_gdf = []\n",
    "    global_processed_pairs = set()\n",
    "    cumulative_GT_indices = set()\n",
    "    G_weighted = G_weighteds[(placeid, scenario)]\n",
    "    G_carall = G_caralls[(placeid, scenario)]\n",
    "\n",
    "    greedy_gdf = greedy_gdfs_dict[(placeid, scenario)]\n",
    "    shortest_paths_all = shortest_paths_all_pairs_dict[(placeid, scenario)]\n",
    "    demand_all = demand_all_dict[(placeid, scenario)]\n",
    "    ltn_gdf = ltn_gdfs_dict[(placeid, scenario)] if scenario != \"no_ltn_scenario\" else None\n",
    "    tess_gdf = tess_gdfs_dict[(placeid, scenario)]\n",
    "    all_centroids = all_centroids_dict[(placeid, scenario)] \n",
    "    exit_points = exit_points_dict[(placeid, scenario)] if scenario != \"no_ltn_scenario\" else None\n",
    "\n",
    "    for D in tqdm(investment_levels_dict[(placeid, scenario)], desc=f\"Pruning GT abstract and routing on network for meters of investment - {scenario}\"):\n",
    "        # make abstract greedy triangulation graph\n",
    "        GT_abstract_gdf, previous_selected_edges, connected_pairs = utils.adjust_triangulation_to_budget(\n",
    "            greedy_gdf, D, shortest_paths_all, demand_all, previous_selected_edges\n",
    "        )\n",
    "\n",
    "        remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Remaining edges to add: {remaining_edges}\")\n",
    "        \n",
    "        GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "        GT_abstract_nx = utils.gdf_to_nx_graph(GT_abstract_gdf)\n",
    "        GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "        if debug:\n",
    "            ax = GT_abstract_gdf.to_crs(epsg=3857).plot()\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                ltn_gdf.to_crs(epsg=3857).plot(ax=ax, color='red', markersize=10)\n",
    "            tess_gdf.to_crs(epsg=3857).plot(ax=ax, color='green', markersize=5)\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                for idx, row in ltn_gdf.iterrows():\n",
    "                    ax.annotate(\n",
    "                        text=str(row['osmid']),\n",
    "                        xy=(row.geometry.x, row.geometry.y),\n",
    "                        xytext=(3, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        fontsize=8,\n",
    "                        color=\"red\"\n",
    "                    )\n",
    "        \n",
    "        poipairs = list(GT_abstract_nx.edges())\n",
    "        routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"[{scenario}] Routing on network for investment level: {D} with routenodepairs\", routenodepairs)\n",
    "\n",
    "        GT_indices = set()\n",
    "        processed_pairs = set()\n",
    "\n",
    "        for u, v in routenodepairs:\n",
    "            poipair = (u, v)\n",
    "            if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "                continue\n",
    "            \n",
    "            is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "            is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "\n",
    "            if is_u_neighbourhood and is_v_neighbourhood:\n",
    "                neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "                neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "                exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for ea in exit_points_a:\n",
    "                    for eb in exit_points_b:\n",
    "                        pair_id = tuple(sorted((ea, eb)))\n",
    "                        if pair_id in processed_pairs:\n",
    "                            continue\n",
    "                        processed_pairs.add(pair_id)\n",
    "                        try:\n",
    "                            sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "                            sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "                            if sp_length < shortest_path_length:\n",
    "                                shortest_path_length, best_path = sp_length, sp\n",
    "                        except nx.NetworkXNoPath:\n",
    "                            continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "                neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "                exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for ea in exit_points_a:\n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "                neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "                exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "                shortest_path_length, best_path = float('inf'), None\n",
    "                for eb in exit_points_b:\n",
    "                    try:\n",
    "                        sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "                        sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "                        if sp_length < shortest_path_length:\n",
    "                            shortest_path_length, best_path = sp_length, sp\n",
    "                    except nx.NetworkXNoPath:\n",
    "                        continue\n",
    "                if best_path:\n",
    "                    GT_indices.update(best_path)\n",
    "\n",
    "            elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "                try:\n",
    "                    sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "                    GT_indices.update(sp)\n",
    "                except nx.NetworkXNoPath:\n",
    "                    continue\n",
    "\n",
    "            global_processed_pairs.add(poipair)\n",
    "\n",
    "        cumulative_GT_indices.update(GT_indices)\n",
    "\n",
    "        GT = G_carall.subgraph(cumulative_GT_indices)\n",
    "        for u, v, data in GT.edges(data=True):\n",
    "            if 'length' in data:\n",
    "                data['weight'] = data['length']\n",
    "\n",
    "        GTs.append(GT)\n",
    "        GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "        GTs_gdf.append(GT_edges)\n",
    "\n",
    "        if debug:\n",
    "            ax = GT_edges.to_crs(epsg=3857).plot()\n",
    "            if scenario != \"no_ltn_scenario\":\n",
    "                ltn_gdfs_dict[(placeid, scenario)].to_crs(epsg=3857).plot(ax=ax, color='red', markersize=10)\n",
    "            tess_gdfs_dict[(placeid, scenario)].to_crs(epsg=3857).plot(ax=ax, color='green', markersize=5)\n",
    "            ax.set_title(f\"[{scenario}] Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "            plt.show()\n",
    "\n",
    "    # save to plot gifs\n",
    "    GTs_dict[scenario] = GTs\n",
    "    GT_abstracts_dict[scenario] = GT_abstracts\n",
    "    \n",
    "    prune_measure = \"demand\"\n",
    "    # save results\n",
    "    results = {\n",
    "        \"placeid\": placeid,\n",
    "        \"prune_measure\": prune_measure,\n",
    "        \"poi_source\": params[\"poi_source\"],\n",
    "        \"prune_quantiles\": investment_levels_dict[(placeid, scenario)],\n",
    "        \"GTs\": GTs,\n",
    "        \"GT_abstracts\": GT_abstracts\n",
    "    }\n",
    "    utils.write_result(\n",
    "        results,\n",
    "        \"pickle\",\n",
    "        placeid,\n",
    "        params[\"poi_source\"],\n",
    "        prune_measure,\n",
    "        \".pickle\",\n",
    "        weighting=params[\"weighting\"],\n",
    "        scenario=scenario\n",
    "    )\n",
    "\n",
    "if gifs:\n",
    "    growth_metric = \"demand\"  \n",
    "    for scenario in params[\"scenarios\"]:\n",
    "        ltn_points = ltn_gdfs_dict.get((placeid, scenario))\n",
    "        tess_points = tess_gdfs_dict.get((placeid, scenario))\n",
    "        neighbourhoods = utils.load_neighbourhoods(os.path.join(PATH[\"data\"], placeid, scenario)) if scenario != \"no_ltn_scenario\" else None\n",
    "        for graph_type, graph_list in [(\"routed\", GTs_dict[scenario]), (\"abstract\", GT_abstracts_dict[scenario])]:\n",
    "            output_file = os.path.join(PATH[\"videos\"], placeid, scenario, f\"{placeid}_{scenario}_{graph_type}_{growth_metric}_growth.gif\")\n",
    "            title_prefix = f\"{scenario} - {graph_type.capitalize()} ({growth_metric.replace('_', ' ').capitalize()}) iteration\"\n",
    "            utils.plot_investment_growth_gifs(\n",
    "                graph_list=graph_list,\n",
    "                output_path=output_file,\n",
    "                G_biketrackcarall=G_caralls[(placeid, scenario)],\n",
    "                G_biketrack=G_biketracks[(placeid, scenario)],\n",
    "                ltn_points=ltn_points,\n",
    "                tess_points=tess_points,\n",
    "                neighbourhoods=neighbourhoods,\n",
    "                investment_levels=investment_levels_dict[(placeid, scenario)],\n",
    "                title_prefix=title_prefix\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Finished processing placeid: {placeid} with scenarios: {params['scenarios']}. You can now move to running script 05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - You don't need to run any of the code below here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important code above is finished :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expermient with determining k in pct aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of neighbours used in in the k-nn has an impact on how accurate assigning demand will be. Currently testing adaptive demand and using an elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# k_test = 5  # Test with different values\n",
    "# nbrs = NearestNeighbors(n_neighbors=k_test).fit(lsoa[['x', 'y']])\n",
    "# distances, _ = nbrs.kneighbors(lsoa[['x', 'y']])\n",
    "\n",
    "# # Sort distances to the k-th neighbor\n",
    "# sorted_distances = np.sort(distances[:, k_test-1])\n",
    "\n",
    "# # Plot the sorted k-distances\n",
    "# plt.plot(sorted_distances)\n",
    "# plt.xlabel(\"Points Sorted by Distance\")\n",
    "# plt.ylabel(f\"Distance to {k_test}-th Nearest Neighbor\")\n",
    "# plt.title(\"k-Distance Graph\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # Compute average distance to nearest 5 neighbors\n",
    "# nbrs = NearestNeighbors(n_neighbors=5).fit(lsoa[['x', 'y']])\n",
    "# distances, _ = nbrs.kneighbors(lsoa[['x', 'y']])\n",
    "# avg_dist = distances.mean(axis=1)\n",
    "\n",
    "# # Set adaptive k: more neighbors in sparse regions, fewer in dense regions\n",
    "# adaptive_k = np.round(1 + (avg_dist / avg_dist.max()) * 10).astype(int)\n",
    "\n",
    "# print(\"Adaptive k values (first 100):\", adaptive_k[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy_gdf.explore(column='total_flow', cmap='viridis', legend=True, legend_name='Dutch SLC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce a network using routed PCT dutch scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Sort the GeoDataFrame by 'dutch_slc' descending - most potential first\n",
    "# sorted_rnet = rnet.sort_values('dutch_slc', ascending=False)\n",
    "# cumulative_lengths = sorted_rnet['length_m'].cumsum().values\n",
    "\n",
    "# G_pct_gdfs = []\n",
    "\n",
    "# # Track the maximum index reached to avoid reprocessing\n",
    "# max_idx = 0\n",
    "\n",
    "# for D in investment_levels:\n",
    "#     # Find the furthest index where cumulative length <= D\n",
    "#     idx = np.searchsorted(cumulative_lengths[max_idx:], D, side='right') + max_idx\n",
    "#     idx = min(idx, len(sorted_rnet))  # Ensure we don't exceed the dataframe\n",
    "    \n",
    "#     # Select edges up to this index\n",
    "#     subset = sorted_rnet.iloc[:idx]\n",
    "#     G_pct_gdfs.append(subset)\n",
    "    \n",
    "#     # Update max_idx to reflect edges already included\n",
    "#     max_idx = idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt at getting population into buildings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ukcensusapi.Nomisweb as census_api\n",
    "\n",
    "# os.environ[\"NOMIS_API_KEY\"] = \"0x98f2ffbbe685f2623fa5c201d4ff86a8c9c46dee\"\n",
    "\n",
    "# api = census_api.Nomisweb(cache_dir = PATH[\"data\"]+ \"/\" + placeid)\n",
    "\n",
    "# # Define the dataset and geography\n",
    "# dataset_id = \"NM_2010_1\"  # Ensure this is the correct dataset ID\n",
    "# geography = \"LSOA11\"      # 2021 LSOA geography type\n",
    "# measures = [\"OBS_VALUE\"]  # Population measure\n",
    "\n",
    "# # List of LSOA codes you're interested in\n",
    "# lsoa_codes = lsoa_bound[\"geo_code\"].tolist()  # Assuming lsoa_bound is your GeoDataFrame\n",
    "\n",
    "# # Fetch data for the specified LSOA codes\n",
    "# population_data = api.get_data(\n",
    "#     dataset_id,\n",
    "#     {\"geography\": geography, \"measures\": measures, \"geography_code\": lsoa_codes, \"date\": \"latest\"}\n",
    "# )\n",
    "\n",
    "# # Display the fetched data\n",
    "# print(population_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investment_levels = [5503.54106,\n",
    "#  369233.16903,\n",
    "#  372964.797]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## many combinations\n",
    "# random_edges = pd.Series(False, index=greedy_gdf.index)  \n",
    "# distance = 0.0 # needed for budget\n",
    "\n",
    "# Random_GT_abstracts = []\n",
    "# Random_GTs = []\n",
    "\n",
    "# # reset global variables\n",
    "# global_processed_pairs_random = set()\n",
    "# cumulative_GT_indices_random = set()\n",
    "\n",
    "\n",
    "# for D in tqdm(investment_levels, desc=\"Pruning GT abstract randomly and routing on network for meters of investment\"):\n",
    "#     # make abstract greedy triangulation graph\n",
    "#     # Calculate remaining budget for new edges\n",
    "#     remaining_budget = D - distance\n",
    "    \n",
    "#     if remaining_budget > 0:\n",
    "#         unselected_edges = greedy_gdf[~random_edges]  # Get edges not yet selected\n",
    "#         if not unselected_edges.empty:\n",
    "#             shuffled_edges = unselected_edges.sample(frac=1) # Shuffle unselected edges to randomize selection\n",
    "#             cumulative_distances = shuffled_edges['distance'].cumsum()\n",
    "#             within_budget = cumulative_distances <= remaining_budget  # Find edges that fit within the remaining budget\n",
    "#             new_indices = shuffled_edges[within_budget].index   # Get indices of edges to add\n",
    "#             random_edges[new_indices] = True\n",
    "#             remaining_edges = (~random_edges).sum()\n",
    "#             print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "#             if within_budget.any():\n",
    "#                 distance += cumulative_distances[within_budget].iloc[-1]\n",
    "#     # save edges\n",
    "#     GT_abstract_gdf = greedy_gdf[random_edges].copy()\n",
    "#     GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "#     Random_GT_abstracts.append(GT_abstract_nx)\n",
    "#     poipairs = list(GT_abstract_nx.edges()) # store currently connected points for routing\n",
    "\n",
    "#     if debug:\n",
    "#         ax = GT_abstract_gdf.plot()\n",
    "#         ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "#         tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "#         for idx, row in ltn_gdf.iterrows():\n",
    "#             ax.annotate(\n",
    "#                 text=str(row['osmid']),  # Use index or another column for labeling\n",
    "#                 xy=(row.geometry.x, row.geometry.y),  # Get the coordinates of the point\n",
    "#                 xytext=(3, 3),  # Offset for better readability\n",
    "#                 textcoords=\"offset points\",\n",
    "#                 fontsize=8,  \n",
    "#                 color=\"red\"\n",
    "#         )\n",
    "        \n",
    "    \n",
    "#     routenodepairs = [(u, v) for u, v in poipairs]\n",
    "   \n",
    "\n",
    "#     if debug:\n",
    "#         print(f\"Routing on network for investment level: {D} with routdenodepairs\", routenodepairs)\n",
    "    \n",
    "    \n",
    "#     ## conditional routing \n",
    "#     # ltn --> ltn (all)\n",
    "#     # ltn --> tess (all to one)\n",
    "#     # tess --> tess (one to one)\n",
    "#     # tess --> ltn (one to all)\n",
    "\n",
    "#     GT_indices = set()\n",
    "#     processed_pairs = set()\n",
    "\n",
    "#     for u, v in routenodepairs:\n",
    "#         poipair = (u, v)\n",
    "#         if poipair in global_processed_pairs_random or tuple(reversed(poipair)) in global_processed_pairs_random:\n",
    "#             continue\n",
    "        \n",
    "#         # Determine if nodes are neighbourhood or tessellation\n",
    "#         is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "#         is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "        \n",
    "#         if is_u_neighbourhood and is_v_neighbourhood:\n",
    "#             # Both nodes are neighbourhoods\n",
    "#             neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "#             neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            \n",
    "#             exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "#             exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "            \n",
    "#             shortest_path_length, best_path = float('inf'), None\n",
    "#             for ea in exit_points_a:\n",
    "#                 for eb in exit_points_b:\n",
    "#                     pair_id = tuple(sorted((ea, eb)))\n",
    "#                     if pair_id in processed_pairs:\n",
    "#                         continue\n",
    "#                     processed_pairs.add(pair_id)\n",
    "                    \n",
    "#                     try:\n",
    "#                         sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "#                         sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "#                         if sp_length < shortest_path_length:\n",
    "#                             shortest_path_length, best_path = sp_length, sp\n",
    "#                     except nx.NetworkXNoPath:\n",
    "#                         continue\n",
    "            \n",
    "#             if best_path:\n",
    "#                 GT_indices.update(best_path)\n",
    "#                 if debug:\n",
    "#                     print(\"Routed between:\", u, v,\"on path: \", best_path)\n",
    "\n",
    "#         elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "#             # Neighbourhood to Tessellation\n",
    "#             neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "#             exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "#             shortest_path_length, best_path = float('inf'), None\n",
    "#             for ea in exit_points_a:\n",
    "#                 try:\n",
    "#                     sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "#                     sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "#                     if sp_length < shortest_path_length:\n",
    "#                         shortest_path_length, best_path = sp_length, sp\n",
    "#                 except nx.NetworkXNoPath:\n",
    "#                     continue\n",
    "            \n",
    "#             if best_path:\n",
    "#                 GT_indices.update(best_path)\n",
    "#                 if debug:\n",
    "#                     print(\"Routed between:\", u, v,\"with a path length of\", shortest_path_length , \"on path: \", best_path)\n",
    "#                     print(\"The exit points were:\", exit_points_a)\n",
    "\n",
    "#         elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "#             # Tessellation to Neighbourhood\n",
    "#             neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "#             exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "#             shortest_path_length, best_path = float('inf'), None\n",
    "#             for eb in exit_points_b:\n",
    "#                 try:\n",
    "#                     sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "#                     sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "#                     if sp_length < shortest_path_length:\n",
    "#                         shortest_path_length, best_path = sp_length, sp\n",
    "#                 except nx.NetworkXNoPath:\n",
    "#                     continue\n",
    "            \n",
    "#             if best_path:\n",
    "#                 GT_indices.update(best_path)\n",
    "#                 if debug:\n",
    "#                     print(\"Routed between:\", u, v,\"on path: \", best_path)\n",
    "\n",
    "#         elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "#             # Tessellation to Tessellation\n",
    "#             try:\n",
    "#                 sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "#                 GT_indices.update(sp)\n",
    "#             except nx.NetworkXNoPath:\n",
    "#                 continue\n",
    "\n",
    "#         else:\n",
    "#             print(\"This should never happen.\")\n",
    "\n",
    "#         global_processed_pairs.add(poipair) # track pairs that we've already routed between\n",
    "    \n",
    "#     cumulative_GT_indices_random.update(GT_indices) # add routes we've already calculated previously to current GT\n",
    "\n",
    "#     # Generate subgraph for selected routes\n",
    "#     GT = G_caralls[placeid].subgraph(cumulative_GT_indices_random)\n",
    "#     #deweight_edges(GT, tag_lts)\n",
    "#     # ensure weight attibute is stored\n",
    "#     for u, v, data in GT.edges(data=True):\n",
    "#         if 'length' in data:\n",
    "#             data['weight'] = data['length']\n",
    "            \n",
    "#     Random_GTs.append(GT)\n",
    "        \n",
    "    \n",
    "\n",
    "#     if debug:\n",
    "#         GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "#         GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "#         ax = GT_edges.plot()\n",
    "#         ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "#         tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "#         ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "\n",
    "#     # write results\n",
    "#     #results = {\"placeid\": placeid, \"prune_measure\": random, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": Random_GTs_igraph, \"GT_abstracts\": Random_GT_abstracts_igraph}\n",
    "#     #write_result(results, \"pickle\", placeid, poi_source, random, \".pickle\", weighting=weighting)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# final_remaining = (~random_edges).sum()\n",
    "# print(f\"Final remaining edges: {final_remaining}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(previous_selected_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_unused_edges(gdf, selected_edges):\n",
    "#     def normalize_edge(row):\n",
    "#         return tuple(sorted((row['start_osmid'], row['end_osmid'])))\n",
    "#     return gdf[~gdf.apply(lambda row: normalize_edge(row) in selected_edges, axis=1)]\n",
    "\n",
    "# adjusted_gdf, selected_edges, _, _ = adjust_triangulation_to_budget_ltn_priority(greedy_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges, ltn_node_pairs)\n",
    "# remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "# unused_edges_gdf = get_unused_edges(greedy_gdf, selected_edges)\n",
    "\n",
    "# # Plotting the results\n",
    "# ax = greedy_gdf.plot(color='lightgrey', linewidth=1, alpha=0.5)  # background context\n",
    "# unused_edges_gdf.plot(ax=ax, color='red', linewidth=2)\n",
    "\n",
    "# unused_edges_gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ebc_ltn.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_triangulation_to_budget_ltn_priority(triangulation_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges=None, ltn_node_pairs=None):\n",
    "#     \"\"\"\n",
    "#     Adjust a given triangulation to fit within the specified budget D,\n",
    "#     ensuring that previously selected edges are always included.\n",
    "#     Only after all ltns are connected do we move to include the growth of other areas.\n",
    "#     \"\"\"\n",
    "#     # make a graph\n",
    "#     G = nx.Graph()\n",
    "#     for _, row in triangulation_gdf.iterrows():\n",
    "#         G.add_edge(\n",
    "#             row['start_osmid'],\n",
    "#             row['end_osmid'],\n",
    "#             geometry=row['geometry'],\n",
    "#             distance=row['distance']\n",
    "#         )\n",
    "\n",
    "#     total_length = 0\n",
    "#     selected_edges = set(tuple(sorted(edge)) for edge in (previous_selected_edges or [])) # use tuple to ensure we don't double count edges\n",
    "\n",
    "#     # Include previously selected edges so that we aren't starting from stratch each loop through\n",
    "#     for u, v in selected_edges:\n",
    "#         if G.has_edge(u, v):\n",
    "#             total_length += G[u][v]['distance']\n",
    "\n",
    "#     # Track the ltns which are connected\n",
    "#     connected_ltn_pairs = set()\n",
    "\n",
    "#     # Track all other connected pairs\n",
    "#     connected_other_pairs = set()\n",
    "\n",
    "#     # Prune for ltn node pairs first\n",
    "#     for (node1, node2), centrality in ebc_ltn.items():\n",
    "#         if node1 in G.nodes and node2 in G.nodes:\n",
    "#             edges = shortest_paths_ltn.get((node1, node2), [])\n",
    "#             if edges:  # If a valid path exists\n",
    "#                 # Calculate new edges and their length\n",
    "#                 new_edges = [tuple(sorted((u, v))) for u, v in edges if tuple(sorted((u, v))) not in selected_edges]\n",
    "#                 new_length = sum(G[min(u, v)][max(u, v)]['distance'] for u, v in new_edges)\n",
    "#                 # Check if adding this path exceeds the budget D\n",
    "#                 if total_length + new_length > D:\n",
    "#                     continue\n",
    "#                 # Add the edges to selected_edges\n",
    "#                 selected_edges.update(new_edges)\n",
    "#                 total_length += new_length\n",
    "#                 connected_ltn_pairs.add((node1, node2))\n",
    "\n",
    "    \n",
    "#     # Check if all ltn node pairs are connected\n",
    "#     if set(ltn_node_pairs).issubset(connected_ltn_pairs):\n",
    "#         # Now move to all other connections (ltn to tess, tess to tess, tess to ltn etc)\n",
    "#         for (node1, node2), centrality in ebc_other.items():\n",
    "#             if node1 in G.nodes and node2 in G.nodes:\n",
    "#                 edges = shortest_paths_other.get((node1, node2), [])\n",
    "#                 if edges:  # If a valid path exists\n",
    "#                     new_edges = [tuple(sorted((u, v))) for u, v in edges if tuple(sorted((u, v))) not in selected_edges]\n",
    "#                     new_length = sum(G[min(u, v)][max(u, v)]['distance'] for u, v in new_edges)\n",
    "#                     # Check if adding this path exceeds the budget D\n",
    "#                     if total_length + new_length > D:\n",
    "#                         continue\n",
    "#                     # Add the edges to selected_edges\n",
    "#                     selected_edges.update(new_edges)\n",
    "#                     total_length += new_length\n",
    "#                     connected_other_pairs.add((node1, node2))\n",
    "#     # missing_pairs = [pair for pair in ltn_node_pairs if pair not in connected_ltn_pairs]\n",
    "\n",
    "\n",
    "#     # edges which aren't in a shortest path won't have been selected\n",
    "#     # we will add these last, as they are the least important\n",
    "#     unused_edges = []\n",
    "#     for _, row in triangulation_gdf.iterrows():\n",
    "#         e = tuple(sorted((row['start_osmid'], row['end_osmid'])))\n",
    "#         dist = row['distance']\n",
    "#         if e not in selected_edges:\n",
    "#             unused_edges.append((dist, e))\n",
    "#     unused_edges.sort(key=lambda x: x[0])\n",
    "#     for dist, edge in unused_edges:\n",
    "#         if total_length + dist <= D:\n",
    "#             selected_edges.add(edge)\n",
    "#             total_length += dist\n",
    "#         else:\n",
    "#             break\n",
    "\n",
    "#     # Build the adjusted GeoDataFrame\n",
    "#     lines = []\n",
    "#     distances = []\n",
    "#     start_osmids = []\n",
    "#     end_osmids = []\n",
    "\n",
    "\n",
    "#     for u, v in selected_edges:\n",
    "#         lines.append(G[u][v]['geometry'])\n",
    "#         distances.append(G[u][v]['distance'])\n",
    "#         start_osmids.append(u)\n",
    "#         end_osmids.append(v)\n",
    "\n",
    "#     adjusted_gdf = gpd.GeoDataFrame({\n",
    "#         'geometry': lines,\n",
    "#         'start_osmid': start_osmids,\n",
    "#         'end_osmid': end_osmids,\n",
    "#         'distance': distances,\n",
    "#     }, crs=triangulation_gdf.crs)\n",
    "\n",
    "#     return adjusted_gdf, selected_edges, connected_ltn_pairs, connected_other_pairs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############# PLOT EVERY GT_ABSTRACT  if debug #############\n",
    "\n",
    "\n",
    "# ## betwenness\n",
    "# ## many combinations\n",
    "# previous_selected_edges = set()\n",
    "\n",
    "# GT_abstracts = []\n",
    "# GTs = []\n",
    "\n",
    "# global_processed_pairs = set()\n",
    "# cumulative_GT_indices = set()\n",
    "\n",
    "\n",
    "# for D in tqdm(investment_levels, desc=\"Pruning GT abstract and routing on network for meters of investment\"):\n",
    "#     # make abstract greedy triangulation graph\n",
    "#     GT_abstract_gdf, previous_selected_edges, connected_ltn_pairs, connected_other_pairs = adjust_triangulation_to_budget_ltn_priority(greedy_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges, ltn_node_pairs)\n",
    "#     remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "#     if debug:\n",
    "#         print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "    \n",
    "#     GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "#     GT_abstracts.append(GT_abstract_nx)\n",
    "#     if debug:\n",
    "#         # Plot GT_abstract_gdf with a title indicating the number of meters used to invest at that level\n",
    "#         ax = GT_abstract_gdf.plot()\n",
    "#         ax.set_title(f\"Investment level: {D} meters\")\n",
    "#         ax.set_title(f\"Investment level: {D} meters, Number of edges: {len(GT_abstract_gdf)}\")\n",
    "#         plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test = GT_abstracts[2]\n",
    "# Test.edges(data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List to store total lengths of each graph in GT_abstracts\n",
    "# total_lengths_abstracts = []\n",
    "\n",
    "# for G in GT_abstracts:\n",
    "#     # Calculate total edge length of the graph\n",
    "#     lengths = nx.get_edge_attributes(G, 'weight')\n",
    "#     total_length = sum(lengths.values())\n",
    "#     total_lengths_abstracts.append(total_length)\n",
    "\n",
    "# # Create a line plot\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(total_lengths_abstracts, marker='o', linestyle='-')\n",
    "# plt.title('Total Length of Abstract Graphs')\n",
    "# plt.xlabel('Graph Index in GT_abstracts')\n",
    "# plt.ylabel('Total Length (meters)')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set investment max to a fully connected bicycle network, and interval to 1% incremental steps\n",
    "# points = np.linspace(0, total_distance, 101)\n",
    "# # Drop 0 as we don't need to consider the empty network\n",
    "# investment_levels = points[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = list(GT_abstract_nx.edges())\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## betwenness\n",
    "# ## many combinations\n",
    "# previous_selected_edges = set()\n",
    "\n",
    "# GT_abstracts = []\n",
    "# GT_abstracts_gdf = []\n",
    "# GTs = []\n",
    "# GTs_gdf = []\n",
    "\n",
    "\n",
    "\n",
    "# global_processed_pairs = set()\n",
    "# cumulative_GT_indices = set()\n",
    "\n",
    "\n",
    "# for D in tqdm(investment_levels, desc=\"Pruning GT abstract and routing on network for meters of investment\"):\n",
    "#     # make abstract greedy triangulation graph\n",
    "#     GT_abstract_gdf, previous_selected_edges, connected_ltn_pairs, connected_other_pairs = adjust_triangulation_to_budget_ltn_priority(greedy_gdf, D, shortest_paths_ltn, ebc_ltn, shortest_paths_other, ebc_other, previous_selected_edges, ltn_node_pairs)\n",
    "#     remaining_edges = len(greedy_gdf) - len(previous_selected_edges)\n",
    "#     if debug:\n",
    "#         print(f\"Remaining edges to add: {remaining_edges}\")\n",
    "    \n",
    "#     GT_abstracts_gdf.append(GT_abstract_gdf)\n",
    "#     GT_abstract_nx = gdf_to_nx_graph(GT_abstract_gdf)\n",
    "#     GT_abstracts.append(GT_abstract_nx)\n",
    "\n",
    "#     if debug:\n",
    "#         ax = GT_abstract_gdf.plot()\n",
    "#         ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "#         tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "#         for idx, row in ltn_gdf.iterrows():\n",
    "#             ax.annotate(\n",
    "#                 text=str(row['osmid']),  # Use index or another column for labeling\n",
    "#                 xy=(row.geometry.x, row.geometry.y),  # Get the coordinates of the point\n",
    "#                 xytext=(3, 3),  # Offset for better readability\n",
    "#                 textcoords=\"offset points\",\n",
    "#                 fontsize=8,  \n",
    "#                 color=\"red\"\n",
    "#         )\n",
    "        \n",
    "#     # poipairs = connected_ltn_pairs | connected_other_pairs\n",
    "#     poipairs = list(GT_abstract_nx.edges())\n",
    "#     routenodepairs = [(u, v) for u, v in poipairs]\n",
    "\n",
    "#     if debug:\n",
    "#         print(f\"Routing on network for investment level: {D} with routdenodepairs\", routenodepairs)\n",
    "    \n",
    "#     GT_indices = set()\n",
    "    \n",
    "#     ## conditional routing \n",
    "#     # ltn --> ltn (all)\n",
    "#     # ltn --> tess (all to one)\n",
    "#     # tess --> tess (one to one)\n",
    "#     # tess --> ltn (one to all)\n",
    "\n",
    "#     GT_indices = set()\n",
    "#     processed_pairs = set()\n",
    "\n",
    "#     for u, v in routenodepairs:\n",
    "#         poipair = (u, v)\n",
    "#         if poipair in global_processed_pairs or tuple(reversed(poipair)) in global_processed_pairs:\n",
    "#             continue\n",
    "        \n",
    "#         # Determine if nodes are neighbourhood or tessellation\n",
    "#         is_u_neighbourhood = u in all_centroids['nearest_node'].values\n",
    "#         is_v_neighbourhood = v in all_centroids['nearest_node'].values\n",
    "        \n",
    "#         if is_u_neighbourhood and is_v_neighbourhood:\n",
    "#             # Both nodes are neighbourhoods\n",
    "#             neighbourhood_a = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "#             neighbourhood_b = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "            \n",
    "#             exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_a, 'osmid']\n",
    "#             exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_b, 'osmid']\n",
    "            \n",
    "#             shortest_path_length, best_path = float('inf'), None\n",
    "#             for ea in exit_points_a:\n",
    "#                 for eb in exit_points_b:\n",
    "#                     pair_id = tuple(sorted((ea, eb)))\n",
    "#                     if pair_id in processed_pairs:\n",
    "#                         continue\n",
    "#                     processed_pairs.add(pair_id)\n",
    "                    \n",
    "#                     try:\n",
    "#                         sp = nx.shortest_path(G_weighted, source=ea, target=eb, weight='length')\n",
    "#                         sp_length = nx.shortest_path_length(G_weighted, source=ea, target=eb, weight='length')\n",
    "#                         if sp_length < shortest_path_length:\n",
    "#                             shortest_path_length, best_path = sp_length, sp\n",
    "#                     except nx.NetworkXNoPath:\n",
    "#                         continue\n",
    "            \n",
    "#             if best_path:\n",
    "#                 GT_indices.update(best_path)\n",
    "\n",
    "#         elif is_u_neighbourhood and not is_v_neighbourhood:\n",
    "#             # Neighbourhood to Tessellation\n",
    "#             neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == u, 'neighbourhood_id'].values[0]\n",
    "#             exit_points_a = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "#             shortest_path_length, best_path = float('inf'), None\n",
    "#             for ea in exit_points_a:\n",
    "#                 try:\n",
    "#                     sp = nx.shortest_path(G_weighted, source=ea, target=v, weight='length')\n",
    "#                     sp_length = nx.shortest_path_length(G_weighted, source=ea, target=v, weight='length')\n",
    "#                     if sp_length < shortest_path_length:\n",
    "#                         shortest_path_length, best_path = sp_length, sp\n",
    "#                 except nx.NetworkXNoPath:\n",
    "#                     continue\n",
    "            \n",
    "#             if best_path:\n",
    "#                 GT_indices.update(best_path)\n",
    "\n",
    "#         elif not is_u_neighbourhood and is_v_neighbourhood:\n",
    "#             # Tessellation to Neighbourhood\n",
    "#             neighbourhood_id = all_centroids.loc[all_centroids['nearest_node'] == v, 'neighbourhood_id'].values[0]\n",
    "#             exit_points_b = exit_points.loc[exit_points['neighbourhood_id'] == neighbourhood_id, 'osmid']\n",
    "\n",
    "#             shortest_path_length, best_path = float('inf'), None\n",
    "#             for eb in exit_points_b:\n",
    "#                 try:\n",
    "#                     sp = nx.shortest_path(G_weighted, source=u, target=eb, weight='length')\n",
    "#                     sp_length = nx.shortest_path_length(G_weighted, source=u, target=eb, weight='length')\n",
    "#                     if sp_length < shortest_path_length:\n",
    "#                         shortest_path_length, best_path = sp_length, sp\n",
    "#                 except nx.NetworkXNoPath:\n",
    "#                     continue\n",
    "            \n",
    "#             if best_path:\n",
    "#                 GT_indices.update(best_path)\n",
    "\n",
    "#         elif not is_u_neighbourhood and not is_v_neighbourhood:\n",
    "#             # Tessellation to Tessellation\n",
    "#             try:\n",
    "#                 sp = nx.shortest_path(G_weighted, source=u, target=v, weight='length')\n",
    "#                 GT_indices.update(sp)\n",
    "#             except nx.NetworkXNoPath:\n",
    "#                 continue\n",
    "\n",
    "#         else:\n",
    "#             print(\"This should never happen.\")\n",
    "\n",
    "#         global_processed_pairs.add(poipair) # track pairs that we've already routed between\n",
    "    \n",
    "#     cumulative_GT_indices.update(GT_indices) # add routes we've already calculated previously to current GT\n",
    "\n",
    "#     # Generate subgraph for selected routes\n",
    "#     GT = G_caralls[placeid].subgraph(cumulative_GT_indices)\n",
    "#     #deweight_edges(GT, tag_lts)\n",
    "#     # ensure weight attibute is stored\n",
    "#     for u, v, data in GT.edges(data=True):\n",
    "#         if 'length' in data:\n",
    "#             data['weight'] = data['length']\n",
    "            \n",
    "#     GTs.append(GT)\n",
    "#     GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "#     GTs_gdf.append(GT_edges)\n",
    "\n",
    "\n",
    "#     if debug:\n",
    "#         GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "#         GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "#         ax = GT_edges.plot()\n",
    "#         ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "#         tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "#         ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")\n",
    "\n",
    "\n",
    "#     # write results\n",
    "#     #results = {\"placeid\": placeid, \"prune_measure\": prune_measure, \"poi_source\": poi_source, \"prune_quantiles\": investment_levels, \"GTs\": GTs_igraph, \"GT_abstracts\": GT_abstracts_igraph}\n",
    "#     #write_result(results, \"pickle\", placeid, poi_source, prune_measure, \".pickle\", weighting=weighting)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration = 0\n",
    "\n",
    "# ax = GTs_gdf[iteration].to_crs(epsg=3857).plot()\n",
    "# GT_abstracts_gdf[iteration].to_crs(epsg=3857).plot(ax=ax, color='red')\n",
    "\n",
    "# ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "# tess_gdf.plot(ax=ax, color='green', markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy_nx\n",
    "\n",
    "# # Calculate edge betweenness centrality\n",
    "# edge_betweenness = nx.edge_betweenness_centrality(greedy_nx, weight='sp_lts_distance')\n",
    "\n",
    "# # Convert to GeoDataFrame\n",
    "# edges = []\n",
    "# for u, v, data in greedy_nx.edges(data=True):\n",
    "#     edge_data = {\n",
    "#         'start_osmid': u,\n",
    "#         'end_osmid': v,\n",
    "#         'geometry': data['geometry'],\n",
    "#         'betweenness_centrality': edge_betweenness[(u, v)]\n",
    "#     }\n",
    "#     edges.append(edge_data)\n",
    "\n",
    "# greedy_gdf_with_betweenness = gpd.GeoDataFrame(edges).set_crs(3857)\n",
    "\n",
    "# # Calculate the rank of betweenness centrality\n",
    "# greedy_gdf_with_betweenness['betweenness_rank'] = greedy_gdf_with_betweenness['betweenness_centrality'].rank(ascending=False).astype(int)\n",
    "\n",
    "# greedy_gdf_with_betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the top 20 rows based on betweenness_rank\n",
    "# top_20_betweenness = greedy_gdf_with_betweenness.nsmallest(20, 'betweenness_rank')\n",
    "\n",
    "# # Use .explore() to visualize the top 20 betweenness ranking\n",
    "# top_20_betweenness.explore(cmap='viridis', column='betweenness_rank', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy_gdf_with_betweenness.explore(cmap='viridis', column='betweenness_rank', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration = 50\n",
    "# m= GT_abstracts_gdf[iteration].to_crs(epsg=3857).explore(color='red')\n",
    "# neighbourhoods['Newcastle Upon Tyne'].explore(m=m, color='red', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Calculate total lengths for each graph in GTs\n",
    "# total_lengths = []\n",
    "# for G in GTs:\n",
    "#     lengths = nx.get_edge_attributes(G, 'length')\n",
    "#     total_length = sum(lengths.values())\n",
    "#     total_lengths.append(total_length)\n",
    "\n",
    "# # Plot the total lengths\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(total_lengths, marker='o', linestyle='-')\n",
    "# plt.xlabel('Graph Index')\n",
    "# plt.ylabel('Total Length (meters)')\n",
    "# plt.title('Total Length of Graphs in GTs')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investment_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "# GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "# ax = GT_edges.plot()\n",
    "\n",
    "# GT_abs_nodes, GT_abs_edges = ox.graph_to_gdfs(GT_abs)\n",
    "# GT_abs_edges = GT_abs_edges.to_crs(epsg=3857)\n",
    "# GT_abs_edges.plot(ax=ax, color='red', linewidth=2)\n",
    "\n",
    "# ltn_gdf.plot(ax=ax, color='red', markersize=10)\n",
    "# tess_gdf.plot(ax=ax, color='green', markersize=5)\n",
    "# ax.set_title(f\"Investment level: {D}, Number of edges: {len(GT.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpkg_path = PATH[\"data\"] + placeid + \"/\" + placeid + '_biketrack.gpkg'\n",
    "# G_biketrack = ox_gpkg_to_graph(gpkg_path)\n",
    "# G_biketrack.remove_nodes_from(list(nx.isolates(G_biketrack)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# import geopandas as gpd\n",
    "\n",
    "# # Assuming G_biketrack_edges and exit_points_3857 are already defined as GeoDataFrames\n",
    "\n",
    "# # Ensure both GeoDataFrames are in the same CRS (e.g., EPSG:4326 for folium compatibility)\n",
    "# if G_biketrack_edges.crs != 'EPSG:4326':\n",
    "#     G_biketrack_edges = G_biketrack_edges.to_crs('EPSG:4326')\n",
    "# if exit_points_3857.crs != 'EPSG:4326':\n",
    "#     exit_points_3857 = exit_points_3857.to_crs('EPSG:4326')\n",
    "\n",
    "# # Create a folium map centered on the first point of G_biketrack_edges\n",
    "# map_center = [G_biketrack_edges.geometry.centroid.y.iloc[0], G_biketrack_edges.geometry.centroid.x.iloc[0]]\n",
    "# m = folium.Map(location=map_center, zoom_start=14)\n",
    "\n",
    "# # Add G_biketrack_edges to the map as a line layer\n",
    "# for _, row in G_biketrack_edges.iterrows():\n",
    "#     folium.PolyLine(\n",
    "#         locations=[(point[1], point[0]) for point in row.geometry.coords],  # Swap lat/lon for folium\n",
    "#         color='blue',\n",
    "#         weight=2,\n",
    "#         opacity=0.7\n",
    "#     ).add_to(m)\n",
    "\n",
    "# # Add exit_points_3857 to the map as a point layer\n",
    "# for _, row in exit_points_3857.iterrows():\n",
    "#     folium.CircleMarker(\n",
    "#         location=(row.geometry.y, row.geometry.x),  # Swap lat/lon for folium\n",
    "#         radius=5,\n",
    "#         color='red',\n",
    "#         fill=True,\n",
    "#         fill_color='red',\n",
    "#         fill_opacity=0.7\n",
    "#     ).add_to(m)\n",
    "\n",
    "# # Display the map\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## investigate if exit points match with neighbourhoods?\n",
    "\n",
    "# # Set up a larger figure\n",
    "# fig, ax = plt.subplots(figsize=(20, 14))  # Adjust the width and height as needed\n",
    "\n",
    "\n",
    "# # Add G_weighted edges\n",
    "# G_weighted_nodes, G_weighted_edges = ox.graph_to_gdfs(G_weighted)\n",
    "# G_weighted_edges = G_weighted_edges.to_crs(epsg=3857)  # Ensure CRS matches\n",
    "# G_weighted_edges.plot(ax=ax, color='lightgrey', linewidth=0.5, alpha=0.8, zorder = 0)  # Light grey with thin linewidth\n",
    "\n",
    "# # Add bike track edges\n",
    "# G_biketrack_nodes, G_biketrack_edges = ox.graph_to_gdfs(G_biketrack)\n",
    "# G_biketrack_edges = G_biketrack_edges.to_crs(epsg=3857)\n",
    "# G_biketrack_edges.plot(ax=ax, color='turquoise', linewidth=0.5, alpha=0.8, zorder = 2)  # Light grey with thin linewidth\n",
    "\n",
    "\n",
    "# # Plot the main graph and layers\n",
    "# GT_nodes, GT_edges = ox.graph_to_gdfs(GT)\n",
    "# GT_edges = GT_edges.to_crs(epsg=3857)\n",
    "# GT_edges.plot(ax=ax, color='orange', zorder = 1)\n",
    "\n",
    "# # plot exit points\n",
    "# exit_points_3857 = exit_points.to_crs(epsg=3857)\n",
    "# exit_points_3857.plot(ax=ax, color='red', markersize=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Remove x and y axis labels and ticks\n",
    "# ax.axis('off')  # This removes the entire axis, including labels and ticks\n",
    "\n",
    "# # Enhance plot aesthetics\n",
    "# ax.set_title(f\"Meters of investment: {D/10}\")\n",
    "# ax.legend(loc=\"upper left\")\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
